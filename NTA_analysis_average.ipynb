{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c874b02-5be1-4231-830e-293c3db3d557",
   "metadata": {
    "tags": []
   },
   "source": [
    "# NTA Data Analysis Framework\n",
    "\n",
    "## Program Description\n",
    "This comprehensive program analyzes Nanoparticle Tracking Analysis (NTA) data files generated by ZetaView (Particle Metrix QUATT, ZetaView version 8.06.01 SP1). The framework handles both single-file and multi-file (replicate) analysis, performing sophisticated data averaging with uncertainty propagation, calculating key statistical parameters, and generating comprehensive visualizations and output files.\n",
    "\n",
    "## Author\n",
    "- Heike Boehm, Department of Cellular Biophysics, MPI for Medical Research (MPImF-CBP-GS)\n",
    "\n",
    "## References\n",
    "This analysis framework is based on data generated according to:\n",
    "- Particle Metrix GmbH. (2020). Operating Instructions for ZetaView Nanoparticle Tracking Analyzers PMX-120, PMX-220, PMX-420 (Version 4.2).\n",
    "\n",
    "## Complete Analysis Workflow\n",
    "The program follows a linear workflow executed sequentially:\n",
    "\n",
    "### 1. **File Selection Module** (Cell 01)\n",
    "   - Interactive selection of single or multiple NTA files\n",
    "   - Configuration settings and file validation\n",
    "\n",
    "### 2. **File I/O Module** (Cell 02)\n",
    "   - Reading NTA files with latin1 encoding\n",
    "   - Section identification and structure validation\n",
    "\n",
    "### 3. **Data Extraction and Averaging Module** (Cell 03)\n",
    "   - Extraction from linear and logarithmic scales\n",
    "   - **Multi-file averaging**: Bin-by-bin averaging across replicates: xÃÑ·µ¢ = (1/n)Œ£‚±º x·µ¢‚±º\n",
    "   - **Standard deviation calculation**: Sample standard deviation for each bin: s·µ¢ = ‚àö[(1/(n-1))Œ£‚±º(x·µ¢‚±º - xÃÑ·µ¢)¬≤]\n",
    "\n",
    "### 4. **Automated Metadata Extraction** (Cell 04)\n",
    "   - Comprehensive metadata extraction using regex patterns\n",
    "   - Cross-file comparison and quality control flagging\n",
    "\n",
    "### 5. **Core Calculations with Uncertainty Propagation** (Cell 05)\n",
    "   - **Dilution correction**: C‚Çõ‚Çê‚Çò‚Çö‚Çó‚Çë = C‚Çò‚Çë‚Çê‚Çõ·µ§·µ£‚Çëùíπ √ó D, with œÉ‚Çõ‚Çê‚Çò‚Çö‚Çó‚Çë = œÉ‚Çò‚Çë‚Çê‚Çõ·µ§·µ£‚Çëùíπ √ó D\n",
    "   - **Area normalization**: f_norm(x) = f(x) / ‚à´f(x)dx using trapezoidal integration\n",
    "   - **Cumulative uncertainty**: œÉ_cumsum[j] = ‚àö(Œ£·µ¢‚Çå‚ÇÄ ≤ œÉ·µ¢¬≤) for independent errors\n",
    "   - Total metrics calculation with uncertainty combination\n",
    "\n",
    "### 6. **Statistics with Bounds-Based Uncertainty** (Cell 06)\n",
    "   - **D-value calculation**: D10, D50, D90 with asymmetric confidence bounds\n",
    "   - **Bounds-based approach**: Interpolation of cumsum¬±SD curves for uncertainty bounds\n",
    "   - Statistics for number-, volume-, and surface area-weighted distributions\n",
    "\n",
    "### 7. **Visualization Suite** (Cells 10a-10d)\n",
    "   - **Number-weighted plots**: Shape analysis with lognormal fits (linear and log scales)\n",
    "   - **Volume-weighted plots**: Volume contribution analysis with lognormal fits (linear and log scales)\n",
    "   - **Surface area-weighted plots**: Surface area analysis with lognormal fits (linear and log scales)\n",
    "   - **Raw particle count plots**: Quality control with instrument settings\n",
    "   - Each plot: main distribution + cumulative distribution with error bars and D-value markers\n",
    "\n",
    "## Data Processing and Uncertainty Propagation\n",
    "\n",
    "### Multi-File Averaging\n",
    "For replicate analysis:\n",
    "1. **Bin-by-bin averaging**: xÃÑ·µ¢ = (1/n)Œ£‚±º x·µ¢‚±º for each size bin i across n replicates\n",
    "2. **Sample standard deviation**: s·µ¢ = ‚àö[(1/(n-1))Œ£‚±º(x·µ¢‚±º - xÃÑ·µ¢)¬≤]\n",
    "3. **Error propagation**: Uncertainties maintained through all mathematical operations\n",
    "\n",
    "### Statistical Methods\n",
    "- **Cumulative uncertainty**: œÉ_cumsum[j] = ‚àö(Œ£·µ¢‚Çå‚ÇÄ ≤ œÉ·µ¢¬≤) for cumulative distributions\n",
    "- **Bounds-based D-values**: Asymmetric confidence intervals via interpolation of cumsum¬±SD curves\n",
    "- **Total metrics uncertainty**: œÉ_total = ‚àö(Œ£·µ¢ œÉ·µ¢¬≤) for summed quantities\n",
    "\n",
    "### Distribution Types\n",
    "- **Number-weighted**: Area-normalized for shape analysis: ‚à´f_norm(x)dx = 1\n",
    "- **Volume/Surface area-weighted**: Absolute quantities per mL with dilution correction\n",
    "\n",
    "## Output Files and Visualizations\n",
    "The framework generates a complete set of standardized output files:\n",
    "\n",
    "### Metadata Files\n",
    "- **`Data_[uniqueID]_metadata.txt`**: Comprehensive metadata with measurement parameters, quality control flags, calculated metrics, and total volume/surface area per mL (enabling normalization for samples with identical volumes/areas)\n",
    "\n",
    "### Distribution Data\n",
    "- **`Data_[uniqueID]_PSD.txt`**: Complete particle size distribution data with uncertainties for all scales and distribution types\n",
    "\n",
    "### Statistical Summaries\n",
    "- **`Stats_[uniqueID]_comprehensive.txt`**: D-values, span, and total metrics with uncertainty bounds for all distribution types\n",
    "\n",
    "### Visualization Files\n",
    "- **Number-weighted plots**: `Plot_[uniqueID]_linear_number.pdf/png` and `Plot_[uniqueID]_log_number.pdf/png`\n",
    "- **Volume-weighted plots**: `Plot_[uniqueID]_linear_volume.pdf/png` and `Plot_[uniqueID]_log_volume.pdf/png`\n",
    "- **Surface area-weighted plots**: `Plot_[uniqueID]_linear_surface_area.pdf/png` and `Plot_[uniqueID]_log_surface_area.pdf/png`\n",
    "- **Quality control plots**: `Plot_[uniqueID]_raw_counts.pdf/png`\n",
    "\n",
    "### Model Fitting Results\n",
    "- **`Fits_[uniqueID]_all.json`**: Comprehensive fitting parameters for all distribution types and scales\n",
    "\n",
    "## Key Features\n",
    "- **Replicate handling**: Automatic averaging with rigorous uncertainty quantification\n",
    "- **Quality control**: Built-in validation and alert systems  \n",
    "- **Multiple weightings**: Number, volume, and surface area perspectives\n",
    "- **Comprehensive statistics**: D-values with asymmetric confidence bounds\n",
    "- **Standardized output**: Consistent file formats for downstream analysis\n",
    "- **Total metrics**: Volume and surface area per mL saved in metadata for sample comparison\n",
    "\n",
    "## Usage Notes\n",
    "- Execute cells sequentially from Cell 01 through visualization cells\n",
    "- Single-file analysis: standard deviations = 0, complete pipeline still applies\n",
    "- Configuration settings in Cell 01 must match your directory structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29530ae2-8dc8-4681-af6f-446abcb70fc9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "NTA DATA ANALYSIS - MULTI-FILE SELECTION\n",
      "================================================================================\n",
      "Current directory: Inbox\n",
      "\n",
      "Error: Directory not found: Inbox\n",
      "No NTA files found in the current directory.\n",
      "Please set a different directory using set_data_directory()\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "NTA Data Analysis - Multi-file Selection Module (Cell 01)\n",
    "\n",
    "This module provides file selection for single or multiple NTA files\n",
    "for replicate analysis with averaging and standard deviation calculations.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "\n",
    "if 'CONFIG' not in globals():\n",
    "    CONFIG = {\n",
    "        \"directory\": \"Inbox\",  # Update this path\n",
    "        \"file_identifier\": \".txt\",\n",
    "        \"output_subdirs\": [\"metadata\", \"processed\"],\n",
    "        \"nta_concentration_calibration_factor\": 4.61E+5,  # Default ZetaView calibration\n",
    "        \"project_metadata\": {\n",
    "            \"experimenter\": \"Your_Initials\",\n",
    "            \"location\": \"Your_Lab_Location\",\n",
    "            \"project\": \"Your_Project_Name\",\n",
    "            \"meta_version\": \"v03\",\n",
    "            \"pi\": \"Principal_Investigator_Initials\",\n",
    "            \"funding\": \"Funding_Source\",\n",
    "            \"data_collection_method\": \"NTA\",\n",
    "            \"unit_of_analysis\": '[\"nm\", \"nm^2\", \"nm^3\"]',\n",
    "            \"keywords\": \"particle_size_distribution\",\n",
    "            \"publications\": \"None\",\n",
    "        }\n",
    "    }\n",
    "\n",
    "def set_data_directory(directory_path):\n",
    "    \"\"\"\n",
    "    Set or update the data directory in CONFIG.\n",
    "    \n",
    "    Parameters:\n",
    "    directory_path (str): Path to directory containing NTA data\n",
    "    \n",
    "    Returns:\n",
    "    bool: True if directory exists and was set, False otherwise\n",
    "    \"\"\"\n",
    "    global CONFIG\n",
    "    \n",
    "    # Validate directory exists\n",
    "    if not os.path.exists(directory_path):\n",
    "        print(f\"Error: Directory not found: {directory_path}\")\n",
    "        return False\n",
    "    \n",
    "    # Update CONFIG with new directory\n",
    "    CONFIG[\"directory\"] = directory_path\n",
    "    \n",
    "    # Ensure output subdirectories exist\n",
    "    for subdir in CONFIG[\"output_subdirs\"]:\n",
    "        output_dir = os.path.join(directory_path, subdir)\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    print(f\"Data directory set to: {directory_path}\")\n",
    "    print(f\"Output directories created/verified.\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "def find_nta_files():\n",
    "    \"\"\"\n",
    "    Find NTA data files in the current directory.\n",
    "    \n",
    "    Returns:\n",
    "    list: List of NTA files, or empty list if none found\n",
    "    \"\"\"\n",
    "    directory = CONFIG[\"directory\"]\n",
    "    file_identifier = CONFIG[\"file_identifier\"]\n",
    "    \n",
    "    # Validate directory exists\n",
    "    if not os.path.exists(directory):\n",
    "        print(f\"Error: Directory not found: {directory}\")\n",
    "        return []\n",
    "    \n",
    "    # Find all matching files\n",
    "    all_files = os.listdir(directory)\n",
    "    nta_files = [f for f in all_files if f.endswith(file_identifier)]\n",
    "    \n",
    "    return nta_files\n",
    "\n",
    "def extract_sample_info(filename):\n",
    "    \"\"\"\n",
    "    Extract sample information from filename for more readable display.\n",
    "    \n",
    "    Parameters:\n",
    "    filename (str): Filename to analyze\n",
    "    \n",
    "    Returns:\n",
    "    str: Formatted sample information\n",
    "    \"\"\"\n",
    "    # Remove common prefixes and suffixes\n",
    "    base_name = filename.replace(\"_rawdata.txt\", \"\").replace(\"size_NTA\", \"\")\n",
    "    \n",
    "    if base_name.startswith(\"Data_\"):\n",
    "        base_name = base_name[5:]\n",
    "    \n",
    "    # Look for date pattern (e.g., 20250311)\n",
    "    date_match = re.search(r'_(\\d{8})_', base_name)\n",
    "    date_str = \"\"\n",
    "    if date_match:\n",
    "        date = date_match.group(1)\n",
    "        date_str = f\" (Date: {date[0:4]}-{date[4:6]}-{date[6:8]})\"\n",
    "    \n",
    "    return f\"{base_name}{date_str}\"\n",
    "\n",
    "def parse_indices(indices_input):\n",
    "    \"\"\"\n",
    "    Parse file indices from string input.\n",
    "    \n",
    "    Parameters:\n",
    "    indices_input (str): Comma-separated indices (e.g., \"0,1,2\" or \"0\")\n",
    "    \n",
    "    Returns:\n",
    "    list: List of integer indices, or empty list if invalid\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Remove spaces and split by commas\n",
    "        indices_str = indices_input.replace(\" \", \"\").split(\",\")\n",
    "        indices = [int(idx) for idx in indices_str if idx.strip()]\n",
    "        return indices\n",
    "    except ValueError:\n",
    "        print(f\"Error: Invalid indices format. Use comma-separated numbers (e.g., '0,1,2')\")\n",
    "        return []\n",
    "\n",
    "def select_files(indices_input):\n",
    "    \"\"\"\n",
    "    Select multiple files by indices for replicate analysis.\n",
    "    \n",
    "    Parameters:\n",
    "    indices_input (str): Comma-separated string of indices (e.g., \"0,1,2\")\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (filenames_list, filepaths_list) or (None, None) if invalid\n",
    "    \"\"\"\n",
    "    nta_files = find_nta_files()\n",
    "    \n",
    "    if not nta_files:\n",
    "        print(\"No NTA files found in the current directory.\")\n",
    "        return None, None\n",
    "    \n",
    "    # Parse indices\n",
    "    indices = parse_indices(indices_input)\n",
    "    if not indices:\n",
    "        return None, None\n",
    "    \n",
    "    # Validate indices\n",
    "    invalid_indices = [idx for idx in indices if idx < 0 or idx >= len(nta_files)]\n",
    "    if invalid_indices:\n",
    "        print(f\"Invalid file indices: {invalid_indices}. Must be between 0 and {len(nta_files)-1}\")\n",
    "        return None, None\n",
    "    \n",
    "    # Remove duplicates while preserving order\n",
    "    unique_indices = []\n",
    "    for idx in indices:\n",
    "        if idx not in unique_indices:\n",
    "            unique_indices.append(idx)\n",
    "    \n",
    "    # Select files\n",
    "    selected_filenames = []\n",
    "    selected_filepaths = []\n",
    "    \n",
    "    for idx in unique_indices:\n",
    "        filename = nta_files[idx]\n",
    "        filepath = os.path.join(CONFIG[\"directory\"], filename)\n",
    "        selected_filenames.append(filename)\n",
    "        selected_filepaths.append(filepath)\n",
    "    \n",
    "    # Display selection\n",
    "    print(f\"\\nSelected {len(selected_filenames)} file(s) for analysis:\")\n",
    "    for i, (idx, filename) in enumerate(zip(unique_indices, selected_filenames)):\n",
    "        print(f\"  File {i+1}: #{idx} - {filename}\")\n",
    "    \n",
    "    # Set global variables for use in other cells (always as lists)\n",
    "    global selected_filenames_list, selected_filepaths_list, num_replicates\n",
    "    selected_filenames_list = selected_filenames\n",
    "    selected_filepaths_list = selected_filepaths\n",
    "    num_replicates = len(selected_filenames)\n",
    "    \n",
    "    print(f\"\\nNumber of replicates: {num_replicates}\")\n",
    "    if num_replicates == 1:\n",
    "        print(\"Single file analysis - SD values will be 0\")\n",
    "    else:\n",
    "        print(\"Multi-file analysis - averaging with SD calculations\")\n",
    "    \n",
    "    return selected_filenames, selected_filepaths\n",
    "\n",
    "# Set a custom directory (uncomment and modify if needed)\n",
    "# set_data_directory(\"/path/to/your/data\")\n",
    "\n",
    "# List available files\n",
    "print(\"=\" * 80)\n",
    "print(\"NTA DATA ANALYSIS - MULTI-FILE SELECTION\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Current directory: {CONFIG['directory']}\")\n",
    "print()\n",
    "\n",
    "nta_files = find_nta_files()\n",
    "if nta_files:\n",
    "    print(f\"Found {len(nta_files)} NTA files:\")\n",
    "    for i, file in enumerate(nta_files):\n",
    "        print(f\"  {i}: {file}\")\n",
    "    \n",
    "    print(\"\\nTo select files belonging to the same sample for analysis:\")\n",
    "    print(\"  Single file: select_files('0')\")\n",
    "    print(\"  Multiple files: select_files('0,1,2')\")\n",
    "    print(\"  Example: select_files('0,2,5') for files 0, 2, and 5\")\n",
    "else:\n",
    "    print(\"No NTA files found in the current directory.\")\n",
    "    print(\"Please set a different directory using set_data_directory()\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Initialize global variables for file selection (always as lists)\n",
    "selected_filenames_list = None\n",
    "selected_filepaths_list = None\n",
    "num_replicates = 0\n",
    "\n",
    "# Default selection - uncomment to automatically select the first file\n",
    "# if nta_files:\n",
    "#     select_files('0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa30cfa1-4ff8-4ea1-a68a-913dcb8a01ea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Directory not found: Inbox\n",
      "No NTA files found in the current directory.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select_files('4,25,27,29,30')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9a60bf4-6497-432a-ad49-366467681613",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No files selected. Please run Cell 00 (file selection) first.\n",
      "Use: select_files('0,1,2') to select files for analysis.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "NTA Data Analysis - Multi-file I/O Module (Cell 02)\n",
    "\n",
    "This module handles:\n",
    "1. Reading multiple NTA data files\n",
    "2. Identifying key sections in file content\n",
    "3. Validating file structure before processing\n",
    "4. Handling errors gracefully by skipping problematic files\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "\n",
    "\n",
    "def read_nta_file(filepath):\n",
    "    \"\"\"\n",
    "    Read an NTA data file with appropriate encoding.\n",
    "    \n",
    "    Parameters:\n",
    "    filepath (str): Path to the NTA file\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (success_flag, result)\n",
    "        - If successful: (True, file_content)\n",
    "        - If failed: (False, error_message)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='latin1') as file:\n",
    "            content = file.read()\n",
    "        \n",
    "        if not content or len(content) < 100:\n",
    "            return False, f\"File appears to be empty or too small: {filepath}\"\n",
    "        \n",
    "        return True, content\n",
    "    except Exception as e:\n",
    "        return False, f\"Error reading file: {str(e)}\"\n",
    "\n",
    "\n",
    "def identify_sections(content):\n",
    "    \"\"\"\n",
    "    Identify key data sections in the file content.\n",
    "    \n",
    "    Parameters:\n",
    "    content (str): File content to analyze\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (success_flag, result)\n",
    "        - If successful: (True, dict_of_sections)\n",
    "        - If failed: (False, error_message)\n",
    "    \"\"\"\n",
    "    sections = {}\n",
    "    \n",
    "    # Find linear size distribution section\n",
    "    size_lin_start = content.find(\"Size Distribution\")\n",
    "    if size_lin_start == -1:\n",
    "        return False, \"Could not find 'Size Distribution' section for linear data\"\n",
    "    sections['linear_start'] = size_lin_start\n",
    "    \n",
    "    # Find logarithmic data section (starts with -1.000E+0 separator)\n",
    "    size_log_start = content.find(\"-1.000E+0\")\n",
    "    if size_log_start == -1:\n",
    "        # Try alternative approach - look for second \"Size / nm\" header\n",
    "        second_header = content.find(\"Size / nm\", size_lin_start + 100)\n",
    "        if second_header == -1:\n",
    "            return False, \"Could not find logarithmic data section\"\n",
    "        sections['logarithmic_start'] = second_header\n",
    "    else:\n",
    "        sections['logarithmic_start'] = size_log_start\n",
    "    \n",
    "    # Validate section order\n",
    "    if sections['linear_start'] >= sections['logarithmic_start']:\n",
    "        return False, \"Invalid file structure: linear section should come before logarithmic section\"\n",
    "    \n",
    "    return True, sections\n",
    "\n",
    "\n",
    "def validate_file_structure(content, sections):\n",
    "    \"\"\"\n",
    "    Validate file structure to ensure it can be processed.\n",
    "    \n",
    "    Parameters:\n",
    "    content (str): File content\n",
    "    sections (dict): Section positions from identify_sections\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (success_flag, message)\n",
    "    \"\"\"\n",
    "    # Check linear section has expected header pattern\n",
    "    linear_section = content[sections['linear_start']:sections['logarithmic_start']]\n",
    "    if not re.search(r'Size / nm\\s+Number\\s+Concentration', linear_section):\n",
    "        return False, \"Missing expected header pattern in linear section\"\n",
    "    \n",
    "    # Check logarithmic section has data in expected format\n",
    "    log_section = content[sections['logarithmic_start']:]\n",
    "    if not re.search(r'[\\d.-]+E[\\+\\-]\\d+\\s+[\\d.-]+E[\\+\\-]\\d+', log_section):\n",
    "        return False, \"Could not find data rows in logarithmic section\"\n",
    "    \n",
    "    return True, \"File structure is valid\"\n",
    "\n",
    "\n",
    "def process_single_file_content(filepath):\n",
    "    \"\"\"\n",
    "    Complete file processing workflow for a single file.\n",
    "    \n",
    "    Parameters:\n",
    "    filepath (str): Path to the NTA file\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (success_flag, result)\n",
    "        - If successful: (True, (content, sections))\n",
    "        - If failed: (False, error_message)\n",
    "    \"\"\"\n",
    "    # Read the file\n",
    "    success, result = read_nta_file(filepath)\n",
    "    if not success:\n",
    "        return False, result\n",
    "    content = result\n",
    "    \n",
    "    # Identify sections\n",
    "    success, result = identify_sections(content)\n",
    "    if not success:\n",
    "        return False, result\n",
    "    sections = result\n",
    "    \n",
    "    # Validate structure\n",
    "    success, message = validate_file_structure(content, sections)\n",
    "    if not success:\n",
    "        return False, message\n",
    "    \n",
    "    return True, (content, sections)\n",
    "\n",
    "\n",
    "def process_multiple_file_contents(filepaths):\n",
    "    \"\"\"\n",
    "    Process multiple NTA files, skipping any that fail with error messages.\n",
    "    \n",
    "    Parameters:\n",
    "    filepaths (list): List of file paths to process\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (file_data_list, error_summary)\n",
    "        - file_data_list: List of (filename, content, sections) for successful files\n",
    "        - error_summary: Dict with failed files and their error messages\n",
    "    \"\"\"\n",
    "    successful_files = []\n",
    "    failed_files = {}\n",
    "    \n",
    "    print(f\"Processing {len(filepaths)} file(s)...\")\n",
    "    \n",
    "    for filepath in filepaths:\n",
    "        filename = os.path.basename(filepath)\n",
    "        print(f\"  Processing: {filename}\")\n",
    "        \n",
    "        success, result = process_single_file_content(filepath)\n",
    "        \n",
    "        if success:\n",
    "            content, sections = result\n",
    "            successful_files.append((filename, content, sections))\n",
    "            print(f\"    ‚úì Success\")\n",
    "        else:\n",
    "            failed_files[filename] = result\n",
    "            print(f\"    ‚úó Failed: {result}\")\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\nProcessing complete:\")\n",
    "    print(f\"  Successful: {len(successful_files)} files\")\n",
    "    print(f\"  Failed: {len(failed_files)} files\")\n",
    "    \n",
    "    if failed_files:\n",
    "        print(\"\\nFailed files:\")\n",
    "        for filename, error in failed_files.items():\n",
    "            print(f\"  {filename}: {error}\")\n",
    "    \n",
    "    return successful_files, failed_files\n",
    "\n",
    "\n",
    "def preview_file_content(content, max_chars=500):\n",
    "    \"\"\"\n",
    "    Display a preview of the file content to verify it was read correctly.\n",
    "    \n",
    "    Parameters:\n",
    "    content (str): File content to preview\n",
    "    max_chars (int): Maximum number of characters to display\n",
    "    \n",
    "    Returns:\n",
    "    str: Preview text for display\n",
    "    \"\"\"\n",
    "    preview = content[:max_chars]\n",
    "    return \"-\" * 50 + \"\\n\" + preview + \"\\n\" + \"-\" * 50\n",
    "\n",
    "\n",
    "# Execute file processing if files were selected in Cell 00\n",
    "if 'selected_filepaths_list' in globals() and selected_filepaths_list:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"PROCESSING SELECTED NTA FILES\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Process all selected files\n",
    "    successful_files, failed_files = process_multiple_file_contents(selected_filepaths_list)\n",
    "    \n",
    "    if successful_files:\n",
    "        print(f\"\\nSuccessfully processed {len(successful_files)} file(s):\")\n",
    "        for filename, content, sections in successful_files:\n",
    "            print(f\"  {filename}\")\n",
    "            print(f\"    Linear section at position: {sections['linear_start']}\")\n",
    "            print(f\"    Logarithmic section at position: {sections['logarithmic_start']}\")\n",
    "        \n",
    "        # Show preview of first file\n",
    "        if successful_files:\n",
    "            first_filename, first_content, first_sections = successful_files[0]\n",
    "            print(f\"\\nFile Preview ({first_filename}):\")\n",
    "            print(preview_file_content(first_content))\n",
    "        \n",
    "        # Store results for use in subsequent cells\n",
    "        current_files_data = successful_files\n",
    "        current_failed_files = failed_files\n",
    "        \n",
    "        print(\"\\nFile processing completed successfully!\")\n",
    "        print(\"Ready for data extraction and averaging (Cell 03)\")\n",
    "        \n",
    "    else:\n",
    "        print(\"\\nERROR: No files could be processed successfully.\")\n",
    "        if failed_files:\n",
    "            print(\"All files failed with errors (see details above).\")\n",
    "        \n",
    "else:\n",
    "    print(\"No files selected. Please run Cell 00 (file selection) first.\")\n",
    "    print(\"Use: select_files('0,1,2') to select files for analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3155e92e-c640-4fa6-a83a-92a708abf155",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No processed files found. Please run Cell 02 (file processing) first.\n",
      "Make sure to run Cell 00 (file selection) before Cell 02.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "NTA Data Analysis - Data Extraction and Averaging Module (Cell 03)\n",
    "\n",
    "This module handles:\n",
    "1. Extracting distribution data from multiple NTA files\n",
    "2. Averaging raw particle counts bin-by-bin across replicates\n",
    "3. Calculating standard deviations for each size bin\n",
    "4. Creating a single averaged dataset for downstream analysis\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "\n",
    "def extract_data_section(content, start_pos, end_pos=None, is_log_section=False):\n",
    "    \"\"\"\n",
    "    Extract tabular data from a section of the file content.\n",
    "    \n",
    "    Parameters:\n",
    "    content (str): File content\n",
    "    start_pos (int): Starting position of the section\n",
    "    end_pos (int, optional): Ending position of the section\n",
    "    is_log_section (bool): Whether this is the logarithmic section\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (success_flag, result)\n",
    "        - If successful: (True, (header, data_lines))\n",
    "        - If failed: (False, error_message)\n",
    "    \"\"\"\n",
    "    # Extract the section\n",
    "    section = content[start_pos:end_pos]\n",
    "    \n",
    "    # For logarithmic section, skip past the separator line\n",
    "    if is_log_section:\n",
    "        sep_pos = section.find(\"-1.000E+0\")\n",
    "        if sep_pos != -1:\n",
    "            sep_line_end = section.find('\\n', sep_pos)\n",
    "            if sep_line_end != -1:\n",
    "                section = section[sep_line_end + 1:]\n",
    "            else:\n",
    "                section = section[sep_pos + 20:]\n",
    "        \n",
    "        # Sometimes there's a second \"Size Distribution\" header\n",
    "        second_header = section.find(\"Size Distribution\")\n",
    "        if second_header != -1:\n",
    "            section = section[second_header:]\n",
    "    \n",
    "    # Find the header line\n",
    "    header_match = re.search(r'Size / nm\\s+Number\\s+Concentration.+', section)\n",
    "    \n",
    "    if not header_match and is_log_section:\n",
    "        # For log section, try more relaxed pattern or use default\n",
    "        header_match = re.search(r'Size.*Number', section)\n",
    "        if not header_match:\n",
    "            header_line = \"Size / nm\\tNumber\\tConcentration / cm-3\\tVolume / nm^3\\tArea / nm^2\"\n",
    "            data_lines = []\n",
    "            for line in section.split('\\n'):\n",
    "                if \"-1.000E+0\" in line:\n",
    "                    continue\n",
    "                if re.match(r'^\\s*[\\d.-]+E[\\+\\-]\\d+\\s+[\\d.-]+E[\\+\\-]\\d+', line):\n",
    "                    data_lines.append(line)\n",
    "            \n",
    "            if not data_lines:\n",
    "                return False, \"Could not find any valid data lines in logarithmic section\"\n",
    "            return True, (header_line, data_lines)\n",
    "        else:\n",
    "            header_line = header_match.group(0)\n",
    "            data_start = section.find(header_line) + len(header_line)\n",
    "            data_section = section[data_start:]\n",
    "    else:\n",
    "        if header_match:\n",
    "            header_line = header_match.group(0)\n",
    "            data_start = section.find(header_line) + len(header_line)\n",
    "            data_section = section[data_start:]\n",
    "        else:\n",
    "            return False, \"Could not find header line in data section\"\n",
    "    \n",
    "    # Extract data lines\n",
    "    all_lines = data_section.split('\\n')\n",
    "    data_lines = []\n",
    "    \n",
    "    for line in all_lines:\n",
    "        if \"-1.000E+0\" in line:\n",
    "            continue\n",
    "        if re.match(r'^\\s*[\\d.-]+E[\\+\\-]\\d+\\s+[\\d.-]+E[\\+\\-]\\d+', line):\n",
    "            data_lines.append(line)\n",
    "        elif len(data_lines) > 0 and (line.strip() == '' or line.strip().startswith('-')):\n",
    "            break\n",
    "    \n",
    "    if not data_lines:\n",
    "        return False, \"No data lines found in section\"\n",
    "    \n",
    "    return True, (header_line, data_lines)\n",
    "\n",
    "\n",
    "def parse_data_lines(header_line, data_lines, scale_type):\n",
    "    \"\"\"\n",
    "    Parse data lines into a structured DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    header_line (str): Header line with column names\n",
    "    data_lines (list): List of data lines to parse\n",
    "    scale_type (str): Type of scale ('linear' or 'logarithmic')\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (success_flag, result)\n",
    "        - If successful: (True, DataFrame)\n",
    "        - If failed: (False, error_message)\n",
    "    \"\"\"\n",
    "    parsed_data = []\n",
    "    \n",
    "    for line in data_lines:\n",
    "        if \"-1.000E+0\" in line:\n",
    "            continue\n",
    "            \n",
    "        values = re.findall(r'[\\d.-]+E[\\+\\-]\\d+', line)\n",
    "        \n",
    "        if len(values) >= 5:\n",
    "            try:\n",
    "                if values[0] == \"-1.000E+0\":\n",
    "                    continue\n",
    "                \n",
    "                row = {\n",
    "                    'size_nm': float(values[0]),\n",
    "                    'number': float(values[1]),\n",
    "                    'concentration_cm-3': float(values[2]),\n",
    "                    'volume_nm^3': float(values[3]),\n",
    "                    'area_nm^2': float(values[4]),\n",
    "                    'scale': scale_type\n",
    "                }\n",
    "                parsed_data.append(row)\n",
    "            except ValueError as e:\n",
    "                print(f\"Error parsing line: {e}\")\n",
    "                continue\n",
    "    \n",
    "    if not parsed_data:\n",
    "        return False, f\"Failed to parse any data lines for {scale_type} scale\"\n",
    "    \n",
    "    return True, pd.DataFrame(parsed_data)\n",
    "\n",
    "\n",
    "def extract_single_file_distribution(content, sections, filename):\n",
    "    \"\"\"\n",
    "    Extract distribution data from a single file.\n",
    "    \n",
    "    Parameters:\n",
    "    content (str): File content\n",
    "    sections (dict): Dictionary with section positions\n",
    "    filename (str): Filename for reference\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (success_flag, result)\n",
    "        - If successful: (True, combined_df)\n",
    "        - If failed: (False, error_message)\n",
    "    \"\"\"\n",
    "    # Extract linear data\n",
    "    lin_start = sections['linear_start']\n",
    "    log_start = sections['logarithmic_start']\n",
    "    \n",
    "    success, lin_result = extract_data_section(content, lin_start, log_start, is_log_section=False)\n",
    "    if not success:\n",
    "        return False, f\"Failed to extract linear data from {filename}: {lin_result}\"\n",
    "    \n",
    "    lin_header, lin_data_lines = lin_result\n",
    "    success, lin_df = parse_data_lines(lin_header, lin_data_lines, 'linear')\n",
    "    if not success:\n",
    "        return False, f\"Failed to parse linear data from {filename}: {lin_df}\"\n",
    "    \n",
    "    # Extract logarithmic data\n",
    "    success, log_result = extract_data_section(content, log_start, is_log_section=True)\n",
    "    if not success:\n",
    "        print(f\"Warning: Could not extract logarithmic data from {filename}: {log_result}\")\n",
    "        combined_df = lin_df.copy()\n",
    "    else:\n",
    "        log_header, log_data_lines = log_result\n",
    "        success, log_df = parse_data_lines(log_header, log_data_lines, 'logarithmic')\n",
    "        if not success:\n",
    "            print(f\"Warning: Could not parse logarithmic data from {filename}: {log_df}\")\n",
    "            combined_df = lin_df.copy()\n",
    "        else:\n",
    "            combined_df = pd.concat([lin_df, log_df], ignore_index=True)\n",
    "    \n",
    "    # Add filename for tracking\n",
    "    combined_df['source_file'] = filename\n",
    "    \n",
    "    return True, combined_df\n",
    "\n",
    "\n",
    "def average_replicate_data(dataframes_list, filenames_list):\n",
    "    \"\"\"\n",
    "    Average distribution data across multiple replicates bin-by-bin.\n",
    "    \n",
    "    Parameters:\n",
    "    dataframes_list (list): List of DataFrames from individual files\n",
    "    filenames_list (list): List of filenames for reference\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (success_flag, result)\n",
    "        - If successful: (True, averaged_df)\n",
    "        - If failed: (False, error_message)\n",
    "    \"\"\"\n",
    "    if not dataframes_list:\n",
    "        return False, \"No dataframes to average\"\n",
    "    \n",
    "    if len(dataframes_list) == 1:\n",
    "        # Single file case - add SD columns with zeros\n",
    "        df = dataframes_list[0].copy()\n",
    "        \n",
    "        # Add SD columns (all zeros for single file)\n",
    "        df['number_sd'] = 0.0\n",
    "        df['concentration_cm-3_sd'] = 0.0\n",
    "        df['volume_nm^3_sd'] = 0.0\n",
    "        df['area_nm^2_sd'] = 0.0\n",
    "        \n",
    "        # Rename value columns to indicate they're averages\n",
    "        df.rename(columns={\n",
    "            'number': 'number_avg',\n",
    "            'concentration_cm-3': 'concentration_cm-3_avg',\n",
    "            'volume_nm^3': 'volume_nm^3_avg',\n",
    "            'area_nm^2': 'area_nm^2_avg'\n",
    "        }, inplace=True)\n",
    "        \n",
    "        # Add replicate info\n",
    "        df['num_replicates'] = 1\n",
    "        df['source_files'] = filenames_list[0]\n",
    "        \n",
    "        print(f\"Single file analysis: {filenames_list[0]}\")\n",
    "        print(\"Standard deviation values set to 0\")\n",
    "        \n",
    "        return True, df\n",
    "    \n",
    "    # Multiple files case - perform averaging\n",
    "    print(f\"Averaging data from {len(dataframes_list)} files:\")\n",
    "    for filename in filenames_list:\n",
    "        print(f\"  {filename}\")\n",
    "    \n",
    "    # Process each scale separately\n",
    "    averaged_dfs = []\n",
    "    \n",
    "    for scale in ['linear', 'logarithmic']:\n",
    "        scale_dfs = []\n",
    "        for df in dataframes_list:\n",
    "            scale_data = df[df['scale'] == scale]\n",
    "            if not scale_data.empty:\n",
    "                scale_dfs.append(scale_data)\n",
    "        \n",
    "        if not scale_dfs:\n",
    "            continue\n",
    "        \n",
    "        # Find common size bins across all files\n",
    "        all_sizes = []\n",
    "        for df in scale_dfs:\n",
    "            all_sizes.extend(df['size_nm'].values)\n",
    "        unique_sizes = sorted(set(all_sizes))\n",
    "        \n",
    "        # Create averaged data for this scale\n",
    "        averaged_data = []\n",
    "        \n",
    "        for size in unique_sizes:\n",
    "            # Collect values for this size across all files\n",
    "            numbers = []\n",
    "            concentrations = []\n",
    "            volumes = []\n",
    "            areas = []\n",
    "            \n",
    "            for df in scale_dfs:\n",
    "                size_row = df[df['size_nm'] == size]\n",
    "                if not size_row.empty:\n",
    "                    numbers.append(size_row['number'].iloc[0])\n",
    "                    concentrations.append(size_row['concentration_cm-3'].iloc[0])\n",
    "                    volumes.append(size_row['volume_nm^3'].iloc[0])\n",
    "                    areas.append(size_row['area_nm^2'].iloc[0])\n",
    "            \n",
    "            # Calculate averages and standard deviations\n",
    "            if numbers:  # Only if we have data for this size\n",
    "                row = {\n",
    "                    'size_nm': size,\n",
    "                    'number_avg': np.mean(numbers),\n",
    "                    'number_sd': np.std(numbers, ddof=1) if len(numbers) > 1 else 0.0,\n",
    "                    'concentration_cm-3_avg': np.mean(concentrations),\n",
    "                    'concentration_cm-3_sd': np.std(concentrations, ddof=1) if len(concentrations) > 1 else 0.0,\n",
    "                    'volume_nm^3_avg': np.mean(volumes),\n",
    "                    'volume_nm^3_sd': np.std(volumes, ddof=1) if len(volumes) > 1 else 0.0,\n",
    "                    'area_nm^2_avg': np.mean(areas),\n",
    "                    'area_nm^2_sd': np.std(areas, ddof=1) if len(areas) > 1 else 0.0,\n",
    "                    'scale': scale,\n",
    "                    'num_replicates': len(numbers),\n",
    "                    'source_files': '; '.join(filenames_list)\n",
    "                }\n",
    "                averaged_data.append(row)\n",
    "        \n",
    "        if averaged_data:\n",
    "            scale_df = pd.DataFrame(averaged_data)\n",
    "            averaged_dfs.append(scale_df)\n",
    "    \n",
    "    if not averaged_dfs:\n",
    "        return False, \"No data could be averaged\"\n",
    "    \n",
    "    # Combine linear and logarithmic scales\n",
    "    final_df = pd.concat(averaged_dfs, ignore_index=True)\n",
    "    \n",
    "    # Sort by scale and size\n",
    "    final_df = final_df.sort_values(['scale', 'size_nm']).reset_index(drop=True)\n",
    "    \n",
    "    print(f\"Averaging completed:\")\n",
    "    print(f\"  Total size bins: {len(final_df)}\")\n",
    "    print(f\"  Linear scale bins: {len(final_df[final_df['scale'] == 'linear'])}\")\n",
    "    print(f\"  Log scale bins: {len(final_df[final_df['scale'] == 'logarithmic'])}\")\n",
    "    \n",
    "    return True, final_df\n",
    "\n",
    "\n",
    "# Execute data extraction and averaging if files were processed in Cell 02\n",
    "if 'current_files_data' in globals() and current_files_data:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"EXTRACTING AND AVERAGING DISTRIBUTION DATA\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Extract data from each file\n",
    "    individual_dataframes = []\n",
    "    filenames = []\n",
    "    extraction_errors = []\n",
    "    \n",
    "    for filename, content, sections in current_files_data:\n",
    "        print(f\"Extracting data from: {filename}\")\n",
    "        \n",
    "        success, result = extract_single_file_distribution(content, sections, filename)\n",
    "        \n",
    "        if success:\n",
    "            individual_dataframes.append(result)\n",
    "            filenames.append(filename)\n",
    "            print(f\"  ‚úì Extracted {len(result)} data points\")\n",
    "        else:\n",
    "            extraction_errors.append((filename, result))\n",
    "            print(f\"  ‚úó Failed: {result}\")\n",
    "    \n",
    "    if individual_dataframes:\n",
    "        # Average the data across replicates\n",
    "        print(f\"\\nAveraging data from {len(individual_dataframes)} successful file(s)...\")\n",
    "        \n",
    "        success, averaged_df = average_replicate_data(individual_dataframes, filenames)\n",
    "        \n",
    "        if success:\n",
    "            # Store results for downstream cells (mimicking old format)\n",
    "            current_distribution_df = averaged_df\n",
    "            current_file_content = None  # Not applicable for averaged data\n",
    "            current_file_sections = None  # Not applicable for averaged data\n",
    "            \n",
    "            # Add uniqueID based on first filename\n",
    "            first_filename = filenames[0]\n",
    "            # Extract base name for uniqueID\n",
    "            base_name = first_filename.replace('_rawdata.txt', '').replace('.txt', '')\n",
    "            if base_name.startswith('Data_'):\n",
    "                base_name = base_name[5:]\n",
    "            \n",
    "            # For multiple files, add indication\n",
    "            if len(filenames) > 1:\n",
    "                uniqueID = f\"{base_name}_avg{len(filenames)}\"\n",
    "            else:\n",
    "                uniqueID = base_name\n",
    "            \n",
    "            current_distribution_df['uniqueID'] = uniqueID\n",
    "            \n",
    "            print(f\"\\nData extraction and averaging completed successfully!\")\n",
    "            print(f\"Dataset ID: {uniqueID}\")\n",
    "            print(f\"Ready for metadata extraction (Cell 04)\")\n",
    "            \n",
    "            # Display preview\n",
    "            print(f\"\\nAveraged Data Preview:\")\n",
    "            display(current_distribution_df.head())\n",
    "            \n",
    "        else:\n",
    "            print(f\"\\nERROR: Failed to average data: {averaged_df}\")\n",
    "    \n",
    "    else:\n",
    "        print(f\"\\nERROR: No files could be processed for data extraction.\")\n",
    "        if extraction_errors:\n",
    "            print(\"Extraction errors:\")\n",
    "            for filename, error in extraction_errors:\n",
    "                print(f\"  {filename}: {error}\")\n",
    "\n",
    "else:\n",
    "    print(\"No processed files found. Please run Cell 02 (file processing) first.\")\n",
    "    print(\"Make sure to run Cell 00 (file selection) before Cell 02.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55b15e38-6bcd-438c-a9d3-dcd7735c608f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No processed files found. Please run Cell 02 and Cell 03 first.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "NTA Data Analysis - Automated Modular Metadata System (Cell 04)\n",
    "\n",
    "This module handles:\n",
    "1. Automated extraction of ALL metadata fields from multiple files\n",
    "2. Automatic detection of differences vs. identical values\n",
    "3. Smart array creation for differing values\n",
    "4. Detailed conflict reporting for analysis\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import ntpath  \n",
    "from datetime import date\n",
    "\n",
    "\n",
    "def extract_all_metadata_fields(content, filename):\n",
    "    \"\"\"\n",
    "    Extract ALL possible metadata fields from file content using comprehensive regex patterns.\n",
    "    \n",
    "    Parameters:\n",
    "    content (str): File content to extract metadata from\n",
    "    filename (str): Original filename for reference\n",
    "    \n",
    "    Returns:\n",
    "    dict: Dictionary with all found metadata fields\n",
    "    \"\"\"\n",
    "    # Comprehensive regex patterns for ALL possible NTA metadata fields\n",
    "    metadata_patterns = [\n",
    "        ('original_file', r'Original File:\\s+(.+?)(?:\\s+Section:|$)'),\n",
    "        ('section', r'Section:\\s+(.+)'),\n",
    "        ('operator', r'Operator:\\s+(.+)'),\n",
    "        ('experiment', r'Experiment:\\s+(.+)'),\n",
    "        ('zetaview_sn', r'ZetaView S/N:\\s+(.+)'),\n",
    "        ('cell_sn', r'Cell S/N:\\s+(.+)'),\n",
    "        ('software', r'Software:\\s+(.+?)(?:\\s+Analyze:|$)'),\n",
    "        ('analyze', r'Analyze:\\s+(.+)'),\n",
    "        ('sop', r'SOP:\\s+(.+)'),\n",
    "        ('sample', r'Sample:\\s+(.+)'),\n",
    "        ('electrolyte', r'Electrolyte:(?:\\s*(.*?))?(?:\\r?\\n|$)'),\n",
    "        ('ph', r'pH:\\s+(.+?)(?:\\s+entered|$)'),\n",
    "        ('conductivity', r'Conductivity:\\s+(.+?)(?:\\s+sensed|$)'),\n",
    "        ('temp_control', r'TempControl:\\s+(.+)'),\n",
    "        ('set_temperature', r'SetTemperature:\\s+(.+)'),\n",
    "        ('temperature', r'Temperature:\\s+(.+?)(?:\\s+sensed|$)'),\n",
    "        ('viscosity', r'Viscosity:\\s+(.+)'),\n",
    "        ('date', r'Date:\\s+(.+)'),\n",
    "        ('time', r'Time:\\s+(.+)'),\n",
    "        ('general_remarks', r'General Remarks:\\s+(.+)'),\n",
    "        ('remarks', r'Remarks:\\s+(.+)'),\n",
    "        ('sample_info_1', r'Sample Info 1:\\s+(.+)'),\n",
    "        ('sample_info_2', r'Sample Info 2:\\s+(.+)'),\n",
    "        ('sample_info_3', r'Sample Info 3:\\s*(.*)'),\n",
    "        ('scattering_intensity', r'Scattering Intensity:\\s+(.+)'),\n",
    "        ('detected_particles', r'Detected Particles:\\s+(.+)'),\n",
    "        ('particle_drift_checked', r'Particle Drift Checked:\\s+(.+)'),\n",
    "        ('particle_drift_check_result', r'Particle Drift Check Result:\\s+(.+)'),\n",
    "        ('cell_check_date', r'Cell Checked:\\s+(\\d{4}-\\d{2}-\\d{2})'),\n",
    "        ('cell_check_result', r'Cell Check Result:\\s+(.+)'),\n",
    "        ('type_of_measurement', r'Type of Measurement:\\s+(.+)'),\n",
    "        ('positions', r'Positions:\\s+(.+)'),\n",
    "        ('microscope_position', r'Microscope Position:\\s+(.+)'),\n",
    "        ('number_of_traces', r'Number of Traces:\\s+(\\d+)'),\n",
    "        ('average_number_of_particles', r'Average Number of Particles:\\s+(\\d+\\.\\d+)'),\n",
    "        ('dilution', r'Dilution::\\s+(\\d+\\.\\d+)'),\n",
    "        ('concentration_correction_factor', r'Concentration Correction Factor:\\s+(.+)'),\n",
    "        ('laser_wavelength', r'Laser Wavelength nm:\\s+(\\d+\\.\\d+)'),\n",
    "        ('median_number_d50', r'Median Number \\(D50\\):\\s+(.+)'),\n",
    "        ('median_concentration_d50', r'Median Concentration \\(D50\\):\\s+(.+)'),\n",
    "        ('median_volume_d50', r'Median Volume \\(D50\\):\\s+(.+)'),\n",
    "        ('minimum_brightness', r'Minimum Brightness:\\s+(\\d+)'),\n",
    "        ('minimum_area', r'Minimum Area:\\s+(\\d+)'),\n",
    "        ('maximum_area', r'Maximum Area:\\s+(\\d+)'),\n",
    "        ('maximum_brightness', r'Maximum Brightness:\\s+(\\d+)'),\n",
    "        ('tracking_radius2', r'Tracking Radius2:\\s+(\\d+)'),\n",
    "        ('minimum_tracelength', r'Minimum Tracelength:\\s+(\\d+)'),\n",
    "        ('fps', r'Camera:\\s*FpSec\\s+(\\d+)\\s+#Cycles'),\n",
    "        ('cycles', r'#Cycles\\s+(\\d+)'),\n",
    "        # Additional patterns for more comprehensive extraction\n",
    "        ('camera_settings', r'Camera:\\s+(.+)'),\n",
    "        ('frame_rate', r'FRate\\s+(\\d+\\.\\d+)'),\n",
    "        ('auto_settings', r'Auto:\\s+(.+)'),\n",
    "        ('sensitivity', r'Sensitivity:\\s+(.+)'),\n",
    "        ('shutter', r'Shutter:\\s+(.+)'),\n",
    "        ('gain', r'Gain:\\s+(.+)'),\n",
    "    ]\n",
    "    \n",
    "    # Extract metadata\n",
    "    metadata = {}\n",
    "    \n",
    "    for key, pattern in metadata_patterns:\n",
    "        match = re.search(pattern, content)\n",
    "        if match:\n",
    "            value = match.group(1).strip() if match.group(1) else ''\n",
    "            # Clean up common issues\n",
    "            if value and not value.lower() in ['none', 'null', '']:\n",
    "                metadata[key] = value\n",
    "    \n",
    "    # Add filename and derived fields\n",
    "    metadata['filename'] = filename\n",
    "    \n",
    "    # Extract unique ID from filename\n",
    "    base_name = os.path.splitext(filename)[0]\n",
    "    if base_name.endswith(\"_rawdata\"):\n",
    "        base_name = base_name[:-8]\n",
    "    if base_name.startswith(\"Data_\"):\n",
    "        base_name = base_name[5:]\n",
    "    metadata['uniqueID'] = base_name\n",
    "    \n",
    "    # Try to find AVI file information\n",
    "    original_file = metadata.get('original_file', '')\n",
    "    if original_file:\n",
    "        avi_filename = ntpath.basename(original_file)\n",
    "        metadata['avi_filename'] = avi_filename\n",
    "        \n",
    "        # Try to find AVI file size if file exists\n",
    "        if 'CONFIG' in globals():\n",
    "            directory = CONFIG.get('directory', '')\n",
    "            full_avi_path = os.path.join(directory, avi_filename)\n",
    "            if os.path.exists(full_avi_path) and os.path.isfile(full_avi_path):\n",
    "                size_bytes = os.path.getsize(full_avi_path)\n",
    "                metadata['avi_filesize'] = f\"{size_bytes / (1024 * 1024):.2f} MB\"\n",
    "    \n",
    "    return metadata\n",
    "\n",
    "\n",
    "def extract_metadata_from_all_files(files_data):\n",
    "    \"\"\"\n",
    "    Extract metadata from all files and organize by filename.\n",
    "    \n",
    "    Parameters:\n",
    "    files_data (list): List of (filename, content, sections) tuples\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (success_flag, result)\n",
    "        - If successful: (True, all_files_metadata_dict)\n",
    "        - If failed: (False, error_message)\n",
    "    \"\"\"\n",
    "    if not files_data:\n",
    "        return False, \"No files provided for metadata extraction\"\n",
    "    \n",
    "    all_files_metadata = {}\n",
    "    \n",
    "    print(f\"Extracting ALL metadata fields from {len(files_data)} file(s):\")\n",
    "    \n",
    "    for filename, content, sections in files_data:\n",
    "        print(f\"  Processing: {filename}\")\n",
    "        \n",
    "        metadata = extract_all_metadata_fields(content, filename)\n",
    "        all_files_metadata[filename] = metadata\n",
    "        \n",
    "        print(f\"    ‚úì Extracted {len(metadata)} metadata fields\")\n",
    "    \n",
    "    print(f\"\\nExtracted metadata from {len(all_files_metadata)} files\")\n",
    "    return True, all_files_metadata\n",
    "\n",
    "\n",
    "def analyze_field_differences(all_files_metadata):\n",
    "    \"\"\"\n",
    "    Analyze which fields are identical vs. different across files.\n",
    "    \n",
    "    Parameters:\n",
    "    all_files_metadata (dict): Dictionary of {filename: metadata_dict}\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (identical_fields, different_fields, field_analysis)\n",
    "    \"\"\"\n",
    "    if not all_files_metadata:\n",
    "        return {}, {}, {}\n",
    "    \n",
    "    # Get all unique field names across all files\n",
    "    all_field_names = set()\n",
    "    for metadata in all_files_metadata.values():\n",
    "        all_field_names.update(metadata.keys())\n",
    "    \n",
    "    identical_fields = {}\n",
    "    different_fields = {}\n",
    "    field_analysis = {}\n",
    "    \n",
    "    print(f\"\\nAnalyzing {len(all_field_names)} unique metadata fields:\")\n",
    "    \n",
    "    for field_name in sorted(all_field_names):\n",
    "        # Collect values for this field from all files\n",
    "        values = []\n",
    "        files_with_field = []\n",
    "        \n",
    "        for filename, metadata in all_files_metadata.items():\n",
    "            if field_name in metadata:\n",
    "                values.append(metadata[field_name])\n",
    "                files_with_field.append(filename)\n",
    "        \n",
    "        # Analyze this field\n",
    "        field_info = {\n",
    "            'values': values,\n",
    "            'files_with_field': files_with_field,\n",
    "            'present_in_files': len(files_with_field),\n",
    "            'total_files': len(all_files_metadata)\n",
    "        }\n",
    "        \n",
    "        if len(set(values)) == 1:\n",
    "            # All values are identical\n",
    "            identical_fields[field_name] = values[0]\n",
    "            field_info['status'] = 'identical'\n",
    "        else:\n",
    "            # Values differ\n",
    "            different_fields[field_name] = values\n",
    "            field_info['status'] = 'different'\n",
    "        \n",
    "        field_analysis[field_name] = field_info\n",
    "    \n",
    "    # Report concise summary\n",
    "    print(f\"  Identical fields: {len(identical_fields)}\")\n",
    "    print(f\"  Different fields: {len(different_fields)}\")\n",
    "    \n",
    "    # Only show alerts, not all differences\n",
    "    quality_issues = []\n",
    "    for field_name, values in different_fields.items():\n",
    "        if field_name == 'particle_drift_check_result':\n",
    "            unique_values = list(set(values))\n",
    "            if len(unique_values) > 1:\n",
    "                quality_issues.append(f\"QC Alert - {field_name}: {values}\")\n",
    "    \n",
    "    if quality_issues:\n",
    "        print(f\"\\nQuality Control Issues Detected:\")\n",
    "        for issue in quality_issues:\n",
    "            print(f\"  {issue}\")\n",
    "    \n",
    "    return identical_fields, different_fields, field_analysis\n",
    "\n",
    "\n",
    "def smart_format_field(field_name, values):\n",
    "    \"\"\"\n",
    "    Apply smart formatting rules based on field type and content.\n",
    "    \n",
    "    Parameters:\n",
    "    field_name (str): Name of the metadata field\n",
    "    values (list): List of values from different files\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (formatted_value, notes)\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    \n",
    "    # File-specific info - keep as arrays\n",
    "    file_specific_fields = [\n",
    "        'filename', 'avi_filename', 'experiment', 'original_file', 'uniqueID', \n",
    "        'time', 'particle_drift_checked'\n",
    "    ]\n",
    "    \n",
    "    # Text fields - use first value only\n",
    "    text_first_fields = [\n",
    "        'sample_info_1', 'sample_info_2', 'sample_info_3', 'remarks', 'general_remarks'\n",
    "    ]\n",
    "    \n",
    "    # Sum fields - add them up\n",
    "    sum_fields = [\n",
    "        'number_of_traces', 'detected_particles'\n",
    "    ]\n",
    "    \n",
    "    # Instrument-determined fields - keep as arrays with note\n",
    "    instrument_determined_fields = [\n",
    "        'median_number_d50', 'median_concentration_d50', 'median_volume_d50'\n",
    "    ]\n",
    "    \n",
    "    # Quality control fields - flag differences\n",
    "    quality_control_fields = [\n",
    "        'particle_drift_check_result', 'cell_check_result'\n",
    "    ]\n",
    "    \n",
    "    notes = \"\"\n",
    "    \n",
    "    if field_name in file_specific_fields:\n",
    "        # Keep as JSON array\n",
    "        return json.dumps(values), \"file_specific\"\n",
    "    \n",
    "    elif field_name in text_first_fields:\n",
    "        # Use first value only\n",
    "        return values[0], f\"using_first_of_{len(values)}\"\n",
    "    \n",
    "    elif field_name in sum_fields:\n",
    "        # Sum all values\n",
    "        try:\n",
    "            numeric_values = [float(v) for v in values]\n",
    "            total = sum(numeric_values)\n",
    "            return f\"{total:.0f}\", f\"sum_of_{len(values)}_measurements\"\n",
    "        except ValueError:\n",
    "            return json.dumps(values), \"non_numeric_sum_field\"\n",
    "    \n",
    "    elif field_name in instrument_determined_fields:\n",
    "        # Keep as array with explanatory note\n",
    "        return json.dumps(values), \"instrument_determined_per_measurement\"\n",
    "    \n",
    "    elif field_name in quality_control_fields:\n",
    "        # Check if all values are the same\n",
    "        unique_values = list(set(values))\n",
    "        if len(unique_values) == 1:\n",
    "            return values[0], \"qc_consistent\"\n",
    "        else:\n",
    "            return json.dumps(values), f\"QC_ALERT_inconsistent_values\"\n",
    "    \n",
    "    else:\n",
    "        # Try to calculate mean ¬± SD for numeric fields\n",
    "        try:\n",
    "            # Special handling for file sizes with units\n",
    "            if field_name == 'avi_filesize':\n",
    "                # Extract numeric values from strings like \"219.24 MB\"\n",
    "                numeric_values = []\n",
    "                for v in values:\n",
    "                    if isinstance(v, str) and 'MB' in v:\n",
    "                        numeric_values.append(float(v.replace(' MB', '')))\n",
    "                    else:\n",
    "                        numeric_values.append(float(v))\n",
    "                \n",
    "                mean_val = np.mean(numeric_values)\n",
    "                std_val = np.std(numeric_values, ddof=1) if len(numeric_values) > 1 else 0.0\n",
    "                \n",
    "                cv = (std_val / mean_val * 100) if mean_val != 0 else 0\n",
    "                if cv > 10:\n",
    "                    notes = f\"HIGH_VARIATION_CV_{cv:.1f}%\"\n",
    "                else:\n",
    "                    notes = f\"mean_sd_of_{len(values)}\"\n",
    "                \n",
    "                return f\"{mean_val:.2f} ¬± {std_val:.2f} MB\", notes\n",
    "            \n",
    "            # Regular numeric processing for other fields\n",
    "            numeric_values = [float(v) for v in values]\n",
    "            mean_val = np.mean(numeric_values)\n",
    "            std_val = np.std(numeric_values, ddof=1) if len(numeric_values) > 1 else 0.0\n",
    "            \n",
    "            # Check for concerning variations\n",
    "            cv = (std_val / mean_val * 100) if mean_val != 0 else 0\n",
    "            \n",
    "            if cv > 10:  # More than 10% coefficient of variation\n",
    "                notes = f\"HIGH_VARIATION_CV_{cv:.1f}%\"\n",
    "            else:\n",
    "                notes = f\"mean_sd_of_{len(values)}\"\n",
    "            \n",
    "            return f\"{mean_val:.2f} ¬± {std_val:.2f}\", notes\n",
    "            \n",
    "        except ValueError:\n",
    "            # Non-numeric field - keep as array\n",
    "            return json.dumps(values), \"non_numeric_different\"\n",
    "\n",
    "\n",
    "def create_automated_metadata(all_files_metadata, identical_fields, different_fields, config=None):\n",
    "    \"\"\"\n",
    "    Create essential standardized metadata for multi-file analysis.\n",
    "    \n",
    "    Parameters:\n",
    "    all_files_metadata (dict): All files' metadata\n",
    "    identical_fields (dict): Fields with identical values\n",
    "    different_fields (dict): Fields with different values\n",
    "    config (dict): Configuration dictionary\n",
    "    \n",
    "    Returns:\n",
    "    dict: Essential standardized metadata dictionary with ordered fields\n",
    "    \"\"\"\n",
    "    filenames = list(all_files_metadata.keys())\n",
    "    num_files = len(filenames)\n",
    "    \n",
    "    # Start with ordered metadata structure\n",
    "    metadata = {}\n",
    "    processing_notes = {}\n",
    "    \n",
    "    # Create uniqueID with averaging suffix\n",
    "    if 'uniqueID' in identical_fields:\n",
    "        base_id = identical_fields['uniqueID']\n",
    "    else:\n",
    "        # Take from first file if not identical\n",
    "        base_id = list(all_files_metadata.values())[0].get('uniqueID', 'unknown')\n",
    "    \n",
    "    if num_files > 1:\n",
    "        unique_id = f\"{base_id}_avg{num_files}\"\n",
    "    else:\n",
    "        unique_id = base_id\n",
    "    \n",
    "    # Check for custom persistent_ID from config\n",
    "    if config and \"project_metadata\" in config:\n",
    "        project_meta = config[\"project_metadata\"]\n",
    "        if \"persistent_ID\" in project_meta:\n",
    "            custom_id = project_meta[\"persistent_ID\"]\n",
    "            if num_files > 1:\n",
    "                unique_id = f\"{custom_id}_avg{num_files}\"\n",
    "            else:\n",
    "                unique_id = custom_id\n",
    "            print(f\"Using custom persistent ID: {unique_id}\")\n",
    "    else:\n",
    "        project_meta = {}\n",
    "    \n",
    "    # SECTION 1: CORE IDENTIFICATION (first in file)\n",
    "    metadata['experimenter'] = project_meta.get('experimenter', 'SH/HB')\n",
    "    metadata['location'] = project_meta.get('location', 'HD_MPImF_CBP_R0.106')\n",
    "    metadata['project'] = project_meta.get('project', 'LEAF')\n",
    "    metadata['meta_version'] = project_meta.get('meta_version', 'v02')\n",
    "    metadata['pi'] = project_meta.get('pi', 'HB')\n",
    "    metadata['funding'] = project_meta.get('funding', 'MPG')\n",
    "    metadata['persistentID'] = unique_id\n",
    "    metadata['data_collection_method'] = project_meta.get('data_collection_method', 'NTA')\n",
    "    metadata['nta_instrument'] = 'ZetaView'\n",
    "    \n",
    "    # NTA software version\n",
    "    if 'analyze' in identical_fields:\n",
    "        metadata['nta_software'] = f\"ZetaView {identical_fields['analyze']}\"\n",
    "    elif 'analyze' in different_fields:\n",
    "        metadata['nta_software'] = f\"ZetaView {different_fields['analyze'][0]}\"\n",
    "    else:\n",
    "        metadata['nta_software'] = 'ZetaView'\n",
    "    \n",
    "    metadata['nta_processed_file'] = f\"Data_{unique_id}_PSD.txt\"\n",
    "    \n",
    "    # Sample info\n",
    "    if 'sample' in identical_fields:\n",
    "        metadata['sample'] = identical_fields['sample']\n",
    "    elif 'sample' in different_fields:\n",
    "        metadata['sample'] = different_fields['sample'][0]  # Use first sample name\n",
    "    \n",
    "    # SECTION 2: MULTI-FILE INFO\n",
    "    metadata['num_replicates'] = num_files\n",
    "    metadata['source_files'] = json.dumps(filenames)\n",
    "    \n",
    "    # SECTION 3: MEASUREMENT PARAMETERS\n",
    "    # Define essential fields to save (others will be kept in memory only)\n",
    "    essential_fields = {\n",
    "        # Key measurement parameters\n",
    "        'date', 'temperature', 'ph', 'dilution', 'laser_wavelength', 'electrolyte',\n",
    "        'positions', 'cycles', 'fps', \n",
    "        # Quality control\n",
    "        'particle_drift_check_result', 'cell_check_result',\n",
    "        # Key results (not instrument D50s - we calculate better ones later)\n",
    "        'average_number_of_particles', 'number_of_traces', 'detected_particles',\n",
    "        'conductivity', 'scattering_intensity', 'viscosity',\n",
    "        # File info (simplified)\n",
    "        'avi_filesize'\n",
    "    }\n",
    "    \n",
    "    # Process identical fields (only essential ones)\n",
    "    for field_name, value in identical_fields.items():\n",
    "        if field_name in essential_fields:\n",
    "            # Add nta_ prefix for measurement-related fields\n",
    "            if field_name in ['temperature', 'ph', 'dilution', 'laser_wavelength', 'positions', \n",
    "                             'cycles', 'fps', 'average_number_of_particles', 'number_of_traces', \n",
    "                             'detected_particles', 'particle_drift_check_result', 'cell_check_result',\n",
    "                             'conductivity', 'scattering_intensity', 'viscosity', 'avi_filesize']:\n",
    "                # Special naming for summed fields\n",
    "                if field_name in ['number_of_traces', 'detected_particles']:\n",
    "                    metadata[f'nta_{field_name}_sum'] = value\n",
    "                else:\n",
    "                    metadata[f'nta_{field_name}'] = value\n",
    "            else:\n",
    "                metadata[field_name] = value\n",
    "    \n",
    "    # Process different fields with smart formatting (only essential ones)\n",
    "    quality_alerts = []\n",
    "    high_variation_fields = []\n",
    "    \n",
    "    for field_name, values in different_fields.items():\n",
    "        if field_name in essential_fields:\n",
    "            # Apply smart formatting\n",
    "            formatted_value, notes = smart_format_field(field_name, values)\n",
    "            \n",
    "            # Track quality issues\n",
    "            if \"QC_ALERT\" in notes:\n",
    "                quality_alerts.append(f\"{field_name}: {formatted_value}\")\n",
    "            if \"HIGH_VARIATION\" in notes:\n",
    "                high_variation_fields.append(f\"{field_name}: {notes}\")\n",
    "            \n",
    "            # Add nta_ prefix for measurement-related fields\n",
    "            if field_name in ['temperature', 'ph', 'dilution', 'laser_wavelength', 'positions', \n",
    "                             'cycles', 'fps', 'average_number_of_particles', 'number_of_traces', \n",
    "                             'detected_particles', 'particle_drift_check_result', 'cell_check_result',\n",
    "                             'conductivity', 'scattering_intensity', 'viscosity', 'avi_filesize']:\n",
    "                # Special naming for summed fields\n",
    "                if field_name in ['number_of_traces', 'detected_particles']:\n",
    "                    metadata[f'nta_{field_name}_sum'] = formatted_value\n",
    "                else:\n",
    "                    metadata[f'nta_{field_name}'] = formatted_value\n",
    "            else:\n",
    "                metadata[field_name] = formatted_value\n",
    "            \n",
    "            # Store processing note (in memory only)\n",
    "            processing_notes[field_name] = notes\n",
    "    \n",
    "    # SECTION 4: ADDITIONAL REFERENCES\n",
    "    # Note: nta_plot_file will be added later when plots are actually generated\n",
    "    metadata['python_analysis'] = str(date.today())\n",
    "    \n",
    "    # SECTION 5: QUALITY ALERTS (only if present)\n",
    "    if quality_alerts:\n",
    "        metadata['quality_control_alerts'] = json.dumps(quality_alerts)\n",
    "        print(f\"\\n‚ö† QUALITY CONTROL ALERTS:\")\n",
    "        for alert in quality_alerts:\n",
    "            print(f\"  {alert}\")\n",
    "    \n",
    "    if high_variation_fields:\n",
    "        metadata['high_variation_fields'] = json.dumps(high_variation_fields)\n",
    "        print(f\"\\n‚ö† HIGH VARIATION DETECTED:\")\n",
    "        for field in high_variation_fields:\n",
    "            print(f\"  {field}\")\n",
    "    \n",
    "    # Store detailed analysis in memory only (not in saved file)\n",
    "    global current_processing_notes, current_all_fields_metadata, current_original_differences\n",
    "    current_processing_notes = processing_notes\n",
    "    current_all_fields_metadata = all_files_metadata  # Full detailed metadata\n",
    "    current_original_differences = different_fields    # All original differences\n",
    "    \n",
    "    return metadata\n",
    "\n",
    "\n",
    "def save_metadata_file(metadata, output_dir=None, config=None):\n",
    "    \"\"\"\n",
    "    Save metadata to a file in the specified output directory.\n",
    "    \"\"\"\n",
    "    # Determine output directory\n",
    "    if output_dir is None:\n",
    "        if config is not None and \"directory\" in config:\n",
    "            base_dir = config[\"directory\"]\n",
    "            output_dir = os.path.join(base_dir, \"metadata\")\n",
    "        else:\n",
    "            output_dir = os.path.join(os.getcwd(), \"metadata\")\n",
    "    \n",
    "    # Ensure directory exists\n",
    "    try:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "    except Exception as e:\n",
    "        return False, f\"Failed to create metadata directory: {str(e)}\"\n",
    "    \n",
    "    # Create filepath\n",
    "    unique_id = metadata.get('persistentID', 'unknown')\n",
    "    metadata_path = os.path.join(output_dir, f\"Data_{unique_id}_metadata.txt\")\n",
    "    \n",
    "    try:\n",
    "        # Write metadata\n",
    "        with open(metadata_path, 'w') as f:\n",
    "            for key, value in metadata.items():\n",
    "                f.write(f\"{key}\\t{value}\\t\\n\")\n",
    "        \n",
    "        return True, metadata_path\n",
    "    except Exception as e:\n",
    "        return False, f\"Failed to write metadata file: {str(e)}\"\n",
    "\n",
    "\n",
    "# Execute automated metadata extraction if files were processed\n",
    "if 'current_files_data' in globals() and current_files_data:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"AUTOMATED METADATA EXTRACTION AND ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Step 1: Extract all metadata from all files\n",
    "    success, all_files_metadata = extract_metadata_from_all_files(current_files_data)\n",
    "    \n",
    "    if success:\n",
    "        # Step 2: Analyze which fields are identical vs. different\n",
    "        identical_fields, different_fields, field_analysis = analyze_field_differences(all_files_metadata)\n",
    "        \n",
    "        # Step 3: Create standardized metadata with automated rules\n",
    "        standard_metadata = create_automated_metadata(\n",
    "            all_files_metadata, identical_fields, different_fields, CONFIG\n",
    "        )\n",
    "        \n",
    "        # Step 4: Save metadata file\n",
    "        success, metadata_path = save_metadata_file(standard_metadata, config=CONFIG)\n",
    "        \n",
    "        if success:\n",
    "            print(f\"\\nAutomated metadata processing completed successfully!\")\n",
    "            print(f\"Saved comprehensive metadata to: {metadata_path}\")\n",
    "            \n",
    "            # Store for downstream cells\n",
    "            current_metadata = standard_metadata\n",
    "            \n",
    "            print(f\"\\nMetadata Summary:\")\n",
    "            print(f\"  persistentID: {standard_metadata['persistentID']}\")\n",
    "            print(f\"  num_replicates: {standard_metadata['num_replicates']}\")\n",
    "            print(f\"  total_fields_extracted: {len(standard_metadata)}\")\n",
    "            \n",
    "            # Store detailed analysis for inspection (but don't clutter output)\n",
    "            current_field_analysis = field_analysis\n",
    "            current_identical_fields = identical_fields\n",
    "            current_different_fields = different_fields\n",
    "            # current_processing_notes, current_all_fields_metadata, and current_original_differences\n",
    "            # are stored globally by create_automated_metadata()\n",
    "            \n",
    "            print(f\"\\nDetailed analysis available in memory:\")\n",
    "            print(f\"  current_all_fields_metadata (full extraction from all files)\")\n",
    "            print(f\"  current_original_differences (all field differences)\")\n",
    "            print(f\"  current_processing_notes (how each field was processed)\")\n",
    "            \n",
    "        else:\n",
    "            print(f\"ERROR: Failed to save metadata: {metadata_path}\")\n",
    "    \n",
    "    else:\n",
    "        print(f\"ERROR: Failed to extract metadata: {all_files_metadata}\")\n",
    "\n",
    "else:\n",
    "    print(\"No processed files found. Please run Cell 02 and Cell 03 first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "726df0e5-2b10-4eab-abf5-6f37cc991d11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "NTA Data Analysis - Core Calculations with Uncertainty Propagation (Cell 05)\n",
    "\n",
    "This module provides core calculation functions for NTA data analysis with rigorous\n",
    "uncertainty propagation through averaging of replicates:\n",
    "\n",
    "1. Dilution correction for all measurements\n",
    "2. Normalization of particle distributions with uncertainty propagation\n",
    "3. Calculation of cumulative distributions with proper error propagation\n",
    "4. Total metrics calculation with uncertainty combination\n",
    "\n",
    "These functions provide the mathematical foundation for analyzing averaged\n",
    "number, volume, and surface area distributions from replicate NTA measurements.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import integrate\n",
    "\n",
    "\n",
    "def apply_dilution_correction_with_uncertainty(df, metadata=None, manual_dilution=None):\n",
    "    \"\"\"\n",
    "    Apply dilution correction to all measured values with uncertainty propagation.\n",
    "    \n",
    "    For a dilution factor D, the actual sample concentration = measured √ó D\n",
    "    Uncertainty propagation: œÉ_actual = œÉ_measured √ó D\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): DataFrame containing averaged distribution data\n",
    "    metadata (dict): Metadata dictionary that may contain dilution info\n",
    "    manual_dilution (float): Manual dilution factor override (optional)\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (success_flag, result)\n",
    "        - If successful: (True, updated_df)\n",
    "        - If failed: (False, error_message)\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying the original dataframe\n",
    "    updated_df = df.copy()\n",
    "    \n",
    "    # Determine dilution factor\n",
    "    dilution_factor = 1.0  # Default: no dilution\n",
    "    dilution_source = \"default (no dilution)\"\n",
    "    \n",
    "    if manual_dilution is not None:\n",
    "        try:\n",
    "            dilution_factor = float(manual_dilution)\n",
    "            dilution_source = \"manually specified\"\n",
    "        except (ValueError, TypeError):\n",
    "            return False, f\"Invalid manual dilution factor: {manual_dilution}\"\n",
    "    \n",
    "    elif metadata is not None:\n",
    "        # Try nta_dilution field\n",
    "        if 'nta_dilution' in metadata:\n",
    "            try:\n",
    "                dilution_string = metadata['nta_dilution']\n",
    "                # Handle format like \"10.0\" or other formats\n",
    "                dilution_factor = float(dilution_string.split('¬±')[0].strip()) if '¬±' in dilution_string else float(dilution_string)\n",
    "                dilution_source = \"metadata (nta_dilution)\"\n",
    "            except (ValueError, TypeError):\n",
    "                print(\"Warning: Could not parse dilution factor from metadata, using default (1.0)\")\n",
    "    \n",
    "    print(f\"Applying dilution correction: factor = {dilution_factor} (source: {dilution_source})\")\n",
    "    \n",
    "    # Apply dilution correction to concentration and rename\n",
    "    if 'concentration_cm-3_avg' in updated_df.columns:\n",
    "        updated_df['particles_per_mL_avg'] = updated_df['concentration_cm-3_avg'] * dilution_factor\n",
    "        if 'concentration_cm-3_sd' in updated_df.columns:\n",
    "            updated_df['particles_per_mL_sd'] = updated_df['concentration_cm-3_sd'] * dilution_factor\n",
    "        \n",
    "        # Remove old concentration columns\n",
    "        updated_df = updated_df.drop(['concentration_cm-3_avg', 'concentration_cm-3_sd'], axis=1, errors='ignore')\n",
    "    else:\n",
    "        return False, \"Missing concentration_cm-3_avg column for dilution correction\"\n",
    "    \n",
    "    # Apply dilution correction to volume and rename to per_mL\n",
    "    if 'volume_nm^3_avg' in updated_df.columns:\n",
    "        updated_df['volume_nm^3_per_mL_avg'] = updated_df['volume_nm^3_avg'] * dilution_factor\n",
    "        if 'volume_nm^3_sd' in updated_df.columns:\n",
    "            updated_df['volume_nm^3_per_mL_sd'] = updated_df['volume_nm^3_sd'] * dilution_factor\n",
    "        \n",
    "        # Remove old volume columns\n",
    "        updated_df = updated_df.drop(['volume_nm^3_avg', 'volume_nm^3_sd'], axis=1, errors='ignore')\n",
    "    \n",
    "    # Apply dilution correction to area and rename to per_mL\n",
    "    if 'area_nm^2_avg' in updated_df.columns:\n",
    "        updated_df['area_nm^2_per_mL_avg'] = updated_df['area_nm^2_avg'] * dilution_factor\n",
    "        if 'area_nm^2_sd' in updated_df.columns:\n",
    "            updated_df['area_nm^2_per_mL_sd'] = updated_df['area_nm^2_sd'] * dilution_factor\n",
    "        \n",
    "        # Remove old area columns\n",
    "        updated_df = updated_df.drop(['area_nm^2_avg', 'area_nm^2_sd'], axis=1, errors='ignore')\n",
    "    \n",
    "    return True, updated_df\n",
    "\n",
    "\n",
    "def normalize_distributions_with_uncertainty(df, size_column='size_nm'):\n",
    "    \"\"\"\n",
    "    Normalize particle distributions by area under the curve with uncertainty propagation.\n",
    "    \n",
    "    This creates normalized number distributions from the averaged number data.\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): DataFrame containing averaged particle distribution data\n",
    "    size_column (str): Name of the column containing size values\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: Updated dataframe with normalized columns and uncertainties\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying the original dataframe\n",
    "    normalized_df = df.copy()\n",
    "    \n",
    "    # Process each scale separately\n",
    "    for scale in normalized_df['scale'].unique():\n",
    "        scale_mask = normalized_df['scale'] == scale\n",
    "        scale_data = normalized_df[scale_mask].copy()\n",
    "        \n",
    "        if scale_data.empty or 'number_avg' not in scale_data.columns:\n",
    "            continue\n",
    "        \n",
    "        # Sort by size for correct integration\n",
    "        scale_data = scale_data.sort_values(size_column)\n",
    "        \n",
    "        # Calculate area under the curve using trapezoidal rule\n",
    "        sizes = scale_data[size_column].values\n",
    "        numbers_avg = scale_data['number_avg'].values\n",
    "        \n",
    "        if len(sizes) < 2:\n",
    "            continue\n",
    "        \n",
    "        # Calculate area for normalization\n",
    "        area_avg = np.trapz(numbers_avg, sizes)\n",
    "        \n",
    "        if area_avg > 0:\n",
    "            # Normalize the averages\n",
    "            normalized_df.loc[scale_mask, 'number_normalized_avg'] = numbers_avg / area_avg\n",
    "            \n",
    "            # Normalize the standard deviations (uncertainty propagation)\n",
    "            if 'number_sd' in scale_data.columns:\n",
    "                numbers_sd = scale_data['number_sd'].values\n",
    "                normalized_df.loc[scale_mask, 'number_normalized_sd'] = numbers_sd / area_avg\n",
    "            else:\n",
    "                normalized_df.loc[scale_mask, 'number_normalized_sd'] = 0.0\n",
    "        else:\n",
    "            # If area is zero, set normalized values to zero\n",
    "            normalized_df.loc[scale_mask, 'number_normalized_avg'] = 0.0\n",
    "            normalized_df.loc[scale_mask, 'number_normalized_sd'] = 0.0\n",
    "    \n",
    "    return normalized_df\n",
    "\n",
    "\n",
    "def calculate_cumulative_distributions_with_uncertainty(df, scale_column='scale'):\n",
    "    \"\"\"\n",
    "    Calculate cumulative distributions with proper uncertainty propagation.\n",
    "    \n",
    "    For independent uncertainties, cumulative uncertainties are calculated as:\n",
    "    œÉ_cumsum[j] = ‚àö(Œ£(i=0 to j) œÉ[i]¬≤)\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): DataFrame containing particle distribution data with uncertainties\n",
    "    scale_column (str): Name of the column distinguishing scale types\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: Updated dataframe with cumulative distribution columns and uncertainties\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying the original\n",
    "    result_df = df.copy()\n",
    "    \n",
    "    # Process each scale type separately\n",
    "    for scale in result_df[scale_column].unique():\n",
    "        # Filter for current scale\n",
    "        scale_mask = result_df[scale_column] == scale\n",
    "        scale_indices = result_df[scale_mask].sort_values('size_nm').index\n",
    "        \n",
    "        if len(scale_indices) == 0:\n",
    "            continue\n",
    "        \n",
    "        # 1. Normalized number distribution (shape analysis)\n",
    "        if 'number_normalized_avg' in result_df.columns:\n",
    "            # Calculate cumulative sum\n",
    "            cumsum_avg = result_df.loc[scale_indices, 'number_normalized_avg'].cumsum()\n",
    "            \n",
    "            # Normalize to 0-1 range (should be close to 1 already due to normalization)\n",
    "            if cumsum_avg.iloc[-1] > 0:\n",
    "                result_df.loc[scale_indices, 'number_normalized_cumsum_avg'] = cumsum_avg / cumsum_avg.iloc[-1]\n",
    "            else:\n",
    "                result_df.loc[scale_indices, 'number_normalized_cumsum_avg'] = 0\n",
    "            \n",
    "            # Calculate uncertainty in cumulative sum using error propagation\n",
    "            if 'number_normalized_sd' in result_df.columns:\n",
    "                # For cumulative sum: œÉ_cumsum[j] = ‚àö(Œ£(i=0 to j) œÉ[i]¬≤)\n",
    "                normalized_var_cumsum = (result_df.loc[scale_indices, 'number_normalized_sd'] ** 2).cumsum()\n",
    "                cumsum_sd = np.sqrt(normalized_var_cumsum)\n",
    "                \n",
    "                # Normalize the uncertainty as well\n",
    "                if cumsum_avg.iloc[-1] > 0:\n",
    "                    result_df.loc[scale_indices, 'number_normalized_cumsum_sd'] = cumsum_sd / cumsum_avg.iloc[-1]\n",
    "                else:\n",
    "                    result_df.loc[scale_indices, 'number_normalized_cumsum_sd'] = 0\n",
    "        \n",
    "        # 2. Absolute volume distribution\n",
    "        if 'volume_nm^3_per_mL_avg' in result_df.columns:\n",
    "            # Calculate cumulative sum for averages\n",
    "            cumsum_avg = result_df.loc[scale_indices, 'volume_nm^3_per_mL_avg'].cumsum()\n",
    "            result_df.loc[scale_indices, 'volume_nm^3_per_mL_cumsum_avg'] = cumsum_avg\n",
    "            \n",
    "            # Calculate uncertainty in cumulative sum\n",
    "            if 'volume_nm^3_per_mL_sd' in result_df.columns:\n",
    "                volume_var_cumsum = (result_df.loc[scale_indices, 'volume_nm^3_per_mL_sd'] ** 2).cumsum()\n",
    "                result_df.loc[scale_indices, 'volume_nm^3_per_mL_cumsum_sd'] = np.sqrt(volume_var_cumsum)\n",
    "        \n",
    "        # 3. Absolute surface area distribution\n",
    "        if 'area_nm^2_per_mL_avg' in result_df.columns:\n",
    "            # Calculate cumulative sum for averages\n",
    "            cumsum_avg = result_df.loc[scale_indices, 'area_nm^2_per_mL_avg'].cumsum()\n",
    "            result_df.loc[scale_indices, 'area_nm^2_per_mL_cumsum_avg'] = cumsum_avg\n",
    "            \n",
    "            # Calculate uncertainty in cumulative sum\n",
    "            if 'area_nm^2_per_mL_sd' in result_df.columns:\n",
    "                area_var_cumsum = (result_df.loc[scale_indices, 'area_nm^2_per_mL_sd'] ** 2).cumsum()\n",
    "                result_df.loc[scale_indices, 'area_nm^2_per_mL_cumsum_sd'] = np.sqrt(area_var_cumsum)\n",
    "                \n",
    "    return result_df\n",
    "\n",
    "\n",
    "def calculate_total_metrics_with_uncertainty(df, scale_column='scale'):\n",
    "    \"\"\"\n",
    "    Calculate total metrics for each scale with proper uncertainty propagation.\n",
    "    \n",
    "    For totals across bins, uncertainties are combined as: œÉ_total = ‚àö(Œ£ œÉ_i¬≤)\n",
    "    For derived metrics, simple calculations are used without uncertainty propagation.\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): DataFrame containing particle distribution data with uncertainties\n",
    "    scale_column (str): Name of the column distinguishing scale types\n",
    "    \n",
    "    Returns:\n",
    "    dict: Dictionary with total metrics and uncertainties for each scale\n",
    "    \"\"\"\n",
    "    # Initialize results structure\n",
    "    results = {}\n",
    "    \n",
    "    # Process each scale type separately\n",
    "    for scale in df[scale_column].unique():\n",
    "        # Filter for current scale and sort by size\n",
    "        scale_df = df[df[scale_column] == scale].sort_values('size_nm')\n",
    "        \n",
    "        # Initialize metrics for this scale\n",
    "        scale_metrics = {}\n",
    "        \n",
    "        # Only calculate if we have data for this scale\n",
    "        if not scale_df.empty:\n",
    "            \n",
    "            # 1. Total particles per mL\n",
    "            if 'particles_per_mL_avg' in scale_df.columns:\n",
    "                total_particles_avg = scale_df['particles_per_mL_avg'].sum()\n",
    "                scale_metrics['total_particles_per_mL_avg'] = total_particles_avg\n",
    "                scale_metrics['total_particles_per_uL_avg'] = total_particles_avg / 1000\n",
    "                \n",
    "                # Calculate uncertainty: œÉ_total = ‚àö(Œ£ œÉ_i¬≤)\n",
    "                if 'particles_per_mL_sd' in scale_df.columns:\n",
    "                    total_particles_sd = np.sqrt((scale_df['particles_per_mL_sd'] ** 2).sum())\n",
    "                    scale_metrics['total_particles_per_mL_sd'] = total_particles_sd\n",
    "                    scale_metrics['total_particles_per_uL_sd'] = total_particles_sd / 1000\n",
    "            \n",
    "            # 2. Total volume per mL\n",
    "            if 'volume_nm^3_per_mL_avg' in scale_df.columns:\n",
    "                total_volume_avg = scale_df['volume_nm^3_per_mL_avg'].sum()\n",
    "                scale_metrics['total_volume_nm^3_per_mL_avg'] = total_volume_avg\n",
    "                scale_metrics['total_volume_um^3_per_mL_avg'] = total_volume_avg / 1e9  # nm¬≥ to Œºm¬≥\n",
    "                scale_metrics['total_volume_uL_per_mL_avg'] = total_volume_avg / 1e18  # nm¬≥ to ŒºL\n",
    "                scale_metrics['volume_percentage_avg'] = (total_volume_avg / 1e18) * 0.1  # percentage\n",
    "                \n",
    "                # Calculate uncertainty\n",
    "                if 'volume_nm^3_per_mL_sd' in scale_df.columns:\n",
    "                    total_volume_sd = np.sqrt((scale_df['volume_nm^3_per_mL_sd'] ** 2).sum())\n",
    "                    scale_metrics['total_volume_nm^3_per_mL_sd'] = total_volume_sd\n",
    "                    scale_metrics['total_volume_um^3_per_mL_sd'] = total_volume_sd / 1e9\n",
    "                    scale_metrics['total_volume_uL_per_mL_sd'] = total_volume_sd / 1e18\n",
    "                    scale_metrics['volume_percentage_sd'] = (total_volume_sd / 1e18) * 0.1\n",
    "            \n",
    "            # 3. Total surface area per mL\n",
    "            if 'area_nm^2_per_mL_avg' in scale_df.columns:\n",
    "                total_area_avg = scale_df['area_nm^2_per_mL_avg'].sum()\n",
    "                scale_metrics['total_surface_area_nm^2_per_mL_avg'] = total_area_avg\n",
    "                scale_metrics['total_surface_area_um^2_per_mL_avg'] = total_area_avg / 1e6  # nm¬≤ to Œºm¬≤\n",
    "                scale_metrics['total_surface_area_cm^2_per_mL_avg'] = total_area_avg / 1e14  # nm¬≤ to cm¬≤\n",
    "                \n",
    "                # Calculate uncertainty\n",
    "                if 'area_nm^2_per_mL_sd' in scale_df.columns:\n",
    "                    total_area_sd = np.sqrt((scale_df['area_nm^2_per_mL_sd'] ** 2).sum())\n",
    "                    scale_metrics['total_surface_area_nm^2_per_mL_sd'] = total_area_sd\n",
    "                    scale_metrics['total_surface_area_um^2_per_mL_sd'] = total_area_sd / 1e6\n",
    "                    scale_metrics['total_surface_area_cm^2_per_mL_sd'] = total_area_sd / 1e14\n",
    "                \n",
    "                # 4. Specific surface area (derived metric, no uncertainty propagation for now)\n",
    "                if ('total_volume_nm^3_per_mL_avg' in scale_metrics and \n",
    "                    scale_metrics['total_volume_nm^3_per_mL_avg'] > 0):\n",
    "                    # Surface area (nm¬≤) / volume (nm¬≥) = 1/nm\n",
    "                    ssa_1_per_nm = total_area_avg / scale_metrics['total_volume_nm^3_per_mL_avg']\n",
    "                    # Convert to m¬≤/cm¬≥ (standard unit)\n",
    "                    scale_metrics['specific_surface_area_m^2_per_cm^3_avg'] = ssa_1_per_nm * 10\n",
    "        \n",
    "        # Store metrics for this scale\n",
    "        results[scale] = scale_metrics\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def add_metrics_to_metadata_with_uncertainty(metadata, metrics, scale='linear'):\n",
    "    \"\"\"\n",
    "    Add key metrics with uncertainties to the metadata dictionary.\n",
    "    \n",
    "    Parameters:\n",
    "    metadata (dict): Current metadata dictionary\n",
    "    metrics (dict): Dictionary of calculated metrics with uncertainties\n",
    "    scale (str): Which scale's metrics to use ('linear' or 'logarithmic')\n",
    "    \n",
    "    Returns:\n",
    "    dict: Updated metadata dictionary\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying the original\n",
    "    updated_metadata = metadata.copy()\n",
    "    \n",
    "    # Use linear scale metrics by default, but check if available\n",
    "    if scale not in metrics:\n",
    "        # Fall back to any available scale\n",
    "        if metrics:\n",
    "            scale = list(metrics.keys())[0]\n",
    "        else:\n",
    "            return updated_metadata  # Return unchanged if no metrics available\n",
    "    \n",
    "    scale_metrics = metrics[scale]\n",
    "    \n",
    "    # Add key metrics to metadata with uncertainties\n",
    "    if 'total_particles_per_mL_avg' in scale_metrics:\n",
    "        avg_val = scale_metrics['total_particles_per_mL_avg']\n",
    "        sd_val = scale_metrics.get('total_particles_per_mL_sd', 0)\n",
    "        updated_metadata['nta_total_particles_per_mL'] = f\"{avg_val:.2E} ¬± {sd_val:.2E}\"\n",
    "    \n",
    "    if 'total_volume_uL_per_mL_avg' in scale_metrics:\n",
    "        avg_val = scale_metrics['total_volume_uL_per_mL_avg']\n",
    "        sd_val = scale_metrics.get('total_volume_uL_per_mL_sd', 0)\n",
    "        updated_metadata['nta_total_volume_uL_per_mL'] = f\"{avg_val:.4E} ¬± {sd_val:.4E}\"\n",
    "    \n",
    "    if 'volume_percentage_avg' in scale_metrics:\n",
    "        avg_val = scale_metrics['volume_percentage_avg']\n",
    "        sd_val = scale_metrics.get('volume_percentage_sd', 0)\n",
    "        updated_metadata['nta_volume_percentage'] = f\"{avg_val:.6f} ¬± {sd_val:.6f}\"\n",
    "    \n",
    "    if 'total_surface_area_cm^2_per_mL_avg' in scale_metrics:\n",
    "        avg_val = scale_metrics['total_surface_area_cm^2_per_mL_avg']\n",
    "        sd_val = scale_metrics.get('total_surface_area_cm^2_per_mL_sd', 0)\n",
    "        updated_metadata['nta_total_surface_area_cm^2_per_mL'] = f\"{avg_val:.4E} ¬± {sd_val:.4E}\"\n",
    "    \n",
    "    if 'specific_surface_area_m^2_per_cm^3_avg' in scale_metrics:\n",
    "        avg_val = scale_metrics['specific_surface_area_m^2_per_cm^3_avg']\n",
    "        # No uncertainty for derived metrics yet\n",
    "        updated_metadata['nta_specific_surface_area_m^2_per_cm^3'] = f\"{avg_val:.2f}\"\n",
    "    \n",
    "    # Add a scale indicator and number of replicates\n",
    "    updated_metadata['nta_metrics_scale'] = scale\n",
    "    if 'num_replicates' in updated_metadata:\n",
    "        updated_metadata['nta_metrics_replicates'] = updated_metadata['num_replicates']\n",
    "    \n",
    "    return updated_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17989377-a304-41b4-8772-a83c90bd6eb1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No distribution data found in 'current_distribution_df'.\n",
      "Please run the data extraction and averaging workflow first:\n",
      "  cell00_multipleFiles ‚Üí cell02_multipleFiles ‚Üí cell03_multipleFiles\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# EXECUTE CORE CALCULATIONS WITH UNCERTAINTY PROPAGATION (Cell 05.2)\n",
    "# ================================================================\n",
    "\n",
    "# Execute the full calculation workflow if we have current data\n",
    "if 'current_distribution_df' in globals() and current_distribution_df is not None:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"EXECUTING CORE CALCULATIONS WITH UNCERTAINTY PROPAGATION\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Step 1: Apply dilution correction first (creates proper per_mL columns)\n",
    "    print(\"\\n1. Applying dilution correction with uncertainty propagation...\")\n",
    "    try:\n",
    "        success, dilution_corrected_df = apply_dilution_correction_with_uncertainty(\n",
    "            current_distribution_df, \n",
    "            metadata=current_metadata if 'current_metadata' in globals() else None\n",
    "        )\n",
    "        if success:\n",
    "            print(\"‚úì Dilution correction completed successfully\")\n",
    "            current_distribution_df = dilution_corrected_df  # Update global variable\n",
    "        else:\n",
    "            print(f\"‚úó Dilution correction failed: {dilution_corrected_df}\")\n",
    "            dilution_corrected_df = current_distribution_df\n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Dilution correction failed: {e}\")\n",
    "        dilution_corrected_df = current_distribution_df\n",
    "    \n",
    "    # Step 2: Normalize distributions with uncertainty propagation\n",
    "    print(\"\\n2. Normalizing distributions with uncertainty propagation...\")\n",
    "    try:\n",
    "        normalized_df = normalize_distributions_with_uncertainty(dilution_corrected_df)\n",
    "        print(\"‚úì Normalization completed successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Normalization failed: {e}\")\n",
    "        normalized_df = dilution_corrected_df\n",
    "    \n",
    "    # Step 3: Calculate cumulative distributions with uncertainty (includes volume and surface area)\n",
    "    print(\"\\n3. Calculating cumulative distributions with uncertainty...\")\n",
    "    try:\n",
    "        final_df = calculate_cumulative_distributions_with_uncertainty(normalized_df)\n",
    "        print(\"‚úì Cumulative distributions completed successfully\")\n",
    "        current_distribution_df = final_df  # Update global variable\n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Cumulative distributions failed: {e}\")\n",
    "        current_distribution_df = normalized_df\n",
    "    \n",
    "    # Step 4: Calculate total metrics with uncertainty\n",
    "    print(\"\\n4. Calculating total metrics with uncertainty...\")\n",
    "    try:\n",
    "        total_metrics = calculate_total_metrics_with_uncertainty(current_distribution_df)\n",
    "        \n",
    "        # Display enhanced metrics for each scale\n",
    "        for scale, metrics in total_metrics.items():\n",
    "            if metrics:\n",
    "                print(f\"\\n{scale.upper()} SCALE - TOTAL METRICS WITH UNCERTAINTIES:\")\n",
    "                \n",
    "                # Particle count metrics\n",
    "                if 'total_particles_per_mL_avg' in metrics:\n",
    "                    avg_val = metrics['total_particles_per_mL_avg']\n",
    "                    sd_val = metrics.get('total_particles_per_mL_sd', 0)\n",
    "                    print(f\"  Particle concentration: {avg_val:.2E} ¬± {sd_val:.2E} particles/mL\")\n",
    "                    \n",
    "                    if 'total_particles_per_uL_avg' in metrics:\n",
    "                        avg_val_ul = metrics['total_particles_per_uL_avg']\n",
    "                        sd_val_ul = metrics.get('total_particles_per_uL_sd', 0)\n",
    "                        print(f\"                         {avg_val_ul:.2E} ¬± {sd_val_ul:.2E} particles/¬µL\")\n",
    "                \n",
    "                # Volume metrics\n",
    "                if 'total_volume_nm^3_per_mL_avg' in metrics:\n",
    "                    print(f\"  Volume Information:\")\n",
    "                    \n",
    "                    avg_val = metrics['total_volume_nm^3_per_mL_avg']\n",
    "                    sd_val = metrics.get('total_volume_nm^3_per_mL_sd', 0)\n",
    "                    print(f\"    {avg_val:.2E} ¬± {sd_val:.2E} nm¬≥/mL\")\n",
    "                    \n",
    "                    if 'total_volume_um^3_per_mL_avg' in metrics:\n",
    "                        avg_val_um = metrics['total_volume_um^3_per_mL_avg']\n",
    "                        sd_val_um = metrics.get('total_volume_um^3_per_mL_sd', 0)\n",
    "                        print(f\"    {avg_val_um:.2E} ¬± {sd_val_um:.2E} ¬µm¬≥/mL\")\n",
    "                    \n",
    "                    if 'total_volume_uL_per_mL_avg' in metrics:\n",
    "                        avg_val_ul = metrics['total_volume_uL_per_mL_avg']\n",
    "                        sd_val_ul = metrics.get('total_volume_uL_per_mL_sd', 0)\n",
    "                        print(f\"    {avg_val_ul:.4E} ¬± {sd_val_ul:.4E} ¬µL/mL\")\n",
    "                    \n",
    "                    if 'volume_percentage_avg' in metrics:\n",
    "                        avg_val_pct = metrics['volume_percentage_avg']\n",
    "                        sd_val_pct = metrics.get('volume_percentage_sd', 0)\n",
    "                        print(f\"    {avg_val_pct:.6f} ¬± {sd_val_pct:.6f}% of sample volume\")\n",
    "                \n",
    "                # Surface area metrics\n",
    "                if 'total_surface_area_nm^2_per_mL_avg' in metrics:\n",
    "                    print(f\"  Surface Area Information:\")\n",
    "                    \n",
    "                    avg_val = metrics['total_surface_area_nm^2_per_mL_avg']\n",
    "                    sd_val = metrics.get('total_surface_area_nm^2_per_mL_sd', 0)\n",
    "                    print(f\"    {avg_val:.2E} ¬± {sd_val:.2E} nm¬≤/mL\")\n",
    "                    \n",
    "                    if 'total_surface_area_um^2_per_mL_avg' in metrics:\n",
    "                        avg_val_um = metrics['total_surface_area_um^2_per_mL_avg']\n",
    "                        sd_val_um = metrics.get('total_surface_area_um^2_per_mL_sd', 0)\n",
    "                        print(f\"    {avg_val_um:.2E} ¬± {sd_val_um:.2E} ¬µm¬≤/mL\")\n",
    "                    \n",
    "                    if 'total_surface_area_cm^2_per_mL_avg' in metrics:\n",
    "                        avg_val_cm = metrics['total_surface_area_cm^2_per_mL_avg']\n",
    "                        sd_val_cm = metrics.get('total_surface_area_cm^2_per_mL_sd', 0)\n",
    "                        print(f\"    {avg_val_cm:.4E} ¬± {sd_val_cm:.4E} cm¬≤/mL\")\n",
    "                \n",
    "                # Specific surface area (no uncertainty for now)\n",
    "                if 'specific_surface_area_m^2_per_cm^3_avg' in metrics:\n",
    "                    print(f\"  Specific Surface Area:\")\n",
    "                    print(f\"    {metrics['specific_surface_area_m^2_per_cm^3_avg']:.2f} m¬≤/cm¬≥\")\n",
    "        \n",
    "        # Step 5: Update metadata with enhanced metrics\n",
    "        if 'current_metadata' in globals():\n",
    "            print(\"\\n5. Updating metadata with calculated metrics...\")\n",
    "            try:\n",
    "                current_metadata = add_metrics_to_metadata_with_uncertainty(\n",
    "                    current_metadata, total_metrics\n",
    "                )\n",
    "                print(\"‚úì Metadata updated with enhanced metrics including uncertainties\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚úó Metadata update failed: {e}\")\n",
    "        \n",
    "        # Step 6: Update existing metadata file (don't overwrite)\n",
    "        print(\"\\n6. Updating existing metadata file...\")\n",
    "        try:\n",
    "            # Get output directory\n",
    "            if 'CONFIG' in globals() and \"directory\" in CONFIG:\n",
    "                metadata_dir = os.path.join(CONFIG[\"directory\"], \"metadata\")\n",
    "            else:\n",
    "                metadata_dir = os.path.join(os.getcwd(), \"metadata\")\n",
    "            \n",
    "            # Create metadata file path\n",
    "            unique_id = current_metadata.get('persistentID', 'unknown')\n",
    "            metadata_path = os.path.join(metadata_dir, f\"Data_{unique_id}_metadata.txt\")\n",
    "            \n",
    "            # Read existing metadata file to preserve all fields\n",
    "            existing_metadata = {}\n",
    "            if os.path.exists(metadata_path):\n",
    "                with open(metadata_path, 'r') as f:\n",
    "                    for line in f:\n",
    "                        parts = line.strip().split('\\t')\n",
    "                        if len(parts) >= 2:\n",
    "                            existing_metadata[parts[0]] = parts[1]\n",
    "            \n",
    "            # Update existing metadata with our enhanced values\n",
    "            existing_metadata.update(current_metadata)\n",
    "            \n",
    "            # Write back the combined metadata\n",
    "            with open(metadata_path, 'w') as f:\n",
    "                for key, value in existing_metadata.items():\n",
    "                    f.write(f\"{key}\\t{value}\\t\\n\")\n",
    "            \n",
    "            print(f\"‚úì Updated existing metadata file: {metadata_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚úó Failed to update metadata file: {e}\")\n",
    "        \n",
    "        # Step 7: Save processed distribution data (clean, analysis-ready data only)\n",
    "        print(\"\\n7. Saving processed distribution data...\")\n",
    "        try:\n",
    "            # Get output directory\n",
    "            if 'CONFIG' in globals() and \"directory\" in CONFIG:\n",
    "                processed_dir = os.path.join(CONFIG[\"directory\"], \"processed\")\n",
    "            else:\n",
    "                processed_dir = os.path.join(os.getcwd(), \"processed\")\n",
    "            \n",
    "            # Ensure processed directory exists\n",
    "            os.makedirs(processed_dir, exist_ok=True)\n",
    "            \n",
    "            # The dataframe is already cleaned in the calculation step\n",
    "            # Just save it directly with the correct column order\n",
    "            clean_df = current_distribution_df.copy()\n",
    "            \n",
    "            # Create PSD file path\n",
    "            unique_id = current_metadata.get('persistentID', 'unknown')\n",
    "            psd_path = os.path.join(processed_dir, f\"Data_{unique_id}_PSD.txt\")\n",
    "            \n",
    "            # Save the clean distribution data as tab-separated file\n",
    "            clean_df.to_csv(psd_path, sep='\\t', index=False)\n",
    "            \n",
    "            print(f\"‚úì Saved processed distribution data to: {psd_path}\")\n",
    "            print(f\"  Columns saved: {len(clean_df.columns)}\")\n",
    "            print(f\"  Data points: {len(clean_df)} ({len(clean_df[clean_df['scale']=='linear'])} linear, {len(clean_df[clean_df['scale']=='logarithmic'])} log)\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚úó Failed to save distribution data: {e}\")\n",
    "        \n",
    "        # Store results for downstream analysis\n",
    "        current_total_metrics = total_metrics\n",
    "        \n",
    "        print(f\"\\n\" + \"=\" * 80)\n",
    "        print(\"CORE CALCULATIONS COMPLETED SUCCESSFULLY!\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"Dataset ready for statistics calculation and visualization.\")\n",
    "        print(f\"Enhanced data stored in: current_distribution_df\")\n",
    "        print(f\"Total metrics with uncertainties stored in: current_total_metrics\")\n",
    "        print(f\"Files saved:\")\n",
    "        print(f\"  Metadata: Data_{unique_id}_metadata.txt\")\n",
    "        print(f\"  Distribution: Data_{unique_id}_PSD.txt\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Total metrics calculation failed: {e}\")\n",
    "\n",
    "else:\n",
    "    print(\"No distribution data found in 'current_distribution_df'.\")\n",
    "    print(\"Please run the data extraction and averaging workflow first:\")\n",
    "    print(\"  cell00_multipleFiles ‚Üí cell02_multipleFiles ‚Üí cell03_multipleFiles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab390cbe-ed8e-4128-86e6-2aced16dd881",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No distribution data found in 'current_distribution_df'.\n",
      "Please run the complete workflow first:\n",
      "  cell00_multipleFiles ‚Üí cell02_multipleFiles ‚Üí cell03_multipleFiles ‚Üí\n",
      "  cell04_multipleFiles ‚Üí cell05 ‚Üí cell05.2\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "NTA Data Analysis - Statistics Module with Uncertainty Propagation (Cell 06)\n",
    "\n",
    "This module calculates key statistics (D10, D50, D90, span) from\n",
    "pre-calculated cumulative distributions with proper uncertainty propagation.\n",
    "Uses bounds-based approach: interpolate cumsum¬±SD to get D-value uncertainties.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "def interpolate_d_value_with_bounds(sizes, cumsum_avg, cumsum_sd, target_fraction):\n",
    "    \"\"\"\n",
    "    Calculate D-value with asymmetric confidence bounds using bounds approach.\n",
    "    \n",
    "    Parameters:\n",
    "    sizes (array): Size values (nm)\n",
    "    cumsum_avg (array): Average cumulative distribution values\n",
    "    cumsum_sd (array): Standard deviation of cumulative distribution values\n",
    "    target_fraction (float): Target fraction (e.g., 0.1 for D10, 0.5 for D50)\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (d_value_avg, d_value_lower, d_value_upper)\n",
    "    \"\"\"\n",
    "    # Ensure arrays are sorted by size\n",
    "    sorted_indices = np.argsort(sizes)\n",
    "    sizes_sorted = sizes[sorted_indices]\n",
    "    cumsum_avg_sorted = cumsum_avg[sorted_indices]\n",
    "    cumsum_sd_sorted = cumsum_sd[sorted_indices]\n",
    "    \n",
    "    # Calculate bounds\n",
    "    cumsum_lower = cumsum_avg_sorted - cumsum_sd_sorted\n",
    "    cumsum_upper = cumsum_avg_sorted + cumsum_sd_sorted\n",
    "    \n",
    "    # Ensure cumulative distributions are monotonic and in valid range [0,1]\n",
    "    cumsum_avg_sorted = np.clip(cumsum_avg_sorted, 0, 1)\n",
    "    cumsum_lower = np.clip(cumsum_lower, 0, 1)\n",
    "    cumsum_upper = np.clip(cumsum_upper, 0, 1)\n",
    "    \n",
    "    # Ensure monotonicity by taking cumulative maximum\n",
    "    cumsum_avg_sorted = np.maximum.accumulate(cumsum_avg_sorted)\n",
    "    cumsum_lower = np.maximum.accumulate(cumsum_lower)\n",
    "    cumsum_upper = np.maximum.accumulate(cumsum_upper)\n",
    "    \n",
    "    # Check if target fraction is achievable\n",
    "    if target_fraction < cumsum_avg_sorted[0] or target_fraction > cumsum_avg_sorted[-1]:\n",
    "        return np.nan, np.nan, np.nan\n",
    "    \n",
    "    # Interpolate D-values\n",
    "    try:\n",
    "        d_value_avg = np.interp(target_fraction, cumsum_avg_sorted, sizes_sorted)\n",
    "        d_value_lower = np.interp(target_fraction, cumsum_upper, sizes_sorted)  # Note: swapped!\n",
    "        d_value_upper = np.interp(target_fraction, cumsum_lower, sizes_sorted)  # Note: swapped!\n",
    "        \n",
    "        # The swapping is because:\n",
    "        # - When cumsum is higher (cumsum + sd), we reach target fraction at smaller size ‚Üí lower bound\n",
    "        # - When cumsum is lower (cumsum - sd), we reach target fraction at larger size ‚Üí upper bound\n",
    "        \n",
    "        return d_value_avg, d_value_lower, d_value_upper\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in interpolation for target fraction {target_fraction}: {e}\")\n",
    "        return np.nan, np.nan, np.nan\n",
    "\n",
    "\n",
    "def calculate_percentile_statistics_with_uncertainty(df, size_column='size_nm'):\n",
    "    \"\"\"\n",
    "    Calculate percentile statistics (D10, D50, D90, span) with uncertainties for all\n",
    "    cumulative distributions (number, volume, surface area) and scales.\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): DataFrame containing size and cumulative distribution data with uncertainties\n",
    "    size_column (str): Column name for particle sizes\n",
    "    \n",
    "    Returns:\n",
    "    dict: Nested dictionary of statistics by scale, distribution type, and metric\n",
    "    \"\"\"\n",
    "    # Initialize results dictionary\n",
    "    stats = {'linear': {}, 'logarithmic': {}}\n",
    "    \n",
    "    # Define cumulative distribution configurations\n",
    "    cumsum_configs = [\n",
    "        {\n",
    "            'name': 'number',\n",
    "            'avg_column': 'number_normalized_cumsum_avg',\n",
    "            'sd_column': 'number_normalized_cumsum_sd'\n",
    "        },\n",
    "        {\n",
    "            'name': 'volume',\n",
    "            'avg_column': 'volume_nm^3_per_mL_cumsum_avg', \n",
    "            'sd_column': 'volume_nm^3_per_mL_cumsum_sd'\n",
    "        },\n",
    "        {\n",
    "            'name': 'surface_area',\n",
    "            'avg_column': 'area_nm^2_per_mL_cumsum_avg',\n",
    "            'sd_column': 'area_nm^2_per_mL_cumsum_sd'\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Process each scale\n",
    "    for scale in ['linear', 'logarithmic']:\n",
    "        print(f\"\\nCalculating statistics for {scale.upper()} scale:\")\n",
    "        \n",
    "        # Filter data for this scale\n",
    "        scale_df = df[df['scale'] == scale].copy()\n",
    "        if scale_df.empty:\n",
    "            print(f\"  No data available for {scale} scale\")\n",
    "            continue\n",
    "            \n",
    "        # Sort data by size\n",
    "        scale_df = scale_df.sort_values(size_column)\n",
    "        \n",
    "        # Process each cumulative distribution type\n",
    "        for config in cumsum_configs:\n",
    "            name = config['name']\n",
    "            avg_column = config['avg_column']\n",
    "            sd_column = config['sd_column']\n",
    "            \n",
    "            print(f\"  Processing {name}-weighted distribution...\")\n",
    "            \n",
    "            # Skip if required columns don't exist\n",
    "            if avg_column not in scale_df.columns or sd_column not in scale_df.columns:\n",
    "                print(f\"    Skipping - missing columns: {avg_column} or {sd_column}\")\n",
    "                continue\n",
    "            \n",
    "            # Extract data arrays\n",
    "            sizes = scale_df[size_column].values\n",
    "            cumsum_avg = scale_df[avg_column].values\n",
    "            cumsum_sd = scale_df[sd_column].values\n",
    "            \n",
    "            # Skip if all values are zero or NaN\n",
    "            if np.all(cumsum_avg == 0) or np.all(np.isnan(cumsum_avg)):\n",
    "                print(f\"    Skipping - all cumsum values are zero or NaN\")\n",
    "                continue\n",
    "            \n",
    "            # For absolute distributions (volume, surface area), normalize to 0-1 for D-value calculation\n",
    "            # We want to know \"at what size do we have X% of the total volume/surface area\"\n",
    "            if name in ['volume', 'surface_area']:\n",
    "                max_cumsum = np.nanmax(cumsum_avg)\n",
    "                if max_cumsum > 0:\n",
    "                    cumsum_avg = cumsum_avg / max_cumsum\n",
    "                    cumsum_sd = cumsum_sd / max_cumsum\n",
    "                else:\n",
    "                    print(f\"    Skipping - maximum cumsum is zero\")\n",
    "                    continue\n",
    "            \n",
    "            # Calculate D10, D50, D90 with asymmetric bounds\n",
    "            try:\n",
    "                d10_avg, d10_lower, d10_upper = interpolate_d_value_with_bounds(sizes, cumsum_avg, cumsum_sd, 0.1)\n",
    "                d50_avg, d50_lower, d50_upper = interpolate_d_value_with_bounds(sizes, cumsum_avg, cumsum_sd, 0.5)\n",
    "                d90_avg, d90_lower, d90_upper = interpolate_d_value_with_bounds(sizes, cumsum_avg, cumsum_sd, 0.9)\n",
    "                \n",
    "                # Calculate span with bounds\n",
    "                # span = (D90 - D10) / D50\n",
    "                if not np.isnan(d10_avg) and not np.isnan(d50_avg) and not np.isnan(d90_avg) and d50_avg > 0:\n",
    "                    span_avg = (d90_avg - d10_avg) / d50_avg\n",
    "                    \n",
    "                    # For span bounds, we need to consider which combinations give min/max\n",
    "                    # span_lower = min of all possible combinations\n",
    "                    # span_upper = max of all possible combinations\n",
    "                    possible_spans = [\n",
    "                        (d90_lower - d10_upper) / d50_upper,  # All values that make span smaller\n",
    "                        (d90_lower - d10_upper) / d50_lower,\n",
    "                        (d90_upper - d10_lower) / d50_upper,\n",
    "                        (d90_upper - d10_lower) / d50_lower   # All values that make span larger\n",
    "                    ]\n",
    "                    \n",
    "                    # Filter out invalid spans (negative denominators)\n",
    "                    valid_spans = [s for s in possible_spans if not np.isnan(s) and np.isfinite(s)]\n",
    "                    \n",
    "                    if valid_spans:\n",
    "                        span_lower = min(valid_spans)\n",
    "                        span_upper = max(valid_spans)\n",
    "                    else:\n",
    "                        span_lower, span_upper = np.nan, np.nan\n",
    "                else:\n",
    "                    span_avg, span_lower, span_upper = np.nan, np.nan, np.nan\n",
    "                \n",
    "                # Store results with bounds\n",
    "                metrics = {\n",
    "                    'D10_avg': d10_avg,\n",
    "                    'D10_lower': d10_lower,\n",
    "                    'D10_upper': d10_upper,\n",
    "                    'D50_avg': d50_avg,\n",
    "                    'D50_lower': d50_lower,\n",
    "                    'D50_upper': d50_upper,\n",
    "                    'D90_avg': d90_avg,\n",
    "                    'D90_lower': d90_lower,\n",
    "                    'D90_upper': d90_upper,\n",
    "                    'span_avg': span_avg,\n",
    "                    'span_lower': span_lower,\n",
    "                    'span_upper': span_upper\n",
    "                }\n",
    "                \n",
    "                stats[scale][name] = metrics\n",
    "                \n",
    "                print(f\"    ‚úì D10: {d10_avg:.2f} nm ({d10_lower:.2f} - {d10_upper:.2f})\")\n",
    "                print(f\"    ‚úì D50: {d50_avg:.2f} nm ({d50_lower:.2f} - {d50_upper:.2f})\")\n",
    "                print(f\"    ‚úì D90: {d90_avg:.2f} nm ({d90_lower:.2f} - {d90_upper:.2f})\")\n",
    "                print(f\"    ‚úì Span: {span_avg:.3f} ({span_lower:.3f} - {span_upper:.3f})\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"    Error calculating statistics for {name}: {str(e)}\")\n",
    "    \n",
    "    return stats\n",
    "\n",
    "\n",
    "def create_comprehensive_statistics_table(stats, total_metrics):\n",
    "    \"\"\"\n",
    "    Create a comprehensive wide-format table with percentile statistics and total metrics.\n",
    "    \n",
    "    Parameters:\n",
    "    stats (dict): Dictionary of calculated percentile statistics\n",
    "    total_metrics (dict): Dictionary of total metrics from cell05.2\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: Wide-format table with all statistics\n",
    "    \"\"\"\n",
    "    # Initialize list to store table rows\n",
    "    table_data = []\n",
    "    \n",
    "    # Process each scale\n",
    "    for scale in ['linear', 'logarithmic']:\n",
    "        if scale not in stats or not stats[scale]:\n",
    "            continue\n",
    "            \n",
    "        # Get total metrics for this scale\n",
    "        scale_totals = total_metrics.get(scale, {})\n",
    "        \n",
    "        # Process each distribution type\n",
    "        for dist_type in ['number', 'volume', 'surface_area']:\n",
    "            if dist_type not in stats[scale]:\n",
    "                continue\n",
    "                \n",
    "            metrics = stats[scale][dist_type]\n",
    "            \n",
    "            # Prepare distribution name for table\n",
    "            if dist_type == 'number':\n",
    "                dist_name = 'number_normalized'\n",
    "            elif dist_type == 'volume':\n",
    "                dist_name = 'volume_per_mL' \n",
    "            else:\n",
    "                dist_name = 'surface_area_per_mL'\n",
    "            \n",
    "            # Create row data\n",
    "            row = {\n",
    "                'scale': scale,\n",
    "                'distribution': dist_name,\n",
    "                'D10': metrics.get('D10_avg', np.nan),\n",
    "                'D10_lower': metrics.get('D10_lower', np.nan),\n",
    "                'D10_upper': metrics.get('D10_upper', np.nan),\n",
    "                'D50': metrics.get('D50_avg', np.nan),\n",
    "                'D50_lower': metrics.get('D50_lower', np.nan),\n",
    "                'D50_upper': metrics.get('D50_upper', np.nan),\n",
    "                'D90': metrics.get('D90_avg', np.nan),\n",
    "                'D90_lower': metrics.get('D90_lower', np.nan),\n",
    "                'D90_upper': metrics.get('D90_upper', np.nan),\n",
    "                'span': metrics.get('span_avg', np.nan),\n",
    "                'span_lower': metrics.get('span_lower', np.nan),\n",
    "                'span_upper': metrics.get('span_upper', np.nan)\n",
    "            }\n",
    "            \n",
    "            # Add total metrics (same for all distribution types within a scale)\n",
    "            row.update({\n",
    "                'total_particles_per_mL': scale_totals.get('total_particles_per_mL_avg', np.nan),\n",
    "                'total_particles_sd': scale_totals.get('total_particles_per_mL_sd', np.nan),\n",
    "                'total_volume_uL_per_mL': scale_totals.get('total_volume_uL_per_mL_avg', np.nan),\n",
    "                'total_volume_sd': scale_totals.get('total_volume_uL_per_mL_sd', np.nan),\n",
    "                'total_surface_area_cm2_per_mL': scale_totals.get('total_surface_area_cm^2_per_mL_avg', np.nan),\n",
    "                'total_surface_area_sd': scale_totals.get('total_surface_area_cm^2_per_mL_sd', np.nan),\n",
    "                'volume_percentage': scale_totals.get('volume_percentage_avg', np.nan),\n",
    "                'volume_percentage_sd': scale_totals.get('volume_percentage_sd', np.nan)\n",
    "            })\n",
    "            \n",
    "            # Calculate specific surface area with uncertainty\n",
    "            total_area = scale_totals.get('total_surface_area_cm^2_per_mL_avg', np.nan)\n",
    "            total_area_sd = scale_totals.get('total_surface_area_cm^2_per_mL_sd', np.nan)\n",
    "            total_volume = scale_totals.get('total_volume_uL_per_mL_avg', np.nan)  # in ¬µL/mL\n",
    "            total_volume_sd = scale_totals.get('total_volume_uL_per_mL_sd', np.nan)\n",
    "            \n",
    "            if not np.isnan(total_area) and not np.isnan(total_volume) and total_volume > 0:\n",
    "                # Convert volume from ¬µL/mL to cm¬≥/mL: 1 ¬µL = 1e-3 cm¬≥\n",
    "                volume_cm3_per_mL = total_volume * 1e-3\n",
    "                volume_cm3_per_mL_sd = total_volume_sd * 1e-3\n",
    "                \n",
    "                # Specific surface area = area (cm¬≤/mL) / volume (cm¬≥/mL) = cm‚Åª¬π\n",
    "                # Convert to m¬≤/cm¬≥: 1 cm‚Åª¬π = 100 m¬≤/cm¬≥\n",
    "                ssa_avg = (total_area / volume_cm3_per_mL) / 100  # Convert to m¬≤/cm¬≥\n",
    "                \n",
    "                # Uncertainty propagation for ratio: œÉ_R = R √ó ‚àö((œÉ_A/A)¬≤ + (œÉ_V/V)¬≤)\n",
    "                if not np.isnan(total_area_sd) and not np.isnan(volume_cm3_per_mL_sd):\n",
    "                    rel_area_error = total_area_sd / total_area\n",
    "                    rel_volume_error = volume_cm3_per_mL_sd / volume_cm3_per_mL\n",
    "                    ssa_sd = ssa_avg * np.sqrt(rel_area_error**2 + rel_volume_error**2)\n",
    "                else:\n",
    "                    ssa_sd = np.nan\n",
    "                \n",
    "                row['specific_surface_area_m2_per_cm3'] = ssa_avg\n",
    "                row['specific_surface_area_sd'] = ssa_sd\n",
    "            else:\n",
    "                row['specific_surface_area_m2_per_cm3'] = np.nan\n",
    "                row['specific_surface_area_sd'] = np.nan\n",
    "            \n",
    "            table_data.append(row)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    stats_table = pd.DataFrame(table_data)\n",
    "    \n",
    "    return stats_table\n",
    "\n",
    "\n",
    "def save_statistics_table(stats_table, uniqueID, output_dir=None, config=None):\n",
    "    \"\"\"\n",
    "    Save the comprehensive statistics table to a file.\n",
    "    \n",
    "    Parameters:\n",
    "    stats_table (DataFrame): Statistics table to save\n",
    "    uniqueID (str): Unique identifier for the dataset\n",
    "    output_dir (str): Directory to save the file (optional)\n",
    "    config (dict): Configuration dictionary (optional)\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (success_flag, filepath)\n",
    "    \"\"\"\n",
    "    # Determine output directory\n",
    "    if output_dir is None:\n",
    "        if config is not None and \"directory\" in config:\n",
    "            base_dir = config[\"directory\"]\n",
    "            output_dir = os.path.join(base_dir, \"processed\")\n",
    "        else:\n",
    "            output_dir = os.path.join(os.getcwd(), \"processed\")\n",
    "    \n",
    "    # Ensure output directory exists\n",
    "    try:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "    except Exception as e:\n",
    "        return False, f\"Failed to create output directory: {str(e)}\"\n",
    "    \n",
    "    # Create filepath\n",
    "    stats_path = os.path.join(output_dir, f\"Stats_{uniqueID}_comprehensive.txt\")\n",
    "    \n",
    "    try:\n",
    "        # Save as tab-separated file\n",
    "        stats_table.to_csv(stats_path, sep='\\t', index=False)\n",
    "        return True, stats_path\n",
    "    except Exception as e:\n",
    "        return False, f\"Failed to save statistics table: {str(e)}\"\n",
    "\n",
    "\n",
    "def add_key_statistics_to_metadata(metadata, stats):\n",
    "    \"\"\"\n",
    "    Add key D-values (linear number-weighted D10, D50, D90) to metadata.\n",
    "    \n",
    "    Parameters:\n",
    "    metadata (dict): Current metadata dictionary\n",
    "    stats (dict): Dictionary with calculated statistics\n",
    "    \n",
    "    Returns:\n",
    "    dict: Updated metadata dictionary\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying the original\n",
    "    updated_metadata = metadata.copy()\n",
    "    \n",
    "    # Get linear number-weighted statistics\n",
    "    if ('linear' in stats and \n",
    "        'number' in stats['linear'] and \n",
    "        stats['linear']['number']):\n",
    "        \n",
    "        linear_number_stats = stats['linear']['number']\n",
    "        \n",
    "        # Add D-values with bounds to metadata\n",
    "        for param in ['D10', 'D50', 'D90']:\n",
    "            avg_val = linear_number_stats.get(f'{param}_avg', np.nan)\n",
    "            lower_val = linear_number_stats.get(f'{param}_lower', np.nan)\n",
    "            upper_val = linear_number_stats.get(f'{param}_upper', np.nan)\n",
    "            \n",
    "            if not np.isnan(avg_val):\n",
    "                updated_metadata[f'nta_linear_number_{param.lower()}'] = f\"{avg_val:.2f} nm ({lower_val:.2f} - {upper_val:.2f})\"\n",
    "            else:\n",
    "                updated_metadata[f'nta_linear_number_{param.lower()}'] = \"Not available\"\n",
    "        \n",
    "        # Add span with bounds\n",
    "        span_avg = linear_number_stats.get('span_avg', np.nan)\n",
    "        span_lower = linear_number_stats.get('span_lower', np.nan)\n",
    "        span_upper = linear_number_stats.get('span_upper', np.nan)\n",
    "        if not np.isnan(span_avg):\n",
    "            updated_metadata['nta_linear_number_span'] = f\"{span_avg:.3f} ({span_lower:.3f} - {span_upper:.3f})\"\n",
    "        else:\n",
    "            updated_metadata['nta_linear_number_span'] = \"Not available\"\n",
    "    \n",
    "    return updated_metadata\n",
    "\n",
    "\n",
    "# Execute statistics calculation if we have the required data\n",
    "if 'current_distribution_df' in globals() and current_distribution_df is not None:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"CALCULATING PARTICLE SIZE DISTRIBUTION STATISTICS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Check if we have the required cumulative distribution columns\n",
    "    required_columns = [\n",
    "        'number_normalized_cumsum_avg', 'number_normalized_cumsum_sd',\n",
    "        'volume_nm^3_per_mL_cumsum_avg', 'volume_nm^3_per_mL_cumsum_sd',\n",
    "        'area_nm^2_per_mL_cumsum_avg', 'area_nm^2_per_mL_cumsum_sd'\n",
    "    ]\n",
    "    \n",
    "    missing_columns = [col for col in required_columns if col not in current_distribution_df.columns]\n",
    "    \n",
    "    if missing_columns:\n",
    "        print(f\"ERROR: Missing required cumulative distribution columns:\")\n",
    "        for col in missing_columns:\n",
    "            print(f\"  - {col}\")\n",
    "        print(\"\\nPlease run the core calculations (cell05.2) first to generate cumulative distributions.\")\n",
    "    else:\n",
    "        # Calculate statistics with uncertainties\n",
    "        current_stats = calculate_percentile_statistics_with_uncertainty(current_distribution_df)\n",
    "        \n",
    "        # Create comprehensive statistics table\n",
    "        if 'current_total_metrics' in globals():\n",
    "            print(\"\\nCreating comprehensive statistics table...\")\n",
    "            stats_table = create_comprehensive_statistics_table(current_stats, current_total_metrics)\n",
    "            \n",
    "            print(\"Statistics Table:\")\n",
    "            print(\"=\" * 120)\n",
    "            display(stats_table)\n",
    "            \n",
    "            # Save the statistics table\n",
    "            uniqueID = current_metadata.get('persistentID', 'unknown') if 'current_metadata' in globals() else 'unknown'\n",
    "            success, stats_path = save_statistics_table(\n",
    "                stats_table, \n",
    "                uniqueID, \n",
    "                config=CONFIG if 'CONFIG' in globals() else None\n",
    "            )\n",
    "            \n",
    "            if success:\n",
    "                print(f\"\\n‚úì Saved comprehensive statistics table to: {stats_path}\")\n",
    "            else:\n",
    "                print(f\"\\n‚úó Failed to save statistics table: {stats_path}\")\n",
    "            \n",
    "            # Add key statistics to metadata\n",
    "            if 'current_metadata' in globals():\n",
    "                print(\"\\nAdding key D-values to metadata...\")\n",
    "                try:\n",
    "                    current_metadata = add_key_statistics_to_metadata(current_metadata, current_stats)\n",
    "                    \n",
    "                    # Save updated metadata file\n",
    "                    if 'CONFIG' in globals() and \"directory\" in CONFIG:\n",
    "                        metadata_dir = os.path.join(CONFIG[\"directory\"], \"metadata\")\n",
    "                        metadata_path = os.path.join(metadata_dir, f\"Data_{uniqueID}_metadata.txt\")\n",
    "                        \n",
    "                        # Read existing metadata to preserve all fields\n",
    "                        existing_metadata = {}\n",
    "                        if os.path.exists(metadata_path):\n",
    "                            with open(metadata_path, 'r') as f:\n",
    "                                for line in f:\n",
    "                                    parts = line.strip().split('\\t')\n",
    "                                    if len(parts) >= 2:\n",
    "                                        existing_metadata[parts[0]] = parts[1]\n",
    "                        \n",
    "                        # Update with our enhanced metadata\n",
    "                        existing_metadata.update(current_metadata)\n",
    "                        \n",
    "                        # Write back\n",
    "                        with open(metadata_path, 'w') as f:\n",
    "                            for key, value in existing_metadata.items():\n",
    "                                f.write(f\"{key}\\t{value}\\t\\n\")\n",
    "                        \n",
    "                        print(f\"‚úì Updated metadata with key D-values: {metadata_path}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"‚úó Failed to update metadata: {e}\")\n",
    "            \n",
    "            # Store the table for further use\n",
    "            current_stats_table = stats_table\n",
    "            \n",
    "        else:\n",
    "            print(\"\\nWarning: 'current_total_metrics' not found. Creating table with percentile statistics only.\")\n",
    "            # Could create a simplified table here if needed\n",
    "        \n",
    "        print(f\"\\n\" + \"=\" * 80)\n",
    "        print(\"STATISTICS CALCULATION COMPLETED SUCCESSFULLY!\")\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Statistics with uncertainties stored in: current_stats\")\n",
    "        print(\"Comprehensive statistics table stored in: current_stats_table\")\n",
    "        print(\"Key D-values added to metadata.\")\n",
    "        print(\"Ready for visualization and further analysis.\")\n",
    "\n",
    "else:\n",
    "    print(\"No distribution data found in 'current_distribution_df'.\")\n",
    "    print(\"Please run the complete workflow first:\")\n",
    "    print(\"  cell00_multipleFiles ‚Üí cell02_multipleFiles ‚Üí cell03_multipleFiles ‚Üí\")\n",
    "    print(\"  cell04_multipleFiles ‚Üí cell05 ‚Üí cell05.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96adf5d4-2a13-44ba-a80e-d981c7b3c055",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data found. Run the complete workflow first.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "NTA Data Analysis - Number-Weighted Distribution Plots (Cell 10a) - FIXED\n",
    "\n",
    "This module creates two-subplot plots for number-weighted distributions:\n",
    "- Linear scale with lognormal fits (displayed in linear space)\n",
    "- Logarithmic scale with lognormal fits (displayed in log space)\n",
    "\n",
    "Layout: 60% main distribution (top) + 40% cumulative (bottom)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.lines import Line2D\n",
    "from scipy import stats as scipy_stats\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "\n",
    "def lognormal_pdf(x, mu, sigma, amplitude):\n",
    "    \"\"\"Calculate lognormal probability density function.\"\"\"\n",
    "    return amplitude * (1 / (x * sigma * np.sqrt(2 * np.pi))) * np.exp(-(np.log(x) - mu)**2 / (2 * sigma**2))\n",
    "\n",
    "\n",
    "def fit_lognormal_distribution(sizes, weights):\n",
    "    \"\"\"Fit a lognormal distribution to size distribution data.\"\"\"\n",
    "    try:\n",
    "        # Initial parameter estimates\n",
    "        size_log = np.log(sizes)\n",
    "        initial_mu = np.average(size_log, weights=weights)\n",
    "        initial_sigma = np.sqrt(np.average((size_log - initial_mu)**2, weights=weights))\n",
    "        initial_amplitude = np.max(weights) * initial_sigma * np.sqrt(2 * np.pi) * np.exp(initial_mu)\n",
    "        \n",
    "        initial_params = [initial_mu, initial_sigma, initial_amplitude]\n",
    "        \n",
    "        # Perform curve fitting\n",
    "        params, _ = curve_fit(\n",
    "            lognormal_pdf, sizes, weights, p0=initial_params,\n",
    "            bounds=([0, 0, 0], [np.inf, np.inf, np.inf]), maxfev=10000\n",
    "        )\n",
    "        \n",
    "        # Generate fitted curve\n",
    "        size_range = np.linspace(sizes.min(), sizes.max(), 200)\n",
    "        fitted_curve = lognormal_pdf(size_range, *params)\n",
    "        \n",
    "        return True, (size_range, fitted_curve, params)\n",
    "        \n",
    "    except Exception as e:\n",
    "        return False, f\"Lognormal fit failed: {str(e)}\"\n",
    "\n",
    "\n",
    "def add_number_fit_curve_fixed(ax, plot_df, is_log_scale, fit_color='#F25C54'):\n",
    "    \"\"\"Add lognormal fits for number distributions (both linear and log scale).\"\"\"\n",
    "    fit_legend_elements = []\n",
    "    \n",
    "    sizes = plot_df['size_nm'].values\n",
    "    weights = plot_df['number_normalized_avg'].values\n",
    "    \n",
    "    # Remove any zero or negative weights\n",
    "    valid_mask = (weights > 0) & (sizes > 0)\n",
    "    if not np.any(valid_mask):\n",
    "        return fit_legend_elements, None\n",
    "    \n",
    "    sizes, weights = sizes[valid_mask], weights[valid_mask]\n",
    "    \n",
    "    # Use lognormal fit for both linear and log scales\n",
    "    success, result = fit_lognormal_distribution(sizes, weights)\n",
    "    if success:\n",
    "        size_range, fitted_curve, params = result\n",
    "        ax.plot(size_range, fitted_curve, '-', color=fit_color, linewidth=2.5, \n",
    "               alpha=0.9, label='Lognormal Fit', zorder=4)\n",
    "        \n",
    "        geometric_mean = np.exp(params[0])\n",
    "        geometric_std = np.exp(params[1])\n",
    "        \n",
    "        # Add fit info to legend (consistent for both scales)\n",
    "        fit_legend_elements.append(\n",
    "            Line2D([0], [0], color=fit_color, linestyle='-', linewidth=2.5,\n",
    "                  label=f'Lognormal: geo_mean={geometric_mean:.1f} nm, geo_std={geometric_std:.2f}')\n",
    "        )\n",
    "        \n",
    "        return fit_legend_elements, ('lognormal', {'mu': params[0], 'sigma': params[1], 'amplitude': params[2], \n",
    "                                                   'geo_mean': geometric_mean, 'geo_std': geometric_std})\n",
    "    else:\n",
    "        print(f\"    Lognormal fit failed: {result}\")\n",
    "        return fit_legend_elements, None\n",
    "\n",
    "\n",
    "def add_d_value_lines_and_bands(ax, stats):\n",
    "    \"\"\"Add D-value lines and uncertainty bands to a subplot.\"\"\"\n",
    "    legend_elements = []\n",
    "    \n",
    "    if not stats or 'D10_avg' not in stats:\n",
    "        return legend_elements\n",
    "    \n",
    "    d10_avg = stats['D10_avg']\n",
    "    d10_lower = stats.get('D10_lower', d10_avg)\n",
    "    d10_upper = stats.get('D10_upper', d10_avg)\n",
    "    \n",
    "    d50_avg = stats['D50_avg'] \n",
    "    d50_lower = stats.get('D50_lower', d50_avg)\n",
    "    d50_upper = stats.get('D50_upper', d50_avg)\n",
    "    \n",
    "    d90_avg = stats['D90_avg']\n",
    "    d90_lower = stats.get('D90_lower', d90_avg)\n",
    "    d90_upper = stats.get('D90_upper', d90_avg)\n",
    "    \n",
    "    span = stats.get('span_avg', (d90_avg-d10_avg)/d50_avg if d50_avg > 0 else 0)\n",
    "    \n",
    "    # Add D-value lines and bands\n",
    "    for d_val, d_lower, d_upper, style, width, alpha_band in [\n",
    "        (d10_avg, d10_lower, d10_upper, '--', 1.5, 0.15),\n",
    "        (d50_avg, d50_lower, d50_upper, '-', 2.5, 0.25), \n",
    "        (d90_avg, d90_lower, d90_upper, '--', 1.5, 0.15)\n",
    "    ]:\n",
    "        if not np.isnan(d_val):\n",
    "            ax.axvline(x=d_val, color='gray', linestyle=style, alpha=0.8, linewidth=width, zorder=5)\n",
    "            if not np.isnan(d_lower) and not np.isnan(d_upper) and (d_lower != d_val or d_upper != d_val):\n",
    "                ax.axvspan(d_lower, d_upper, alpha=alpha_band, color='gray', zorder=1)\n",
    "    \n",
    "    # Create legend elements\n",
    "    legend_elements.extend([\n",
    "        Line2D([0], [0], color='gray', linestyle='--', linewidth=1.5, \n",
    "              label=f'D10: {d10_avg:.1f} nm ({d10_lower:.1f}-{d10_upper:.1f})'),\n",
    "        Line2D([0], [0], color='gray', linestyle='-', linewidth=2.5, \n",
    "              label=f'D50: {d50_avg:.1f} nm ({d50_lower:.1f}-{d50_upper:.1f})'),\n",
    "        Line2D([0], [0], color='gray', linestyle='--', linewidth=1.5, \n",
    "              label=f'D90: {d90_avg:.1f} nm ({d90_lower:.1f}-{d90_upper:.1f})'),\n",
    "        Line2D([0], [0], color='white', linestyle='', \n",
    "              label=f'Span: {span:.3f}')\n",
    "    ])\n",
    "    \n",
    "    return legend_elements\n",
    "\n",
    "\n",
    "def create_number_plot(plot_df, is_log_scale, stats=None, uniqueID=None, metadata=None):\n",
    "    \"\"\"Create a two-subplot plot for number-weighted distribution.\"\"\"\n",
    "    \n",
    "    scale_name = \"Logarithmic\" if is_log_scale else \"Linear\"\n",
    "    xscale = 'log' if is_log_scale else 'linear'\n",
    "    color = '#4C5B5C'  # Slate gray for number-weighted data\n",
    "    \n",
    "    # Sort by size\n",
    "    plot_df = plot_df.sort_values('size_nm')\n",
    "    \n",
    "    # Create figure\n",
    "    fig = plt.figure(figsize=(7, 9))\n",
    "    gs = gridspec.GridSpec(2, 1, height_ratios=[0.6, 0.4], hspace=0.3, \n",
    "                          top=0.82, bottom=0.08)\n",
    "    \n",
    "    # =================================================================\n",
    "    # TOP SUBPLOT: MAIN DISTRIBUTION WITH ERROR BARS AND FITS\n",
    "    # =================================================================\n",
    "    ax1 = fig.add_subplot(gs[0])\n",
    "    \n",
    "    # Plot main distribution with error bars\n",
    "    if 'number_normalized_sd' in plot_df.columns:\n",
    "        ax1.errorbar(plot_df['size_nm'], plot_df['number_normalized_avg'], \n",
    "                    yerr=plot_df['number_normalized_sd'],\n",
    "                    fmt='o', color=color, ecolor=color, alpha=0.7,\n",
    "                    capsize=3, capthick=1, markersize=6, linewidth=1.5,\n",
    "                    label='Number Distribution')\n",
    "    else:\n",
    "        ax1.scatter(plot_df['size_nm'], plot_df['number_normalized_avg'], \n",
    "                   color=color, s=60, alpha=0.8, label='Number Distribution')\n",
    "    \n",
    "    # Add lognormal fit curve\n",
    "    fit_result = add_number_fit_curve_fixed(ax1, plot_df, is_log_scale)\n",
    "    if isinstance(fit_result, tuple):\n",
    "        fit_legend_elements, fit_results = fit_result\n",
    "    else:\n",
    "        fit_legend_elements = fit_result\n",
    "        fit_results = None\n",
    "    \n",
    "    # Format top subplot\n",
    "    ax1.set_ylabel('Normalized Number', color=color, fontsize=14, labelpad=10)\n",
    "    ax1.tick_params(axis='y', labelcolor=color, labelsize=12)\n",
    "    ax1.tick_params(axis='x', labelsize=12)\n",
    "    ax1.spines['left'].set_color(color)\n",
    "    \n",
    "    # Set x-axis scale and limits\n",
    "    ax1.set_xscale(xscale)\n",
    "    if is_log_scale:\n",
    "        # Log scale: start at 30 nm, focus on signal range\n",
    "        min_size = max(30, plot_df['size_nm'].min())\n",
    "        percentile_90 = np.percentile(plot_df['size_nm'], 90)\n",
    "        max_size = min(percentile_90 * 1.3, 300)\n",
    "        ax1.set_xlim([min_size, max_size])\n",
    "    else:\n",
    "        # Linear scale: start at 0, focus on main signal\n",
    "        percentile_85 = np.percentile(plot_df['size_nm'], 85)\n",
    "        max_size = min(percentile_85 * 1.2, 250)\n",
    "        ax1.set_xlim([0, max_size])\n",
    "    \n",
    "    # Set y-axis to start from 0\n",
    "    y_min, y_max = ax1.get_ylim()\n",
    "    ax1.set_ylim([0, y_max])\n",
    "    \n",
    "    # Add D-value lines and bands\n",
    "    d_legend_elements = add_d_value_lines_and_bands(ax1, stats)\n",
    "    \n",
    "    # Create comprehensive legend for top plot - PLACE OUTSIDE\n",
    "    main_legend = [Line2D([0], [0], marker='o', color='w', markerfacecolor=color, \n",
    "                          markersize=8, label='Number Distribution')]\n",
    "    \n",
    "    all_legend_elements = main_legend + fit_legend_elements + d_legend_elements\n",
    "    leg1 = ax1.legend(handles=all_legend_elements, fontsize=9, frameon=True, \n",
    "                     bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    leg1.get_frame().set_alpha(0.95)\n",
    "    leg1.get_frame().set_edgecolor('lightgray')\n",
    "    \n",
    "    ax1.grid(True, linestyle='--', alpha=0.4)\n",
    "    ax1.set_xlabel('')  # No x-label on top plot\n",
    "    \n",
    "    # =================================================================\n",
    "    # BOTTOM SUBPLOT: CUMULATIVE DISTRIBUTION WITH UNCERTAINTY BANDS\n",
    "    # =================================================================\n",
    "    ax2 = fig.add_subplot(gs[1])\n",
    "    \n",
    "    if 'number_normalized_cumsum_avg' in plot_df.columns:\n",
    "        # Plot cumulative curve as percentage\n",
    "        cumsum_values = plot_df['number_normalized_cumsum_avg'] * 100\n",
    "        ax2.plot(plot_df['size_nm'], cumsum_values, '-', \n",
    "                color=color, linewidth=3, alpha=0.9, label='Cumulative %')\n",
    "        \n",
    "        # Add uncertainty bands if available\n",
    "        if 'number_normalized_cumsum_sd' in plot_df.columns:\n",
    "            cumsum_sd = plot_df['number_normalized_cumsum_sd'] * 100\n",
    "            ax2.fill_between(plot_df['size_nm'], \n",
    "                           cumsum_values - cumsum_sd,\n",
    "                           cumsum_values + cumsum_sd,\n",
    "                           color=color, alpha=0.25, zorder=1, label='¬± SD')\n",
    "        \n",
    "        ax2.set_ylim([0, 110])\n",
    "        ax2.set_ylabel('Cumulative Percentage (%)', color=color, fontsize=14, labelpad=10)\n",
    "        ax2.tick_params(axis='y', labelcolor=color, labelsize=12)\n",
    "        ax2.spines['left'].set_color(color)\n",
    "    \n",
    "    # Format bottom subplot\n",
    "    ax2.set_xlabel('Size (nm)', fontsize=14, labelpad=10)\n",
    "    ax2.tick_params(axis='x', labelsize=12)\n",
    "    ax2.set_xscale(xscale)\n",
    "    ax2.set_xlim(ax1.get_xlim())  # Match top plot limits\n",
    "    \n",
    "    # Add D-value lines to bottom plot\n",
    "    if 'number_normalized_cumsum_avg' in plot_df.columns:\n",
    "        add_d_value_lines_and_bands(ax2, stats)\n",
    "    \n",
    "    # Legend for bottom plot - PLACE OUTSIDE\n",
    "    cumulative_legend = [\n",
    "        Line2D([0], [0], color=color, linewidth=3, label='Cumulative %'),\n",
    "        Line2D([0], [0], color=color, alpha=0.25, linewidth=8, label='¬± SD')\n",
    "    ]\n",
    "    \n",
    "    leg2 = ax2.legend(handles=cumulative_legend, fontsize=10, frameon=True, \n",
    "                     bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    leg2.get_frame().set_alpha(0.95)\n",
    "    leg2.get_frame().set_edgecolor('lightgray')\n",
    "    \n",
    "    ax2.grid(True, linestyle='--', alpha=0.4)\n",
    "    \n",
    "    # =================================================================\n",
    "    # TITLE AND METADATA\n",
    "    # =================================================================\n",
    "    \n",
    "    # Extract replicate info\n",
    "    replicate_info = \"\"\n",
    "    if metadata and 'num_replicates' in metadata:\n",
    "        num_reps = metadata['num_replicates']\n",
    "        if num_reps and str(num_reps) != '1':\n",
    "            replicate_info = f\" (n={num_reps})\"\n",
    "    \n",
    "    # Set main title\n",
    "    main_title = f'{scale_name} Number-Weighted\\nDistribution: {uniqueID}{replicate_info}'\n",
    "    fig.suptitle(main_title, fontsize=14, fontweight='bold', y=0.94)\n",
    "    \n",
    "    # Add subtitle\n",
    "    subtitle = f\"Error bars/bands: ¬± SD | Fits: Lognormal\"\n",
    "    fig.text(0.5, 0.87, subtitle, ha='center', fontsize=11, style='italic')\n",
    "    \n",
    "    return fig, fit_results\n",
    "\n",
    "\n",
    "def generate_number_plots(distribution_df, stats_dict=None, uniqueID=None, \n",
    "                         metadata=None, output_dir=None, config=None):\n",
    "    \"\"\"Generate number-weighted distribution plots for both linear and log scales.\"\"\"\n",
    "    \n",
    "    if distribution_df is None or distribution_df.empty:\n",
    "        return False, \"No data available for plotting\"\n",
    "    \n",
    "    plt.style.use('default')\n",
    "    \n",
    "    # Determine output directory\n",
    "    if output_dir is None:\n",
    "        if config is not None and \"directory\" in config:\n",
    "            base_dir = config[\"directory\"]\n",
    "            output_dir = os.path.join(base_dir, \"processed\")\n",
    "        else:\n",
    "            output_dir = os.path.join(os.getcwd(), \"processed\")\n",
    "    \n",
    "    try:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "    except Exception as e:\n",
    "        return False, f\"Failed to create output directory: {str(e)}\"\n",
    "    \n",
    "    created_files = []\n",
    "    \n",
    "    # Generate linear and logarithmic plots\n",
    "    for is_log_scale in [False, True]:\n",
    "        scale_type = 'logarithmic' if is_log_scale else 'linear'\n",
    "        scale_name = 'log' if is_log_scale else 'linear'\n",
    "        \n",
    "        print(f\"Creating {scale_name} number-weighted plot...\")\n",
    "        \n",
    "        # Filter data for this scale\n",
    "        plot_df = distribution_df[distribution_df['scale'] == scale_type].copy()\n",
    "        \n",
    "        if plot_df.empty:\n",
    "            print(f\"  Warning: No {scale_type} scale data available\")\n",
    "            continue\n",
    "        \n",
    "        # Get statistics\n",
    "        stats = None\n",
    "        if stats_dict and scale_type in stats_dict and 'number' in stats_dict[scale_type]:\n",
    "            stats = stats_dict[scale_type]['number']\n",
    "        \n",
    "        # Create the plot\n",
    "        fig, fit_results = create_number_plot(plot_df, is_log_scale, stats, uniqueID, metadata)\n",
    "        \n",
    "        if fig is None:\n",
    "            print(f\"  Failed to create plot\")\n",
    "            continue\n",
    "        \n",
    "        # Save fit results to comprehensive fits file\n",
    "        if fit_results:\n",
    "            fit_type, fit_data = fit_results\n",
    "            try:\n",
    "                import json\n",
    "                \n",
    "                # Load existing fits file or create new one\n",
    "                fits_filename = f\"Fits_{uniqueID}_all.json\"\n",
    "                fits_path = os.path.join(output_dir, fits_filename)\n",
    "                \n",
    "                if os.path.exists(fits_path):\n",
    "                    with open(fits_path, 'r') as f:\n",
    "                        all_fits = json.load(f)\n",
    "                else:\n",
    "                    all_fits = {'dataset': uniqueID, 'fits': {}}\n",
    "                \n",
    "                # Add this fit to the collection\n",
    "                fit_key = f\"number_{scale_name}\"\n",
    "                all_fits['fits'][fit_key] = {\n",
    "                    'distribution_type': 'number',\n",
    "                    'scale': scale_name,\n",
    "                    'fit_type': fit_type,\n",
    "                    'parameters': fit_data\n",
    "                }\n",
    "                \n",
    "                # Save updated fits file\n",
    "                with open(fits_path, 'w') as f:\n",
    "                    json.dump(all_fits, f, indent=2, default=str)\n",
    "                \n",
    "                print(f\"  ‚úì Saved fit to: {fits_filename}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  ‚ö† Failed to save fit: {str(e)}\")\n",
    "        \n",
    "        # Save the plot\n",
    "        try:\n",
    "            base_filename = f\"Plot_{uniqueID}_{scale_name}_number\"\n",
    "            \n",
    "            pdf_path = os.path.join(output_dir, f\"{base_filename}.pdf\")\n",
    "            fig.savefig(pdf_path, bbox_inches='tight', dpi=300)\n",
    "            \n",
    "            png_path = os.path.join(output_dir, f\"{base_filename}.png\")\n",
    "            fig.savefig(png_path, bbox_inches='tight', dpi=300)\n",
    "            \n",
    "            created_files.append(pdf_path)\n",
    "            print(f\"  ‚úì Saved: {base_filename}.pdf/.png\")\n",
    "            \n",
    "            plt.close(fig)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚úó Failed to save plot: {str(e)}\")\n",
    "            plt.close(fig)\n",
    "            continue\n",
    "    \n",
    "    return True, created_files\n",
    "\n",
    "\n",
    "# Execute if we have the required data\n",
    "if 'current_distribution_df' in globals() and current_distribution_df is not None:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"GENERATING NUMBER-WEIGHTED DISTRIBUTION PLOTS (FIXED VERSION)\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    uniqueID = current_metadata.get('persistentID', 'unknown') if 'current_metadata' in globals() else 'unknown'\n",
    "    stats = current_stats if 'current_stats' in globals() else None\n",
    "    metadata = current_metadata if 'current_metadata' in globals() else None\n",
    "    config = CONFIG if 'CONFIG' in globals() else None\n",
    "    \n",
    "    print(f\"Creating number-weighted plots for: {uniqueID}\")\n",
    "    print(\"Includes: Linear + Logarithmic (Lognormal fits for both)\")\n",
    "    \n",
    "    success, plot_files = generate_number_plots(\n",
    "        current_distribution_df,\n",
    "        stats_dict=stats,\n",
    "        uniqueID=uniqueID,\n",
    "        metadata=metadata,\n",
    "        config=config\n",
    "    )\n",
    "    \n",
    "    if not success:\n",
    "        print(f\"ERROR: {plot_files}\")\n",
    "    else:\n",
    "        print(f\"\\n‚úì Successfully created {len(plot_files)} number-weighted plots!\")\n",
    "        for filepath in plot_files:\n",
    "            print(f\"  - {os.path.basename(filepath)}\")\n",
    "        \n",
    "        current_number_plots = plot_files\n",
    "\n",
    "else:\n",
    "    print(\"No data found. Run the complete workflow first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d164de49-a5cd-4e0a-9006-a5b479f939d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data found. Run the complete workflow first.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "NTA Data Analysis - Volume-Weighted Distribution Plots (Cell 10b) - FIXED\n",
    "\n",
    "This module creates two-subplot plots for volume-weighted distributions:\n",
    "- Linear scale with direct GMM fits (no artificial datasets)\n",
    "- Logarithmic scale with lognormal distribution fits\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.lines import Line2D\n",
    "from scipy import stats as scipy_stats\n",
    "from scipy.optimize import curve_fit, minimize\n",
    "\n",
    "\n",
    "def lognormal_pdf(x, mu, sigma, amplitude):\n",
    "    \"\"\"Calculate lognormal probability density function.\"\"\"\n",
    "    return amplitude * (1 / (x * sigma * np.sqrt(2 * np.pi))) * np.exp(-(np.log(x) - mu)**2 / (2 * sigma**2))\n",
    "\n",
    "\n",
    "def fit_lognormal_distribution(sizes, weights):\n",
    "    \"\"\"Fit a lognormal distribution to size distribution data.\"\"\"\n",
    "    try:\n",
    "        # Initial parameter estimates\n",
    "        size_log = np.log(sizes)\n",
    "        initial_mu = np.average(size_log, weights=weights)\n",
    "        initial_sigma = np.sqrt(np.average((size_log - initial_mu)**2, weights=weights))\n",
    "        initial_amplitude = np.max(weights) * initial_sigma * np.sqrt(2 * np.pi) * np.exp(initial_mu)\n",
    "        \n",
    "        initial_params = [initial_mu, initial_sigma, initial_amplitude]\n",
    "        \n",
    "        # Perform curve fitting\n",
    "        params, _ = curve_fit(\n",
    "            lognormal_pdf, sizes, weights, p0=initial_params,\n",
    "            bounds=([0, 0, 0], [np.inf, np.inf, np.inf]), maxfev=10000\n",
    "        )\n",
    "        \n",
    "        # Generate fitted curve\n",
    "        size_range = np.linspace(sizes.min(), sizes.max(), 200)\n",
    "        fitted_curve = lognormal_pdf(size_range, *params)\n",
    "        \n",
    "        return True, (size_range, fitted_curve, params)\n",
    "        \n",
    "    except Exception as e:\n",
    "        return False, f\"Lognormal fit failed: {str(e)}\"\n",
    "\n",
    "\n",
    "def fit_gaussian_mixture_direct(sizes, volumes, n_components_range=range(1, 4)):\n",
    "    \"\"\"\n",
    "    Fit GMM directly to the distribution data without creating artificial datasets.\n",
    "    \n",
    "    Parameters:\n",
    "    sizes (array): Size values (nm)\n",
    "    volumes (array): Volume values (nm¬≥/mL) \n",
    "    n_components_range: Range of component numbers to try\n",
    "    \n",
    "    Returns:\n",
    "    success, (size_range, total_fit, individual_components, model_info)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Remove zeros and sort by size\n",
    "        valid_mask = (volumes > 0) & (sizes > 0)\n",
    "        if not np.any(valid_mask):\n",
    "            return False, \"No valid data points\"\n",
    "        \n",
    "        sizes_clean = sizes[valid_mask]\n",
    "        volumes_clean = volumes[valid_mask]\n",
    "        \n",
    "        # Sort by size for consistency\n",
    "        sort_idx = np.argsort(sizes_clean)\n",
    "        sizes_clean = sizes_clean[sort_idx]\n",
    "        volumes_clean = volumes_clean[sort_idx]\n",
    "        \n",
    "        if len(sizes_clean) < 6:  # Need minimum data points\n",
    "            return False, \"Insufficient data points for GMM\"\n",
    "        \n",
    "        best_model = None\n",
    "        best_aic = np.inf\n",
    "        \n",
    "        # Try different numbers of components\n",
    "        for n_comp in n_components_range:\n",
    "            try:\n",
    "                # Initial parameter guess\n",
    "                size_min, size_max = sizes_clean.min(), sizes_clean.max()\n",
    "                initial_means = np.linspace(size_min + 0.1*(size_max-size_min), \n",
    "                                          size_max - 0.1*(size_max-size_min), n_comp)\n",
    "                initial_stds = np.full(n_comp, (size_max - size_min) / (n_comp * 3))\n",
    "                initial_weights = np.full(n_comp, 1.0 / n_comp)\n",
    "                \n",
    "                # Pack parameters: [means, stds, weights[:-1]]\n",
    "                initial_params = np.concatenate([initial_means, initial_stds, initial_weights[:-1]])\n",
    "                \n",
    "                def unpack_params(params, n_comp):\n",
    "                    means = params[:n_comp]\n",
    "                    stds = params[n_comp:2*n_comp]\n",
    "                    weights = np.zeros(n_comp)\n",
    "                    weights[:-1] = params[2*n_comp:]\n",
    "                    weights[-1] = 1.0 - np.sum(weights[:-1])  # Ensure sum = 1\n",
    "                    return means, stds, weights\n",
    "                \n",
    "                def gmm_objective(params):\n",
    "                    try:\n",
    "                        means, stds, weights = unpack_params(params, n_comp)\n",
    "                        \n",
    "                        # Check constraints\n",
    "                        if np.any(stds <= 0) or np.any(weights <= 0) or weights[-1] <= 0:\n",
    "                            return 1e10\n",
    "                        \n",
    "                        # Calculate predicted volumes using GMM\n",
    "                        predicted = np.zeros_like(sizes_clean)\n",
    "                        for i in range(n_comp):\n",
    "                            predicted += weights[i] * scipy_stats.norm.pdf(sizes_clean, means[i], stds[i])\n",
    "                        \n",
    "                        # Scale to match data magnitude\n",
    "                        if np.max(predicted) > 0:\n",
    "                            scale_factor = np.sum(volumes_clean) / np.sum(predicted)\n",
    "                            predicted *= scale_factor\n",
    "                        \n",
    "                        # Calculate squared error\n",
    "                        mse = np.mean((volumes_clean - predicted) ** 2)\n",
    "                        return mse\n",
    "                        \n",
    "                    except:\n",
    "                        return 1e10\n",
    "                \n",
    "                # Set bounds\n",
    "                bounds = []\n",
    "                # Bounds for means\n",
    "                for _ in range(n_comp):\n",
    "                    bounds.append((size_min, size_max))\n",
    "                # Bounds for stds\n",
    "                for _ in range(n_comp):\n",
    "                    bounds.append((1.0, size_max - size_min))\n",
    "                # Bounds for weights\n",
    "                for _ in range(n_comp-1):\n",
    "                    bounds.append((0.01, 0.98))\n",
    "                \n",
    "                # Optimize\n",
    "                result = minimize(gmm_objective, initial_params, method='L-BFGS-B', \n",
    "                                bounds=bounds, options={'maxiter': 1000})\n",
    "                \n",
    "                if result.success:\n",
    "                    final_means, final_stds, final_weights = unpack_params(result.x, n_comp)\n",
    "                    \n",
    "                    # Calculate final predicted curve\n",
    "                    predicted_final = np.zeros_like(sizes_clean)\n",
    "                    for i in range(n_comp):\n",
    "                        predicted_final += final_weights[i] * scipy_stats.norm.pdf(sizes_clean, final_means[i], final_stds[i])\n",
    "                    \n",
    "                    scale_factor = np.sum(volumes_clean) / np.sum(predicted_final) if np.sum(predicted_final) > 0 else 1.0\n",
    "                    \n",
    "                    # Calculate AIC\n",
    "                    mse = np.mean((volumes_clean - predicted_final * scale_factor) ** 2)\n",
    "                    n_params = 3 * n_comp - 1\n",
    "                    aic = len(sizes_clean) * np.log(mse) + 2 * n_params\n",
    "                    \n",
    "                    if aic < best_aic:\n",
    "                        best_aic = aic\n",
    "                        best_model = {\n",
    "                            'n_components': n_comp,\n",
    "                            'means': final_means,\n",
    "                            'stds': final_stds,\n",
    "                            'weights': final_weights,\n",
    "                            'scale_factor': scale_factor,\n",
    "                            'aic': aic,\n",
    "                            'mse': mse\n",
    "                        }\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"    Failed to fit {n_comp} components: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if best_model is None:\n",
    "            return False, \"No successful fits\"\n",
    "        \n",
    "        # Generate smooth curves for plotting\n",
    "        size_range = np.linspace(sizes_clean.min(), sizes_clean.max(), 200)\n",
    "        \n",
    "        # Calculate total fitted curve and individual components\n",
    "        total_fit = np.zeros_like(size_range)\n",
    "        individual_components = []\n",
    "        \n",
    "        for i in range(best_model['n_components']):\n",
    "            # Individual component\n",
    "            component = (best_model['weights'][i] * \n",
    "                        scipy_stats.norm.pdf(size_range, best_model['means'][i], best_model['stds'][i]) *\n",
    "                        best_model['scale_factor'])\n",
    "            individual_components.append(component)\n",
    "            total_fit += component\n",
    "        \n",
    "        return True, (size_range, total_fit, individual_components, best_model)\n",
    "        \n",
    "    except Exception as e:\n",
    "        return False, f\"GMM fitting failed: {str(e)}\"\n",
    "\n",
    "\n",
    "def add_volume_fit_curve_fixed(ax, plot_df, is_log_scale, fit_color='#F25C54'):\n",
    "    \"\"\"Add lognormal fits for volume distributions (both linear and log scale).\"\"\"\n",
    "    fit_legend_elements = []\n",
    "    \n",
    "    sizes = plot_df['size_nm'].values\n",
    "    volumes = plot_df['volume_nm^3_per_mL_avg'].values\n",
    "    \n",
    "    # Remove any zero or negative values\n",
    "    valid_mask = (volumes > 0) & (sizes > 0)\n",
    "    if not np.any(valid_mask):\n",
    "        return fit_legend_elements, None\n",
    "    \n",
    "    sizes, volumes = sizes[valid_mask], volumes[valid_mask]\n",
    "    \n",
    "    # Use lognormal fit for both linear and log scales\n",
    "    success, result = fit_lognormal_distribution(sizes, volumes)\n",
    "    if success:\n",
    "        size_range, fitted_curve, params = result\n",
    "        ax.plot(size_range, fitted_curve, '-', color=fit_color, linewidth=2.5, \n",
    "               alpha=0.9, label='Lognormal Fit', zorder=4)\n",
    "        \n",
    "        geometric_mean = np.exp(params[0])\n",
    "        geometric_std = np.exp(params[1])\n",
    "        fit_legend_elements.append(\n",
    "            Line2D([0], [0], color=fit_color, linestyle='-', linewidth=2.5,\n",
    "                  label=f'Lognormal: geo_mean={geometric_mean:.1f} nm, geo_std={geometric_std:.2f}')\n",
    "        )\n",
    "        \n",
    "        return fit_legend_elements, ('lognormal', {'mu': params[0], 'sigma': params[1], 'amplitude': params[2]})\n",
    "    else:\n",
    "        print(f\"    Lognormal fit failed: {result}\")\n",
    "        return fit_legend_elements, None\n",
    "\n",
    "\n",
    "def add_d_value_lines_and_bands(ax, stats):\n",
    "    \"\"\"Add D-value lines and uncertainty bands to a subplot.\"\"\"\n",
    "    legend_elements = []\n",
    "    \n",
    "    if not stats or 'D10_avg' not in stats:\n",
    "        return legend_elements\n",
    "    \n",
    "    d10_avg = stats['D10_avg']\n",
    "    d10_lower = stats.get('D10_lower', d10_avg)\n",
    "    d10_upper = stats.get('D10_upper', d10_avg)\n",
    "    \n",
    "    d50_avg = stats['D50_avg'] \n",
    "    d50_lower = stats.get('D50_lower', d50_avg)\n",
    "    d50_upper = stats.get('D50_upper', d50_avg)\n",
    "    \n",
    "    d90_avg = stats['D90_avg']\n",
    "    d90_lower = stats.get('D90_lower', d90_avg)\n",
    "    d90_upper = stats.get('D90_upper', d90_avg)\n",
    "    \n",
    "    span = stats.get('span_avg', (d90_avg-d10_avg)/d50_avg if d50_avg > 0 else 0)\n",
    "    \n",
    "    # Add D-value lines and bands\n",
    "    for d_val, d_lower, d_upper, style, width, alpha_band in [\n",
    "        (d10_avg, d10_lower, d10_upper, '--', 1.5, 0.15),\n",
    "        (d50_avg, d50_lower, d50_upper, '-', 2.5, 0.25), \n",
    "        (d90_avg, d90_lower, d90_upper, '--', 1.5, 0.15)\n",
    "    ]:\n",
    "        if not np.isnan(d_val):\n",
    "            ax.axvline(x=d_val, color='gray', linestyle=style, alpha=0.8, linewidth=width, zorder=5)\n",
    "            if not np.isnan(d_lower) and not np.isnan(d_upper) and (d_lower != d_val or d_upper != d_val):\n",
    "                ax.axvspan(d_lower, d_upper, alpha=alpha_band, color='gray', zorder=1)\n",
    "    \n",
    "    # Create legend elements\n",
    "    legend_elements.extend([\n",
    "        Line2D([0], [0], color='gray', linestyle='--', linewidth=1.5, \n",
    "              label=f'D10: {d10_avg:.1f} nm ({d10_lower:.1f}-{d10_upper:.1f})'),\n",
    "        Line2D([0], [0], color='gray', linestyle='-', linewidth=2.5, \n",
    "              label=f'D50: {d50_avg:.1f} nm ({d50_lower:.1f}-{d50_upper:.1f})'),\n",
    "        Line2D([0], [0], color='gray', linestyle='--', linewidth=1.5, \n",
    "              label=f'D90: {d90_avg:.1f} nm ({d90_lower:.1f}-{d90_upper:.1f})'),\n",
    "        Line2D([0], [0], color='white', linestyle='', \n",
    "              label=f'Span: {span:.3f}')\n",
    "    ])\n",
    "    \n",
    "    return legend_elements\n",
    "\n",
    "\n",
    "def create_volume_plot(plot_df, is_log_scale, stats=None, uniqueID=None, metadata=None):\n",
    "    \"\"\"Create a two-subplot plot for volume-weighted distribution.\"\"\"\n",
    "    \n",
    "    scale_name = \"Logarithmic\" if is_log_scale else \"Linear\"\n",
    "    xscale = 'log' if is_log_scale else 'linear'\n",
    "    color = '#2E7D32'  # Forest green for volume-weighted\n",
    "    \n",
    "    # Sort by size\n",
    "    plot_df = plot_df.sort_values('size_nm')\n",
    "    \n",
    "    # Create figure\n",
    "    fig = plt.figure(figsize=(7, 9))\n",
    "    gs = gridspec.GridSpec(2, 1, height_ratios=[0.6, 0.4], hspace=0.3, \n",
    "                          top=0.82, bottom=0.08)\n",
    "    \n",
    "    # TOP SUBPLOT: MAIN DISTRIBUTION WITH ERROR BARS AND FITS\n",
    "    ax1 = fig.add_subplot(gs[0])\n",
    "    \n",
    "    # Plot main distribution with error bars\n",
    "    if 'volume_nm^3_per_mL_sd' in plot_df.columns:\n",
    "        ax1.errorbar(plot_df['size_nm'], plot_df['volume_nm^3_per_mL_avg'], \n",
    "                    yerr=plot_df['volume_nm^3_per_mL_sd'],\n",
    "                    fmt='o', color=color, ecolor=color, alpha=0.7,\n",
    "                    capsize=3, capthick=1, markersize=6, linewidth=1.5,\n",
    "                    label='Volume Distribution')\n",
    "    else:\n",
    "        ax1.scatter(plot_df['size_nm'], plot_df['volume_nm^3_per_mL_avg'], \n",
    "                   color=color, s=60, alpha=0.8, label='Volume Distribution')\n",
    "    \n",
    "    # Add fit curve and get fit results\n",
    "    fit_result = add_volume_fit_curve_fixed(ax1, plot_df, is_log_scale)\n",
    "    if isinstance(fit_result, tuple):\n",
    "        fit_legend_elements, fit_results = fit_result\n",
    "    else:\n",
    "        fit_legend_elements = fit_result\n",
    "        fit_results = None\n",
    "    \n",
    "    # Format top subplot\n",
    "    ax1.set_ylabel('Volume (nm¬≥/mL)', color=color, fontsize=14, labelpad=10)\n",
    "    ax1.tick_params(axis='y', labelcolor=color, labelsize=12)\n",
    "    ax1.tick_params(axis='x', labelsize=12)\n",
    "    ax1.spines['left'].set_color(color)\n",
    "    \n",
    "    # Set x-axis scale and add better tick labels for log scale\n",
    "    ax1.set_xscale(xscale)\n",
    "    if is_log_scale:\n",
    "        # Add more detailed log scale ticks\n",
    "        from matplotlib.ticker import LogLocator, LogFormatter\n",
    "        ax1.xaxis.set_major_locator(LogLocator(base=10, numticks=12))\n",
    "        ax1.xaxis.set_minor_locator(LogLocator(base=10, subs=(0.2, 0.4, 0.6, 0.8), numticks=12))\n",
    "        ax1.xaxis.set_major_formatter(LogFormatter(base=10, labelOnlyBase=False))\n",
    "    \n",
    "    # Smart volume-weighted range calculation (restored from original)\n",
    "    weights_for_range = plot_df['volume_nm^3_per_mL_avg'].values\n",
    "    sizes_for_range = plot_df['size_nm'].values\n",
    "    \n",
    "    # Find where 99% of the volume signal is contained\n",
    "    cumsum_weights = np.cumsum(weights_for_range)\n",
    "    total_weight = cumsum_weights[-1]\n",
    "    \n",
    "    # Find 1st and 99th percentiles of the volume-weighted distribution\n",
    "    p1_idx = np.searchsorted(cumsum_weights, 0.01 * total_weight)\n",
    "    p99_idx = np.searchsorted(cumsum_weights, 0.99 * total_weight)\n",
    "    \n",
    "    signal_min = sizes_for_range[max(0, p1_idx)]\n",
    "    signal_max = sizes_for_range[min(len(sizes_for_range)-1, p99_idx)]\n",
    "    data_max = plot_df['size_nm'].max()\n",
    "    \n",
    "    if is_log_scale:\n",
    "        # Log scale: focus on the volume-weighted signal range with some padding\n",
    "        min_size = max(signal_min * 0.7, 20)  # Don't go below 20 nm\n",
    "        max_size = min(signal_max * 2.0, data_max * 1.2)  # Cap at reasonable range\n",
    "        ax1.set_xlim([min_size, max_size])\n",
    "    else:\n",
    "        # Linear scale: tighten the range more, focus on main signal\n",
    "        min_size = 0\n",
    "        max_size = min(signal_max * 1.2, 400)  # Tighter range, cap at 400nm for most cases\n",
    "        ax1.set_xlim([min_size, max_size])\n",
    "    \n",
    "    # Set y-axis to start from 0\n",
    "    y_min, y_max = ax1.get_ylim()\n",
    "    ax1.set_ylim([0, y_max])\n",
    "    \n",
    "    # Add D-value lines and bands\n",
    "    d_legend_elements = add_d_value_lines_and_bands(ax1, stats)\n",
    "    \n",
    "    # Create comprehensive legend for top plot - PLACE OUTSIDE\n",
    "    main_legend = [Line2D([0], [0], marker='o', color='w', markerfacecolor=color, \n",
    "                          markersize=8, label='Volume Distribution')]\n",
    "    \n",
    "    all_legend_elements = main_legend + fit_legend_elements + d_legend_elements\n",
    "    leg1 = ax1.legend(handles=all_legend_elements, fontsize=9, frameon=True, \n",
    "                     bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    leg1.get_frame().set_alpha(0.95)\n",
    "    leg1.get_frame().set_edgecolor('lightgray')\n",
    "    \n",
    "    ax1.grid(True, linestyle='--', alpha=0.4)\n",
    "    ax1.set_xlabel('')\n",
    "    \n",
    "    # BOTTOM SUBPLOT: CUMULATIVE DISTRIBUTION\n",
    "    ax2 = fig.add_subplot(gs[1])\n",
    "    \n",
    "    if 'volume_nm^3_per_mL_cumsum_avg' in plot_df.columns:\n",
    "        cumsum_values = plot_df['volume_nm^3_per_mL_cumsum_avg']\n",
    "        max_cumsum = np.max(cumsum_values)\n",
    "        if max_cumsum > 0:\n",
    "            cumsum_percentage = (cumsum_values / max_cumsum) * 100\n",
    "            ax2.plot(plot_df['size_nm'], cumsum_percentage, '-', \n",
    "                    color=color, linewidth=3, alpha=0.9, label='Cumulative %')\n",
    "            \n",
    "            if 'volume_nm^3_per_mL_cumsum_sd' in plot_df.columns:\n",
    "                cumsum_sd = plot_df['volume_nm^3_per_mL_cumsum_sd']\n",
    "                cumsum_sd_percentage = (cumsum_sd / max_cumsum) * 100\n",
    "                ax2.fill_between(plot_df['size_nm'], \n",
    "                               cumsum_percentage - cumsum_sd_percentage,\n",
    "                               cumsum_percentage + cumsum_sd_percentage,\n",
    "                               color=color, alpha=0.25, zorder=1, label='¬± SD')\n",
    "        \n",
    "        ax2.set_ylim([0, 110])\n",
    "        ax2.set_ylabel('Cumulative Percentage (%)', color=color, fontsize=14, labelpad=10)\n",
    "        ax2.tick_params(axis='y', labelcolor=color, labelsize=12)\n",
    "        ax2.spines['left'].set_color(color)\n",
    "    \n",
    "    # Format bottom subplot\n",
    "    ax2.set_xlabel('Size (nm)', fontsize=14, labelpad=10)\n",
    "    ax2.tick_params(axis='x', labelsize=12)\n",
    "    ax2.set_xscale(xscale)\n",
    "    ax2.set_xlim(ax1.get_xlim())\n",
    "    \n",
    "    # Add D-value lines to bottom plot\n",
    "    if 'volume_nm^3_per_mL_cumsum_avg' in plot_df.columns:\n",
    "        add_d_value_lines_and_bands(ax2, stats)\n",
    "    \n",
    "    # Legend for bottom plot\n",
    "    cumulative_legend = [\n",
    "        Line2D([0], [0], color=color, linewidth=3, label='Cumulative %'),\n",
    "        Line2D([0], [0], color=color, alpha=0.25, linewidth=8, label='¬± SD')\n",
    "    ]\n",
    "    \n",
    "    leg2 = ax2.legend(handles=cumulative_legend, fontsize=10, frameon=True, \n",
    "                     bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    leg2.get_frame().set_alpha(0.95)\n",
    "    leg2.get_frame().set_edgecolor('lightgray')\n",
    "    \n",
    "    ax2.grid(True, linestyle='--', alpha=0.4)\n",
    "    \n",
    "    # TITLE AND METADATA\n",
    "    replicate_info = \"\"\n",
    "    if metadata and 'num_replicates' in metadata:\n",
    "        num_reps = metadata['num_replicates']\n",
    "        if num_reps and str(num_reps) != '1':\n",
    "            replicate_info = f\" (n={num_reps})\"\n",
    "    \n",
    "    main_title = f'{scale_name} Volume-Weighted\\nDistribution: {uniqueID}{replicate_info}'\n",
    "    fig.suptitle(main_title, fontsize=14, fontweight='bold', y=0.94)\n",
    "    \n",
    "    subtitle = f\"Error bars/bands: ¬± SD | Fits: Lognormal\"\n",
    "    fig.text(0.5, 0.87, subtitle, ha='center', fontsize=11, style='italic')\n",
    "    \n",
    "    return fig, fit_results\n",
    "\n",
    "\n",
    "def generate_volume_plots(distribution_df, stats_dict=None, uniqueID=None, \n",
    "                         metadata=None, output_dir=None, config=None):\n",
    "    \"\"\"Generate volume-weighted distribution plots for both linear and log scales.\"\"\"\n",
    "    \n",
    "    if distribution_df is None or distribution_df.empty:\n",
    "        return False, \"No data available for plotting\"\n",
    "    \n",
    "    plt.style.use('default')\n",
    "    \n",
    "    # Determine output directory\n",
    "    if output_dir is None:\n",
    "        if config is not None and \"directory\" in config:\n",
    "            base_dir = config[\"directory\"]\n",
    "            output_dir = os.path.join(base_dir, \"processed\")\n",
    "        else:\n",
    "            output_dir = os.path.join(os.getcwd(), \"processed\")\n",
    "    \n",
    "    try:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "    except Exception as e:\n",
    "        return False, f\"Failed to create output directory: {str(e)}\"\n",
    "    \n",
    "    created_files = []\n",
    "    \n",
    "    # Generate linear and logarithmic plots\n",
    "    for is_log_scale in [False, True]:\n",
    "        scale_type = 'logarithmic' if is_log_scale else 'linear'\n",
    "        scale_name = 'log' if is_log_scale else 'linear'\n",
    "        \n",
    "        print(f\"Creating {scale_name} volume-weighted plot...\")\n",
    "        \n",
    "        # Filter data for this scale\n",
    "        plot_df = distribution_df[distribution_df['scale'] == scale_type].copy()\n",
    "        \n",
    "        if plot_df.empty:\n",
    "            print(f\"  Warning: No {scale_type} scale data available\")\n",
    "            continue\n",
    "        \n",
    "        # Get statistics\n",
    "        stats = None\n",
    "        if stats_dict and scale_type in stats_dict and 'volume' in stats_dict[scale_type]:\n",
    "            stats = stats_dict[scale_type]['volume']\n",
    "        \n",
    "        # Create the plot\n",
    "        fig, fit_results = create_volume_plot(plot_df, is_log_scale, stats, uniqueID, metadata)\n",
    "        \n",
    "        if fig is None:\n",
    "            print(f\"  Failed to create plot\")\n",
    "            continue\n",
    "        \n",
    "        # Save fit results to comprehensive fits file\n",
    "        if fit_results:\n",
    "            fit_type, fit_data = fit_results\n",
    "            try:\n",
    "                import json\n",
    "                \n",
    "                # Load existing fits file or create new one\n",
    "                fits_filename = f\"Fits_{uniqueID}_all.json\"\n",
    "                fits_path = os.path.join(output_dir, fits_filename)\n",
    "                \n",
    "                if os.path.exists(fits_path):\n",
    "                    with open(fits_path, 'r') as f:\n",
    "                        all_fits = json.load(f)\n",
    "                else:\n",
    "                    all_fits = {'dataset': uniqueID, 'fits': {}}\n",
    "                \n",
    "                # Add this fit to the collection\n",
    "                fit_key = f\"volume_{scale_name}\"\n",
    "                all_fits['fits'][fit_key] = {\n",
    "                    'distribution_type': 'volume',\n",
    "                    'scale': scale_name,\n",
    "                    'fit_type': fit_type,\n",
    "                    'parameters': fit_data\n",
    "                }\n",
    "                \n",
    "                # Save updated fits file\n",
    "                with open(fits_path, 'w') as f:\n",
    "                    json.dump(all_fits, f, indent=2, default=str)\n",
    "                \n",
    "                print(f\"  ‚úì Saved fit to: {fits_filename}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  ‚ö† Failed to save fit: {str(e)}\")\n",
    "        \n",
    "        # Save the plot\n",
    "        try:\n",
    "            base_filename = f\"Plot_{uniqueID}_{scale_name}_volume\"\n",
    "            \n",
    "            pdf_path = os.path.join(output_dir, f\"{base_filename}.pdf\")\n",
    "            fig.savefig(pdf_path, bbox_inches='tight', dpi=300)\n",
    "            \n",
    "            png_path = os.path.join(output_dir, f\"{base_filename}.png\")\n",
    "            fig.savefig(png_path, bbox_inches='tight', dpi=300)\n",
    "            \n",
    "            created_files.append(pdf_path)\n",
    "            print(f\"  ‚úì Saved: {base_filename}.pdf/.png\")\n",
    "            \n",
    "            plt.close(fig)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚úó Failed to save plot: {str(e)}\")\n",
    "            plt.close(fig)\n",
    "            continue\n",
    "    \n",
    "    return True, created_files\n",
    "\n",
    "\n",
    "# Execute if we have the required data\n",
    "if 'current_distribution_df' in globals() and current_distribution_df is not None:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"GENERATING VOLUME-WEIGHTED DISTRIBUTION PLOTS (FIXED VERSION)\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    uniqueID = current_metadata.get('persistentID', 'unknown') if 'current_metadata' in globals() else 'unknown'\n",
    "    stats = current_stats if 'current_stats' in globals() else None\n",
    "    metadata = current_metadata if 'current_metadata' in globals() else None\n",
    "    config = CONFIG if 'CONFIG' in globals() else None\n",
    "    \n",
    "    print(f\"Creating volume-weighted plots for: {uniqueID}\")\n",
    "    print(\"Includes: Linear + Logarithmic (Lognormal fits for both)\")\n",
    "    \n",
    "    success, plot_files = generate_volume_plots(\n",
    "        current_distribution_df,\n",
    "        stats_dict=stats,\n",
    "        uniqueID=uniqueID,\n",
    "        metadata=metadata,\n",
    "        config=config\n",
    "    )\n",
    "    \n",
    "    if not success:\n",
    "        print(f\"ERROR: {plot_files}\")\n",
    "    else:\n",
    "        print(f\"\\n‚úì Successfully created {len(plot_files)} volume-weighted plots!\")\n",
    "        for filepath in plot_files:\n",
    "            print(f\"  - {os.path.basename(filepath)}\")\n",
    "        \n",
    "        current_volume_plots = plot_files\n",
    "\n",
    "else:\n",
    "    print(\"No data found. Run the complete workflow first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85099e83-1edc-47a8-b89d-a4635c9fb43c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data found. Run the complete workflow first.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "NTA Data Analysis - Surface Area-Weighted Distribution Plots (Cell 10c)\n",
    "\n",
    "This module creates two-subplot plots for surface area-weighted distributions:\n",
    "- Linear scale with lognormal fits (displayed in linear space)\n",
    "- Logarithmic scale with lognormal fits (displayed in log space)\n",
    "\n",
    "Layout: 60% main distribution (top) + 40% cumulative (bottom)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.lines import Line2D\n",
    "from scipy import stats as scipy_stats\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "\n",
    "def lognormal_pdf(x, mu, sigma, amplitude):\n",
    "    \"\"\"Calculate lognormal probability density function.\"\"\"\n",
    "    return amplitude * (1 / (x * sigma * np.sqrt(2 * np.pi))) * np.exp(-(np.log(x) - mu)**2 / (2 * sigma**2))\n",
    "\n",
    "\n",
    "def fit_lognormal_distribution(sizes, weights):\n",
    "    \"\"\"Fit a lognormal distribution to size distribution data.\"\"\"\n",
    "    try:\n",
    "        # Initial parameter estimates\n",
    "        size_log = np.log(sizes)\n",
    "        initial_mu = np.average(size_log, weights=weights)\n",
    "        initial_sigma = np.sqrt(np.average((size_log - initial_mu)**2, weights=weights))\n",
    "        initial_amplitude = np.max(weights) * initial_sigma * np.sqrt(2 * np.pi) * np.exp(initial_mu)\n",
    "        \n",
    "        initial_params = [initial_mu, initial_sigma, initial_amplitude]\n",
    "        \n",
    "        # Perform curve fitting\n",
    "        params, _ = curve_fit(\n",
    "            lognormal_pdf, sizes, weights, p0=initial_params,\n",
    "            bounds=([0, 0, 0], [np.inf, np.inf, np.inf]), maxfev=10000\n",
    "        )\n",
    "        \n",
    "        # Generate fitted curve\n",
    "        size_range = np.linspace(sizes.min(), sizes.max(), 200)\n",
    "        fitted_curve = lognormal_pdf(size_range, *params)\n",
    "        \n",
    "        return True, (size_range, fitted_curve, params)\n",
    "        \n",
    "    except Exception as e:\n",
    "        return False, f\"Lognormal fit failed: {str(e)}\"\n",
    "\n",
    "\n",
    "def add_surface_area_fit_curve(ax, plot_df, is_log_scale, fit_color='#F25C54'):\n",
    "    \"\"\"Add lognormal fits for surface area distributions (both linear and log scale).\"\"\"\n",
    "    fit_legend_elements = []\n",
    "    \n",
    "    sizes = plot_df['size_nm'].values\n",
    "    surface_areas = plot_df['area_nm^2_per_mL_avg'].values\n",
    "    \n",
    "    # Remove any zero or negative values\n",
    "    valid_mask = (surface_areas > 0) & (sizes > 0)\n",
    "    if not np.any(valid_mask):\n",
    "        return fit_legend_elements, None\n",
    "    \n",
    "    sizes, surface_areas = sizes[valid_mask], surface_areas[valid_mask]\n",
    "    \n",
    "    # Use lognormal fit for both linear and log scales\n",
    "    success, result = fit_lognormal_distribution(sizes, surface_areas)\n",
    "    if success:\n",
    "        size_range, fitted_curve, params = result\n",
    "        ax.plot(size_range, fitted_curve, '-', color=fit_color, linewidth=2.5, \n",
    "               alpha=0.9, label='Lognormal Fit', zorder=4)\n",
    "        \n",
    "        geometric_mean = np.exp(params[0])\n",
    "        geometric_std = np.exp(params[1])\n",
    "        \n",
    "        # Add fit info to legend (consistent for both scales)\n",
    "        fit_legend_elements.append(\n",
    "            Line2D([0], [0], color=fit_color, linestyle='-', linewidth=2.5,\n",
    "                  label=f'Lognormal: geo_mean={geometric_mean:.1f} nm, geo_std={geometric_std:.2f}')\n",
    "        )\n",
    "        \n",
    "        return fit_legend_elements, ('lognormal', {'mu': params[0], 'sigma': params[1], 'amplitude': params[2]})\n",
    "    else:\n",
    "        print(f\"    Lognormal fit failed: {result}\")\n",
    "        return fit_legend_elements, None\n",
    "\n",
    "\n",
    "def add_d_value_lines_and_bands(ax, stats):\n",
    "    \"\"\"Add D-value lines and uncertainty bands to a subplot.\"\"\"\n",
    "    legend_elements = []\n",
    "    \n",
    "    if not stats or 'D10_avg' not in stats:\n",
    "        return legend_elements\n",
    "    \n",
    "    d10_avg = stats['D10_avg']\n",
    "    d10_lower = stats.get('D10_lower', d10_avg)\n",
    "    d10_upper = stats.get('D10_upper', d10_avg)\n",
    "    \n",
    "    d50_avg = stats['D50_avg'] \n",
    "    d50_lower = stats.get('D50_lower', d50_avg)\n",
    "    d50_upper = stats.get('D50_upper', d50_avg)\n",
    "    \n",
    "    d90_avg = stats['D90_avg']\n",
    "    d90_lower = stats.get('D90_lower', d90_avg)\n",
    "    d90_upper = stats.get('D90_upper', d90_avg)\n",
    "    \n",
    "    span = stats.get('span_avg', (d90_avg-d10_avg)/d50_avg if d50_avg > 0 else 0)\n",
    "    \n",
    "    # Add D-value lines and bands\n",
    "    for d_val, d_lower, d_upper, style, width, alpha_band in [\n",
    "        (d10_avg, d10_lower, d10_upper, '--', 1.5, 0.15),\n",
    "        (d50_avg, d50_lower, d50_upper, '-', 2.5, 0.25), \n",
    "        (d90_avg, d90_lower, d90_upper, '--', 1.5, 0.15)\n",
    "    ]:\n",
    "        if not np.isnan(d_val):\n",
    "            ax.axvline(x=d_val, color='gray', linestyle=style, alpha=0.8, linewidth=width, zorder=5)\n",
    "            if not np.isnan(d_lower) and not np.isnan(d_upper) and (d_lower != d_val or d_upper != d_val):\n",
    "                ax.axvspan(d_lower, d_upper, alpha=alpha_band, color='gray', zorder=1)\n",
    "    \n",
    "    # Create legend elements\n",
    "    legend_elements.extend([\n",
    "        Line2D([0], [0], color='gray', linestyle='--', linewidth=1.5, \n",
    "              label=f'D10: {d10_avg:.1f} nm ({d10_lower:.1f}-{d10_upper:.1f})'),\n",
    "        Line2D([0], [0], color='gray', linestyle='-', linewidth=2.5, \n",
    "              label=f'D50: {d50_avg:.1f} nm ({d50_lower:.1f}-{d50_upper:.1f})'),\n",
    "        Line2D([0], [0], color='gray', linestyle='--', linewidth=1.5, \n",
    "              label=f'D90: {d90_avg:.1f} nm ({d90_lower:.1f}-{d90_upper:.1f})'),\n",
    "        Line2D([0], [0], color='white', linestyle='', \n",
    "              label=f'Span: {span:.3f}')\n",
    "    ])\n",
    "    \n",
    "    return legend_elements\n",
    "\n",
    "\n",
    "def create_surface_area_plot(plot_df, is_log_scale, stats=None, uniqueID=None, metadata=None):\n",
    "    \"\"\"Create a two-subplot plot for surface area-weighted distribution.\"\"\"\n",
    "    \n",
    "    scale_name = \"Logarithmic\" if is_log_scale else \"Linear\"\n",
    "    xscale = 'log' if is_log_scale else 'linear'\n",
    "    color = '#4059AD'  # Indigo blue for surface area-weighted\n",
    "    \n",
    "    # Sort by size\n",
    "    plot_df = plot_df.sort_values('size_nm')\n",
    "    \n",
    "    # Create figure\n",
    "    fig = plt.figure(figsize=(7, 9))\n",
    "    gs = gridspec.GridSpec(2, 1, height_ratios=[0.6, 0.4], hspace=0.3, \n",
    "                          top=0.82, bottom=0.08)\n",
    "    \n",
    "    # =================================================================\n",
    "    # TOP SUBPLOT: MAIN DISTRIBUTION WITH ERROR BARS AND FITS\n",
    "    # =================================================================\n",
    "    ax1 = fig.add_subplot(gs[0])\n",
    "    \n",
    "    # Plot main distribution with error bars\n",
    "    if 'area_nm^2_per_mL_sd' in plot_df.columns:\n",
    "        ax1.errorbar(plot_df['size_nm'], plot_df['area_nm^2_per_mL_avg'], \n",
    "                    yerr=plot_df['area_nm^2_per_mL_sd'],\n",
    "                    fmt='o', color=color, ecolor=color, alpha=0.7,\n",
    "                    capsize=3, capthick=1, markersize=6, linewidth=1.5,\n",
    "                    label='Surface Area Distribution')\n",
    "    else:\n",
    "        ax1.scatter(plot_df['size_nm'], plot_df['area_nm^2_per_mL_avg'], \n",
    "                   color=color, s=60, alpha=0.8, label='Surface Area Distribution')\n",
    "    \n",
    "    # Add lognormal fit curve\n",
    "    fit_result = add_surface_area_fit_curve(ax1, plot_df, is_log_scale)\n",
    "    if isinstance(fit_result, tuple):\n",
    "        fit_legend_elements, fit_results = fit_result\n",
    "    else:\n",
    "        fit_legend_elements = fit_result\n",
    "        fit_results = None\n",
    "    \n",
    "    # Format top subplot\n",
    "    ax1.set_ylabel('Surface Area (nm¬≤/mL)', color=color, fontsize=14, labelpad=10)\n",
    "    ax1.tick_params(axis='y', labelcolor=color, labelsize=12)\n",
    "    ax1.tick_params(axis='x', labelsize=12)\n",
    "    ax1.spines['left'].set_color(color)\n",
    "    \n",
    "    # Set x-axis scale and add better tick labels for log scale\n",
    "    ax1.set_xscale(xscale)\n",
    "    if is_log_scale:\n",
    "        from matplotlib.ticker import LogLocator, LogFormatter\n",
    "        ax1.xaxis.set_major_locator(LogLocator(base=10, numticks=12))\n",
    "        ax1.xaxis.set_minor_locator(LogLocator(base=10, subs=(0.2, 0.4, 0.6, 0.8), numticks=12))\n",
    "        ax1.xaxis.set_major_formatter(LogFormatter(base=10, labelOnlyBase=False))\n",
    "    \n",
    "    # Smart surface area-weighted range calculation\n",
    "    weights_for_range = plot_df['area_nm^2_per_mL_avg'].values\n",
    "    sizes_for_range = plot_df['size_nm'].values\n",
    "    \n",
    "    # Find where 99% of the surface area signal is contained\n",
    "    cumsum_weights = np.cumsum(weights_for_range)\n",
    "    total_weight = cumsum_weights[-1]\n",
    "    \n",
    "    # Find 1st and 99th percentiles of the surface area-weighted distribution\n",
    "    p1_idx = np.searchsorted(cumsum_weights, 0.01 * total_weight)\n",
    "    p99_idx = np.searchsorted(cumsum_weights, 0.99 * total_weight)\n",
    "    \n",
    "    signal_min = sizes_for_range[max(0, p1_idx)]\n",
    "    signal_max = sizes_for_range[min(len(sizes_for_range)-1, p99_idx)]\n",
    "    data_max = plot_df['size_nm'].max()\n",
    "    \n",
    "    if is_log_scale:\n",
    "        # Log scale: focus on the surface area-weighted signal range with some padding\n",
    "        min_size = max(signal_min * 0.7, 20)  # Don't go below 20 nm\n",
    "        max_size = min(signal_max * 1.8, data_max * 1.2)  # Surface area less skewed than volume\n",
    "        ax1.set_xlim([min_size, max_size])\n",
    "    else:\n",
    "        # Linear scale: tighten the range more, focus on main signal\n",
    "        min_size = 0\n",
    "        max_size = min(signal_max * 1.15, 350)  # Tighter range for surface area\n",
    "        ax1.set_xlim([min_size, max_size])\n",
    "    \n",
    "    # Set y-axis to start from 0\n",
    "    y_min, y_max = ax1.get_ylim()\n",
    "    ax1.set_ylim([0, y_max])\n",
    "    \n",
    "    # Add D-value lines and bands\n",
    "    d_legend_elements = add_d_value_lines_and_bands(ax1, stats)\n",
    "    \n",
    "    # Create comprehensive legend for top plot - PLACE OUTSIDE\n",
    "    main_legend = [Line2D([0], [0], marker='o', color='w', markerfacecolor=color, \n",
    "                          markersize=8, label='Surface Area Distribution')]\n",
    "    \n",
    "    all_legend_elements = main_legend + fit_legend_elements + d_legend_elements\n",
    "    leg1 = ax1.legend(handles=all_legend_elements, fontsize=9, frameon=True, \n",
    "                     bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    leg1.get_frame().set_alpha(0.95)\n",
    "    leg1.get_frame().set_edgecolor('lightgray')\n",
    "    \n",
    "    ax1.grid(True, linestyle='--', alpha=0.4)\n",
    "    ax1.set_xlabel('')  # No x-label on top plot\n",
    "    \n",
    "    # =================================================================\n",
    "    # BOTTOM SUBPLOT: CUMULATIVE DISTRIBUTION WITH UNCERTAINTY BANDS\n",
    "    # =================================================================\n",
    "    ax2 = fig.add_subplot(gs[1])\n",
    "    \n",
    "    if 'area_nm^2_per_mL_cumsum_avg' in plot_df.columns:\n",
    "        # For surface area cumulative, normalize to percentage\n",
    "        cumsum_values = plot_df['area_nm^2_per_mL_cumsum_avg']\n",
    "        max_cumsum = np.max(cumsum_values)\n",
    "        if max_cumsum > 0:\n",
    "            cumsum_percentage = (cumsum_values / max_cumsum) * 100\n",
    "            ax2.plot(plot_df['size_nm'], cumsum_percentage, '-', \n",
    "                    color=color, linewidth=3, alpha=0.9, label='Cumulative %')\n",
    "            \n",
    "            # Add uncertainty bands if available\n",
    "            if 'area_nm^2_per_mL_cumsum_sd' in plot_df.columns:\n",
    "                cumsum_sd = plot_df['area_nm^2_per_mL_cumsum_sd']\n",
    "                cumsum_sd_percentage = (cumsum_sd / max_cumsum) * 100\n",
    "                ax2.fill_between(plot_df['size_nm'], \n",
    "                               cumsum_percentage - cumsum_sd_percentage,\n",
    "                               cumsum_percentage + cumsum_sd_percentage,\n",
    "                               color=color, alpha=0.25, zorder=1, label='¬± SD')\n",
    "        \n",
    "        ax2.set_ylim([0, 110])\n",
    "        ax2.set_ylabel('Cumulative Percentage (%)', color=color, fontsize=14, labelpad=10)\n",
    "        ax2.tick_params(axis='y', labelcolor=color, labelsize=12)\n",
    "        ax2.spines['left'].set_color(color)\n",
    "    \n",
    "    # Format bottom subplot\n",
    "    ax2.set_xlabel('Size (nm)', fontsize=14, labelpad=10)\n",
    "    ax2.tick_params(axis='x', labelsize=12)\n",
    "    ax2.set_xscale(xscale)\n",
    "    ax2.set_xlim(ax1.get_xlim())  # Match top plot limits\n",
    "    \n",
    "    if is_log_scale:\n",
    "        # Add same detailed log scale ticks to bottom plot\n",
    "        from matplotlib.ticker import LogLocator, LogFormatter\n",
    "        ax2.xaxis.set_major_locator(LogLocator(base=10, numticks=12))\n",
    "        ax2.xaxis.set_minor_locator(LogLocator(base=10, subs=(0.2, 0.4, 0.6, 0.8), numticks=12))\n",
    "        ax2.xaxis.set_major_formatter(LogFormatter(base=10, labelOnlyBase=False))\n",
    "    \n",
    "    # Add D-value lines to bottom plot\n",
    "    if 'area_nm^2_per_mL_cumsum_avg' in plot_df.columns:\n",
    "        add_d_value_lines_and_bands(ax2, stats)\n",
    "    \n",
    "    # Legend for bottom plot - PLACE OUTSIDE\n",
    "    cumulative_legend = [\n",
    "        Line2D([0], [0], color=color, linewidth=3, label='Cumulative %'),\n",
    "        Line2D([0], [0], color=color, alpha=0.25, linewidth=8, label='¬± SD')\n",
    "    ]\n",
    "    \n",
    "    leg2 = ax2.legend(handles=cumulative_legend, fontsize=10, frameon=True, \n",
    "                     bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    leg2.get_frame().set_alpha(0.95)\n",
    "    leg2.get_frame().set_edgecolor('lightgray')\n",
    "    \n",
    "    ax2.grid(True, linestyle='--', alpha=0.4)\n",
    "    \n",
    "    # =================================================================\n",
    "    # TITLE AND METADATA\n",
    "    # =================================================================\n",
    "    \n",
    "    # Extract replicate info\n",
    "    replicate_info = \"\"\n",
    "    if metadata and 'num_replicates' in metadata:\n",
    "        num_reps = metadata['num_replicates']\n",
    "        if num_reps and str(num_reps) != '1':\n",
    "            replicate_info = f\" (n={num_reps})\"\n",
    "    \n",
    "    # Set main title\n",
    "    main_title = f'{scale_name} Surface Area-Weighted\\nDistribution: {uniqueID}{replicate_info}'\n",
    "    fig.suptitle(main_title, fontsize=14, fontweight='bold', y=0.94)\n",
    "    \n",
    "    # Add subtitle\n",
    "    subtitle = f\"Error bars/bands: ¬± SD | Fits: Lognormal\"\n",
    "    fig.text(0.5, 0.87, subtitle, ha='center', fontsize=11, style='italic')\n",
    "    \n",
    "    return fig, fit_results\n",
    "\n",
    "\n",
    "def generate_surface_area_plots(distribution_df, stats_dict=None, uniqueID=None, \n",
    "                               metadata=None, output_dir=None, config=None):\n",
    "    \"\"\"Generate surface area-weighted distribution plots for both linear and log scales.\"\"\"\n",
    "    \n",
    "    if distribution_df is None or distribution_df.empty:\n",
    "        return False, \"No data available for plotting\"\n",
    "    \n",
    "    plt.style.use('default')\n",
    "    \n",
    "    # Determine output directory\n",
    "    if output_dir is None:\n",
    "        if config is not None and \"directory\" in config:\n",
    "            base_dir = config[\"directory\"]\n",
    "            output_dir = os.path.join(base_dir, \"processed\")\n",
    "        else:\n",
    "            output_dir = os.path.join(os.getcwd(), \"processed\")\n",
    "    \n",
    "    try:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "    except Exception as e:\n",
    "        return False, f\"Failed to create output directory: {str(e)}\"\n",
    "    \n",
    "    created_files = []\n",
    "    \n",
    "    # Generate linear and logarithmic plots\n",
    "    for is_log_scale in [False, True]:\n",
    "        scale_type = 'logarithmic' if is_log_scale else 'linear'\n",
    "        scale_name = 'log' if is_log_scale else 'linear'\n",
    "        \n",
    "        print(f\"Creating {scale_name} surface area-weighted plot...\")\n",
    "        \n",
    "        # Filter data for this scale\n",
    "        plot_df = distribution_df[distribution_df['scale'] == scale_type].copy()\n",
    "        \n",
    "        if plot_df.empty:\n",
    "            print(f\"  Warning: No {scale_type} scale data available\")\n",
    "            continue\n",
    "        \n",
    "        # Get statistics\n",
    "        stats = None\n",
    "        if stats_dict and scale_type in stats_dict and 'surface_area' in stats_dict[scale_type]:\n",
    "            stats = stats_dict[scale_type]['surface_area']\n",
    "        \n",
    "        # Create the plot\n",
    "        fig, fit_results = create_surface_area_plot(plot_df, is_log_scale, stats, uniqueID, metadata)\n",
    "        \n",
    "        if fig is None:\n",
    "            print(f\"  Failed to create plot\")\n",
    "            continue\n",
    "        \n",
    "        # Save fit results to comprehensive fits file\n",
    "        if fit_results:\n",
    "            fit_type, fit_data = fit_results\n",
    "            try:\n",
    "                import json\n",
    "                \n",
    "                # Load existing fits file or create new one\n",
    "                fits_filename = f\"Fits_{uniqueID}_all.json\"\n",
    "                fits_path = os.path.join(output_dir, fits_filename)\n",
    "                \n",
    "                if os.path.exists(fits_path):\n",
    "                    with open(fits_path, 'r') as f:\n",
    "                        all_fits = json.load(f)\n",
    "                else:\n",
    "                    all_fits = {'dataset': uniqueID, 'fits': {}}\n",
    "                \n",
    "                # Add this fit to the collection\n",
    "                fit_key = f\"surface_area_{scale_name}\"\n",
    "                all_fits['fits'][fit_key] = {\n",
    "                    'distribution_type': 'surface_area',\n",
    "                    'scale': scale_name,\n",
    "                    'fit_type': fit_type,\n",
    "                    'parameters': fit_data\n",
    "                }\n",
    "                \n",
    "                # Save updated fits file\n",
    "                with open(fits_path, 'w') as f:\n",
    "                    json.dump(all_fits, f, indent=2, default=str)\n",
    "                \n",
    "                print(f\"  ‚úì Saved fit to: {fits_filename}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  ‚ö† Failed to save fit: {str(e)}\")\n",
    "        \n",
    "        # Save the plot\n",
    "        try:\n",
    "            base_filename = f\"Plot_{uniqueID}_{scale_name}_surface_area\"\n",
    "            \n",
    "            pdf_path = os.path.join(output_dir, f\"{base_filename}.pdf\")\n",
    "            fig.savefig(pdf_path, bbox_inches='tight', dpi=300)\n",
    "            \n",
    "            png_path = os.path.join(output_dir, f\"{base_filename}.png\")\n",
    "            fig.savefig(png_path, bbox_inches='tight', dpi=300)\n",
    "            \n",
    "            created_files.append(pdf_path)\n",
    "            print(f\"  ‚úì Saved: {base_filename}.pdf/.png\")\n",
    "            \n",
    "            plt.close(fig)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚úó Failed to save plot: {str(e)}\")\n",
    "            plt.close(fig)\n",
    "            continue\n",
    "    \n",
    "    return True, created_files\n",
    "\n",
    "\n",
    "# Execute if we have the required data\n",
    "if 'current_distribution_df' in globals() and current_distribution_df is not None:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"GENERATING SURFACE AREA-WEIGHTED DISTRIBUTION PLOTS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    uniqueID = current_metadata.get('persistentID', 'unknown') if 'current_metadata' in globals() else 'unknown'\n",
    "    stats = current_stats if 'current_stats' in globals() else None\n",
    "    metadata = current_metadata if 'current_metadata' in globals() else None\n",
    "    config = CONFIG if 'CONFIG' in globals() else None\n",
    "    \n",
    "    print(f\"Creating surface area-weighted plots for: {uniqueID}\")\n",
    "    print(\"Includes: Linear + Logarithmic (Lognormal fits for both)\")\n",
    "    \n",
    "    success, plot_files = generate_surface_area_plots(\n",
    "        current_distribution_df,\n",
    "        stats_dict=stats,\n",
    "        uniqueID=uniqueID,\n",
    "        metadata=metadata,\n",
    "        config=config\n",
    "    )\n",
    "    \n",
    "    if not success:\n",
    "        print(f\"ERROR: {plot_files}\")\n",
    "    else:\n",
    "        print(f\"\\n‚úì Successfully created {len(plot_files)} surface area-weighted plots!\")\n",
    "        for filepath in plot_files:\n",
    "            print(f\"  - {os.path.basename(filepath)}\")\n",
    "        \n",
    "        current_surface_area_plots = plot_files\n",
    "\n",
    "else:\n",
    "    print(\"No data found. Run the complete workflow first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01b34272-71a3-4b61-9fb1-34caa08898cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data found. Run the complete workflow first.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "NTA Data Analysis - Raw Particle Count Visualization with Instrument Settings (Cell 10d)\n",
    "\n",
    "This cell creates plots showing the actual raw particle counts vs. size for averaged data,\n",
    "including critical instrument settings from metadata and error bars for replicate variability.\n",
    "\"\"\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "def plot_raw_counts_with_settings(distribution_df, uniqueID=None, metadata=None, config=None):\n",
    "    \"\"\"\n",
    "    Create visualizations of raw particle counts vs. size for averaged data, with instrument settings.\n",
    "    \n",
    "    Parameters:\n",
    "    distribution_df (DataFrame): Processed NTA data with averaged values\n",
    "    uniqueID (str): Unique identifier for the dataset\n",
    "    metadata (dict): Metadata dictionary\n",
    "    config (dict): Configuration dictionary\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (success_flag, filepath)\n",
    "    \"\"\"\n",
    "    # Validate input\n",
    "    if distribution_df is None or distribution_df.empty:\n",
    "        print(\"No data available for plotting\")\n",
    "        return False, \"No data available for plotting\"\n",
    "    \n",
    "    # Extract sample ID\n",
    "    id_text = uniqueID or (distribution_df['uniqueID'].iloc[0] if 'uniqueID' in distribution_df.columns else 'Unknown')\n",
    "    \n",
    "    # Create figure\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # Define color scheme - same as other plots\n",
    "    color = '#4C5B5C'  # Slate gray for raw counts\n",
    "    \n",
    "    # Determine which columns to use for raw counts (before dilution correction)\n",
    "    raw_count_col = None\n",
    "    raw_count_sd_col = None\n",
    "    \n",
    "    # Look for the averaged number data (before normalization)\n",
    "    if 'number_avg' in distribution_df.columns:\n",
    "        raw_count_col = 'number_avg'\n",
    "        raw_count_sd_col = 'number_sd' if 'number_sd' in distribution_df.columns else None\n",
    "    else:\n",
    "        print(\"Raw count data column not found in the dataset\")\n",
    "        return False, \"Raw count data column not found in the dataset\"\n",
    "    \n",
    "    # Plot 1: Linear scale raw counts\n",
    "    lin_df = distribution_df[distribution_df['scale'] == 'linear'].sort_values('size_nm')\n",
    "    if not lin_df.empty:\n",
    "        # Plot raw counts with error bars if available\n",
    "        if raw_count_sd_col and raw_count_sd_col in lin_df.columns:\n",
    "            ax1.errorbar(lin_df['size_nm'], lin_df[raw_count_col], \n",
    "                        yerr=lin_df[raw_count_sd_col],\n",
    "                        fmt='o', color=color, ecolor=color, alpha=0.7,\n",
    "                        capsize=3, capthick=1, markersize=4, linewidth=1.5)\n",
    "        else:\n",
    "            ax1.scatter(lin_df['size_nm'], lin_df[raw_count_col], color=color, s=30, alpha=0.8)\n",
    "        \n",
    "        ax1.set_title('Raw Particle Counts - Linear Scale', fontsize=14)\n",
    "        ax1.set_xlabel('Size (nm)', fontsize=12)\n",
    "        ax1.set_ylabel('Raw Counts (particles)', fontsize=12, color=color)\n",
    "        ax1.tick_params(axis='y', labelcolor=color)\n",
    "        ax1.spines['left'].set_color(color)\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Set reasonable x-axis limits\n",
    "        percentile_95 = np.percentile(lin_df['size_nm'], 95)\n",
    "        max_size = min(percentile_95 * 1.2, 500)  # Cap at 500 nm or use 95th percentile + 20%\n",
    "        ax1.set_xlim([0, max_size])\n",
    "        \n",
    "        # Set y-axis to start from 0\n",
    "        y_min, y_max = ax1.get_ylim()\n",
    "        ax1.set_ylim([0, y_max])\n",
    "    \n",
    "    # Plot 2: Log scale raw counts \n",
    "    log_df = distribution_df[distribution_df['scale'] == 'logarithmic'].sort_values('size_nm')\n",
    "    if not log_df.empty:\n",
    "        # Plot raw counts with error bars if available\n",
    "        if raw_count_sd_col and raw_count_sd_col in log_df.columns:\n",
    "            ax2.errorbar(log_df['size_nm'], log_df[raw_count_col], \n",
    "                        yerr=log_df[raw_count_sd_col],\n",
    "                        fmt='o', color=color, ecolor=color, alpha=0.7,\n",
    "                        capsize=3, capthick=1, markersize=4, linewidth=1.5)\n",
    "        else:\n",
    "            ax2.scatter(log_df['size_nm'], log_df[raw_count_col], color=color, s=30, alpha=0.8)\n",
    "        \n",
    "        ax2.set_title('Raw Particle Counts - Log Scale', fontsize=14)\n",
    "        ax2.set_xlabel('Size (nm)', fontsize=12)\n",
    "        ax2.set_ylabel('Raw Counts (particles)', fontsize=12, color=color)\n",
    "        ax2.tick_params(axis='y', labelcolor=color)\n",
    "        ax2.spines['left'].set_color(color)\n",
    "        ax2.set_xscale('log')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Set reasonable x-axis limits for log scale\n",
    "        min_size = max(30, log_df['size_nm'].min())  # Don't go below 30 nm\n",
    "        percentile_90 = np.percentile(log_df['size_nm'], 90)\n",
    "        max_size = min(percentile_90 * 1.3, 300)\n",
    "        ax2.set_xlim([min_size, max_size])\n",
    "        \n",
    "        # Set y-axis to start from 0\n",
    "        y_min, y_max = ax2.get_ylim()\n",
    "        ax2.set_ylim([0, y_max])\n",
    "    \n",
    "    # Determine replicate info for title\n",
    "    replicate_info = \"\"\n",
    "    if metadata and 'num_replicates' in metadata:\n",
    "        num_reps = metadata['num_replicates']\n",
    "        if num_reps and str(num_reps) != '1':\n",
    "            replicate_info = f\" (n={num_reps} replicates)\"\n",
    "    \n",
    "    # Add overall title\n",
    "    fig.suptitle(f'Raw Particle Counts vs Size: {id_text}{replicate_info}', fontsize=16, y=0.98)\n",
    "    \n",
    "    # Calculate total raw count (use linear data preferentially)\n",
    "    if not lin_df.empty:\n",
    "        total_raw_count = lin_df[raw_count_col].sum()\n",
    "        if raw_count_sd_col and raw_count_sd_col in lin_df.columns:\n",
    "            # Error propagation for sum: sqrt(sum of variances)\n",
    "            total_raw_count_sd = np.sqrt((lin_df[raw_count_sd_col] ** 2).sum())\n",
    "        else:\n",
    "            total_raw_count_sd = 0\n",
    "    elif not log_df.empty:\n",
    "        total_raw_count = log_df[raw_count_col].sum()\n",
    "        if raw_count_sd_col and raw_count_sd_col in log_df.columns:\n",
    "            total_raw_count_sd = np.sqrt((log_df[raw_count_sd_col] ** 2).sum())\n",
    "        else:\n",
    "            total_raw_count_sd = 0\n",
    "    else:\n",
    "        total_raw_count = 0\n",
    "        total_raw_count_sd = 0\n",
    "    \n",
    "    # Extract and format instrument settings from metadata\n",
    "    instrument_settings = []\n",
    "    if metadata:\n",
    "        # Extract key NTA parameters\n",
    "        cycles = metadata.get('nta_cycles', metadata.get('cycles', 'Unknown'))\n",
    "        if cycles != 'Unknown':\n",
    "            instrument_settings.append(f\"Cycles: {cycles}\")\n",
    "        \n",
    "        fps = metadata.get('nta_fps', metadata.get('fps', 'Unknown'))\n",
    "        if fps != 'Unknown':\n",
    "            instrument_settings.append(f\"FPS: {fps}\")\n",
    "        \n",
    "        # Extract total number of traces (summed across replicates)\n",
    "        traces = metadata.get('nta_number_of_traces_sum', metadata.get('nta_number_of_traces', 'Unknown'))\n",
    "        if traces != 'Unknown':\n",
    "            instrument_settings.append(f\"Total traces: {traces}\")\n",
    "        \n",
    "        # Extract video file info\n",
    "        avi_size = metadata.get('nta_avi_filesize', 'Unknown')\n",
    "        if avi_size != 'Unknown':\n",
    "            instrument_settings.append(f\"Video size: {avi_size}\")\n",
    "        \n",
    "        # Extract temperature\n",
    "        temp = metadata.get('nta_temperature', metadata.get('temperature', 'Unknown'))\n",
    "        if temp != 'Unknown':\n",
    "            instrument_settings.append(f\"Temp: {temp}\")\n",
    "    \n",
    "    # Add explanation text about what raw counts represent\n",
    "    explanation_parts = [\n",
    "        \"Raw counts represent the actual number of particles detected at each size bin,\",\n",
    "        \"before normalization or concentration calculation\"\n",
    "    ]\n",
    "    \n",
    "    if replicate_info:\n",
    "        explanation_parts.append(\"Error bars show ¬± SD across replicates\")\n",
    "    \n",
    "    explanation = \" \".join(explanation_parts)\n",
    "    fig.text(0.5, 0.01, explanation, ha='center', fontsize=10, style='italic')\n",
    "    \n",
    "    # Add data statistics and instrument settings\n",
    "    if total_raw_count_sd > 0:\n",
    "        stats_text = f\"Total raw particles detected: {total_raw_count:.0f} ¬± {total_raw_count_sd:.0f}\"\n",
    "    else:\n",
    "        stats_text = f\"Total raw particles detected: {total_raw_count:.0f}\"\n",
    "    \n",
    "    # Create text block for statistics and settings\n",
    "    text_block = [stats_text]\n",
    "    if instrument_settings:\n",
    "        text_block.append(\"Instrument Settings: \" + \" | \".join(instrument_settings))\n",
    "    \n",
    "    # Add the text block to the figure\n",
    "    fig.text(0.5, 0.04, \"\\n\".join(text_block), ha='center', fontsize=11)\n",
    "    \n",
    "    # Adjust layout to make room for the text\n",
    "    bottom_margin = 0.06 + 0.02 * len(text_block)\n",
    "    plt.tight_layout(rect=[0, bottom_margin, 1, 0.95])\n",
    "    \n",
    "    # Save figure\n",
    "    filepath = None\n",
    "    \n",
    "    # Determine the output directory\n",
    "    if config is not None and \"directory\" in config:\n",
    "        output_dir = os.path.join(config[\"directory\"], \"processed\")\n",
    "    else:\n",
    "        output_dir = os.path.join(os.getcwd(), \"processed\")\n",
    "    \n",
    "    # Ensure the output directory exists\n",
    "    try:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Create descriptive filename\n",
    "        base_path = os.path.join(output_dir, f\"Plot_{id_text}_raw_counts\")\n",
    "        \n",
    "        # Save in both PDF and PNG formats\n",
    "        pdf_path = f\"{base_path}.pdf\"\n",
    "        plt.savefig(pdf_path, bbox_inches='tight', dpi=300)\n",
    "        \n",
    "        png_path = f\"{base_path}.png\"\n",
    "        plt.savefig(png_path, bbox_inches='tight', dpi=300)\n",
    "        \n",
    "        print(f\"‚úì Saved raw count plots: {os.path.basename(pdf_path)}/.png\")\n",
    "        filepath = pdf_path\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö† Failed to save plot: {str(e)}\")\n",
    "        return False, str(e)\n",
    "    \n",
    "    # Update metadata with total raw particle count\n",
    "    if metadata and total_raw_count > 0:\n",
    "        try:\n",
    "            # Get output directory for metadata\n",
    "            if config is not None and \"directory\" in config:\n",
    "                metadata_dir = os.path.join(config[\"directory\"], \"metadata\")\n",
    "            else:\n",
    "                metadata_dir = os.path.join(os.getcwd(), \"metadata\")\n",
    "            \n",
    "            # Ensure metadata directory exists\n",
    "            os.makedirs(metadata_dir, exist_ok=True)\n",
    "            \n",
    "            # Construct metadata filepath\n",
    "            unique_id = metadata.get('persistentID', id_text)\n",
    "            metadata_path = os.path.join(metadata_dir, f\"Data_{unique_id}_metadata.txt\")\n",
    "            \n",
    "            # Read existing metadata to preserve all fields\n",
    "            existing_data = {}\n",
    "            if os.path.exists(metadata_path):\n",
    "                with open(metadata_path, 'r') as f:\n",
    "                    for line in f:\n",
    "                        parts = line.strip().split('\\t')\n",
    "                        if len(parts) >= 2:\n",
    "                            existing_data[parts[0]] = parts[1]\n",
    "            \n",
    "            # Add total raw particle count with uncertainty\n",
    "            if total_raw_count_sd > 0:\n",
    "                existing_data['nta_total_raw_particles'] = f\"{total_raw_count:.0f} ¬± {total_raw_count_sd:.0f}\"\n",
    "            else:\n",
    "                existing_data['nta_total_raw_particles'] = f\"{total_raw_count:.0f}\"\n",
    "            \n",
    "            # Write updated metadata\n",
    "            with open(metadata_path, 'w') as f:\n",
    "                for key, value in existing_data.items():\n",
    "                    f.write(f\"{key}\\t{value}\\t\\n\")\n",
    "            \n",
    "            print(f\"‚úì Updated metadata with total raw particle count\")\n",
    "            \n",
    "            # Update global metadata variable if it exists\n",
    "            if 'current_metadata' in globals():\n",
    "                if total_raw_count_sd > 0:\n",
    "                    globals()['current_metadata']['nta_total_raw_particles'] = f\"{total_raw_count:.0f} ¬± {total_raw_count_sd:.0f}\"\n",
    "                else:\n",
    "                    globals()['current_metadata']['nta_total_raw_particles'] = f\"{total_raw_count:.0f}\"\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö† Could not update metadata: {str(e)}\")\n",
    "    \n",
    "    plt.show()\n",
    "    return True, filepath\n",
    "\n",
    "\n",
    "# Execute if we have the required data\n",
    "if 'current_distribution_df' in globals() and current_distribution_df is not None:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"GENERATING RAW PARTICLE COUNT VISUALIZATION\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    uniqueID = current_metadata.get('persistentID', 'unknown') if 'current_metadata' in globals() else 'unknown'\n",
    "    metadata = current_metadata if 'current_metadata' in globals() else None\n",
    "    config = CONFIG if 'CONFIG' in globals() else None\n",
    "    \n",
    "    print(f\"Creating raw particle count plots for: {uniqueID}\")\n",
    "    \n",
    "    success, filepath = plot_raw_counts_with_settings(\n",
    "        current_distribution_df, \n",
    "        uniqueID, \n",
    "        metadata,\n",
    "        config\n",
    "    )\n",
    "    \n",
    "    if success:\n",
    "        current_raw_count_plot = filepath\n",
    "        print(f\"\\n‚úì Successfully created raw count visualization!\")\n",
    "    else:\n",
    "        print(f\"ERROR: {filepath}\")\n",
    "\n",
    "else:\n",
    "    print(\"No data found. Run the complete workflow first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13566dcb-0013-42eb-b53a-a95a94bf902c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": true,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
