{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c874b02-5be1-4231-830e-293c3db3d557",
   "metadata": {
    "tags": []
   },
   "source": [
    "# NTA Data Analysis Framework\n",
    "\n",
    "## Program Description\n",
    "This comprehensive program analyzes Nanoparticle Tracking Analysis (NTA) data files generated by ZetaView (Particle Metrix QUATT, ZetaView version 8.06.01 SP1). The framework handles both single-file and multi-file (replicate) analysis, performing sophisticated data averaging with uncertainty propagation, calculating key statistical parameters, and generating comprehensive visualizations and output files.\n",
    "\n",
    "## Author\n",
    "- Heike Boehm, Department of Cellular Biophysics, MPI for Medical Research (MPImF-CBP-GS)\n",
    "\n",
    "## References\n",
    "This analysis framework is based on data generated according to:\n",
    "- Particle Metrix GmbH. (2020). Operating Instructions for ZetaView Nanoparticle Tracking Analyzers PMX-120, PMX-220, PMX-420 (Version 4.2).\n",
    "\n",
    "## Complete Analysis Workflow\n",
    "The program follows a linear workflow executed sequentially:\n",
    "\n",
    "### 1. **File Selection Module** (Cell 01)\n",
    "   - Interactive selection of single or multiple NTA files\n",
    "   - Configuration settings and file validation\n",
    "\n",
    "### 2. **File I/O Module** (Cell 02)\n",
    "   - Reading NTA files with latin1 encoding\n",
    "   - Section identification and structure validation\n",
    "\n",
    "### 3. **Data Extraction and Averaging Module** (Cell 03)\n",
    "   - Extraction from linear and logarithmic scales\n",
    "   - **Multi-file averaging**: Bin-by-bin averaging across replicates: xÃÑ·µ¢ = (1/n)Œ£‚±º x·µ¢‚±º\n",
    "   - **Standard deviation calculation**: Sample standard deviation for each bin: s·µ¢ = ‚àö[(1/(n-1))Œ£‚±º(x·µ¢‚±º - xÃÑ·µ¢)¬≤]\n",
    "\n",
    "### 4. **Automated Metadata Extraction** (Cell 04)\n",
    "   - Comprehensive metadata extraction using regex patterns\n",
    "   - Cross-file comparison and quality control flagging\n",
    "\n",
    "### 5. **Core Calculations with Uncertainty Propagation** (Cell 05)\n",
    "   - **Dilution correction**: C‚Çõ‚Çê‚Çò‚Çö‚Çó‚Çë = C‚Çò‚Çë‚Çê‚Çõ·µ§·µ£‚Çëùíπ √ó D, with œÉ‚Çõ‚Çê‚Çò‚Çö‚Çó‚Çë = œÉ‚Çò‚Çë‚Çê‚Çõ·µ§·µ£‚Çëùíπ √ó D\n",
    "   - **Area normalization**: f_norm(x) = f(x) / ‚à´f(x)dx using trapezoidal integration\n",
    "   - **Cumulative uncertainty**: œÉ_cumsum[j] = ‚àö(Œ£·µ¢‚Çå‚ÇÄ ≤ œÉ·µ¢¬≤) for independent errors\n",
    "   - Total metrics calculation with uncertainty combination\n",
    "\n",
    "### 6. **Statistics with Bounds-Based Uncertainty** (Cell 06)\n",
    "   - **D-value calculation**: D10, D50, D90 with asymmetric confidence bounds\n",
    "   - **Bounds-based approach**: Interpolation of cumsum¬±SD curves for uncertainty bounds\n",
    "   - Statistics for number-, volume-, and surface area-weighted distributions\n",
    "\n",
    "### 7. **Visualization Suite** (Cells 10a-10f)\n",
    "   - **Number-weighted plots**: Shape analysis with lognormal fits (linear and log scales)\n",
    "   - **Volume-weighted plots**: Volume contribution analysis with lognormal fits (linear and log scales)\n",
    "   - **Count vs. theoretical surface area plots**: Distribution of different surface area bins with lognormal fits (linear and log scales)\n",
    "   - **Surface area-weighted plots**: Surface area analysis with lognormal fits (linear and log scales)\n",
    "   - **Count vs. theoretical volume plots**: Distribution of different volume bins with lognormal fits (linear and log scales)\n",
    "   - **Raw particle count plots**: Quality control with instrument settings\n",
    "   - Each plot: main distribution + cumulative distribution with error bars and D-value markers\n",
    "\n",
    "## Data Processing and Uncertainty Propagation\n",
    "\n",
    "### Multi-File Averaging\n",
    "For replicate analysis:\n",
    "1. **Bin-by-bin averaging**: xÃÑ·µ¢ = (1/n)Œ£‚±º x·µ¢‚±º for each size bin i across n replicates\n",
    "2. **Sample standard deviation**: s·µ¢ = ‚àö[(1/(n-1))Œ£‚±º(x·µ¢‚±º - xÃÑ·µ¢)¬≤]\n",
    "3. **Error propagation**: Uncertainties maintained through all mathematical operations\n",
    "\n",
    "### Statistical Methods\n",
    "- **Cumulative uncertainty**: œÉ_cumsum[j] = ‚àö(Œ£·µ¢‚Çå‚ÇÄ ≤ œÉ·µ¢¬≤) for cumulative distributions\n",
    "- **Bounds-based D-values**: Asymmetric confidence intervals via interpolation of cumsum¬±SD curves\n",
    "- **Total metrics uncertainty**: œÉ_total = ‚àö(Œ£·µ¢ œÉ·µ¢¬≤) for summed quantities\n",
    "\n",
    "### Distribution Types\n",
    "- **Number-weighted**: Area-normalized for shape analysis: ‚à´f_norm(x)dx = 1\n",
    "- **Volume/Surface area-weighted**: Absolute quantities per mL with dilution correction\n",
    "\n",
    "## Output Files and Visualizations\n",
    "The framework generates a complete set of standardized output files:\n",
    "\n",
    "### Metadata Files\n",
    "- **`Data_[uniqueID]_metadata.txt`**: Comprehensive metadata with measurement parameters, quality control flags, calculated metrics, and total volume/surface area per mL (enabling normalization for samples with identical volumes/areas)\n",
    "\n",
    "### Distribution Data\n",
    "- **`Data_[uniqueID]_PSD.txt`**: Complete particle size distribution data with uncertainties for all scales and distribution types\n",
    "\n",
    "### Statistical Summaries\n",
    "- **`Stats_[uniqueID]_comprehensive.txt`**: D-values, span, and total metrics with uncertainty bounds for all distribution types\n",
    "\n",
    "### Visualization Files\n",
    "- **Number-weighted plots**: `Plot_[uniqueID]_linear_number.pdf/png` and `Plot_[uniqueID]_log_number.pdf/png`\n",
    "- **Volume-weighted plots**: `Plot_[uniqueID]_linear_volume.pdf/png` and `Plot_[uniqueID]_log_volume.pdf/png`\n",
    "- **Surface area-weighted plots**: `Plot_[uniqueID]_linear_surface_area.pdf/png` and `Plot_[uniqueID]_log_surface_area.pdf/png`\n",
    "- **Quality control plots**: `Plot_[uniqueID]_raw_counts.pdf/png`\n",
    "- **Count-vs-theoretical surface area plots**: `Plot_[uniqueID]_linear_count_vs_surface_area.pdf/png` and `Plot_[uniqueID]_log_count_vs_surface_area.pdf/png`\n",
    "- **Count-vs-theoretical volume area plots**: `Plot_[uniqueID]_linear_count_vs_volume.pdf/png` and `Plot_[uniqueID]_log_count_vs_volume.pdf/png`\n",
    "\n",
    "### Model Fitting Results\n",
    "- **`Fits_[uniqueID]_all.json`**: Comprehensive fitting parameters for all distribution types and scales\n",
    "\n",
    "## Key Features\n",
    "- **Replicate handling**: Automatic averaging with rigorous uncertainty quantification\n",
    "- **Quality control**: Built-in validation and alert systems  \n",
    "- **Multiple weightings**: Number, volume, and surface area perspectives\n",
    "- **Comprehensive statistics**: D-values with asymmetric confidence bounds\n",
    "- **Standardized output**: Consistent file formats for downstream analysis\n",
    "- **Total metrics**: Volume and surface area per mL saved in metadata for sample comparison\n",
    "\n",
    "## Usage Notes\n",
    "- Execute cells sequentially from Cell 01 through visualization cells\n",
    "- Single-file analysis: standard deviations = 0, complete pipeline still applies\n",
    "- Configuration settings in Cell 01 must match your directory structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29530ae2-8dc8-4681-af6f-446abcb70fc9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "NTA DATA ANALYSIS - MULTI-FILE SELECTION\n",
      "================================================================================\n",
      "Current directory: /Users/hboehm/Seafile/LEAF/NTA_data/!Inbox/20250527\n",
      "\n",
      "Found 47 NTA files:\n",
      "  0: 20250527_0004_23052025_SUV_5136664_after6weeks_size_488.txt\n",
      "  1: 20250527_0013_23052025_SUV_5268891_after3weeks_size_488_11pos.txt\n",
      "  2: 20250527_0011_23052025_SUV_5268891_after3weeks_size_488.txt\n",
      "  3: 20250527_0018_23052025_SUV_5269296_after3weeks_size_488.txt\n",
      "  4: 2025-03-25_142258_nvs.txt\n",
      "  5: 20250527_0008_23052025_SUV_519687_after7weeks_size_488_11pos.txt\n",
      "  6: 20250527_0012_23052025_SUV_5268891_after3weeks_size_488_11pos.txt\n",
      "  7: 20250527_0014_23052025_SUV_5268891_after3weeks_size_488_11pos.txt\n",
      "  8: 20250527_0019_23052025_SUV_5268891_4GradC_size_488_11pos.txt\n",
      "  9: 20250527_0015_23052025_SUV_5269296_after3weeks_size_488.txt\n",
      "  10: 20250527_0025_23052025_SUV_5268891_FrozenwithBSA_size_488.txt\n",
      "  11: 20250527_0005_23052025_SUV_5136664_after6weeks_size_488.txt\n",
      "  12: 20250527_0023_23052025_SUV_5268891_Frozen_size_488_11pos.txt\n",
      "  13: 20250527_0023_23052025_SUV_5268891_Frozen_size_488.txt\n",
      "  14: 20250527_0004_23052025_SUV_5136664_after6weeks_size_488_11pos.txt\n",
      "  15: 20250527_0021_23052025_SUV_5268891_4GradC_size_488_11pos.txt\n",
      "  16: 20250527_0006_23052025_SUV_5136664_after6weeks_size_488.txt\n",
      "  17: 20250527_0016_23052025_SUV_5269296_after3weeks_size_488.txt\n",
      "  18: 20250527_0013_23052025_SUV_5268891_after3weeks_size_488.txt\n",
      "  19: 20250527_0025_23052025_SUV_5268891_FrozenwithBSA_size_488_11pos.txt\n",
      "  20: 20250527_0005_23052025_SUV_5136664_after6weeks_size_488_11pos.txt\n",
      "  21: 20250527_0024_23052025_SUV_5268891_Frozen_size_488.txt\n",
      "  22: 20250527_0012_23052025_SUV_5268891_after3weeks_size_488.txt\n",
      "  23: 20250527_0017_23052025_SUV_5269296_after3weeks_size_488.txt\n",
      "  24: 20250527_0009_23052025_SUV_519687_after7weeks_size_488.txt\n",
      "  25: 20250527_0016_23052025_SUV_5269296_after3weeks_size_488_11pos.txt\n",
      "  26: 20250527_0024_23052025_SUV_5268891_Frozen_size_488_11pos.txt\n",
      "  27: 20250527_0027_23052025_SUV_5268891_FrozenwithBSA_size_488_11pos.txt\n",
      "  28: 20250527_0021_23052025_SUV_5268891_4GradC_size_488.txt\n",
      "  29: 20250527_0017_23052025_SUV_5269296_after3weeks_size_488_11pos.txt\n",
      "  30: 20250527_0027_23052025_SUV_5268891_FrozenwithBSA_size_488.txt\n",
      "  31: 20250527_0009_23052025_SUV_519687_after7weeks_size_488_11pos.txt\n",
      "  32: 20250527_0008_23052025_SUV_519687_after7weeks_size_488.txt\n",
      "  33: 20250527_0011_23052025_SUV_5268891_after3weeks_size_488_11pos.txt\n",
      "  34: 20250527_0022_23052025_SUV_5268891_Frozen_size_488.txt\n",
      "  35: 20250527_0028_23052025_SUV_5268891_FrozenwithBSA_size_488.txt\n",
      "  36: 20250527_0020_23052025_SUV_5268891_4GradC_size_488_11pos.txt\n",
      "  37: 20250527_0014_23052025_SUV_5268891_after3weeks_size_488.txt\n",
      "  38: 20250527_0022_23052025_SUV_5268891_Frozen_size_488_11pos.txt\n",
      "  39: 20250527_0020_23052025_SUV_5268891_4GradC_size_488.txt\n",
      "  40: 20250527_0006_23052025_SUV_5136664_after6weeks_size_488_11pos.txt\n",
      "  41: 20250527_0019_23052025_SUV_5268891_4GradC_size_488.txt\n",
      "  42: 20250527_0028_23052025_SUV_5268891_FrozenwithBSA_size_488_11pos.txt\n",
      "  43: 20250527_0015_23052025_SUV_5269296_after3weeks_size_488_11pos.txt\n",
      "  44: 20250527_0007_23052025_SUV_519687_after7weeks_size_488.txt\n",
      "  45: 20250527_0007_23052025_SUV_519687_after7weeks_size_488_11pos.txt\n",
      "  46: 20250527_0018_23052025_SUV_5269296_after3weeks_size_488_11pos.txt\n",
      "\n",
      "To select files belonging to the same sample for analysis:\n",
      "  Single file: select_files('0')\n",
      "  Multiple files: select_files('0,1,2')\n",
      "  Example: select_files('0,2,5') for files 0, 2, and 5\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "NTA Data Analysis - Multi-file Selection Module (Cell 01)\n",
    "\n",
    "This module provides file selection for single or multiple NTA files\n",
    "for replicate analysis with averaging and standard deviation calculations.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "\n",
    "if 'CONFIG' not in globals():\n",
    "    CONFIG = {\n",
    "        \"directory\": \"Inbox\",  # Update this path\n",
    "        \"file_identifier\": \".txt\",\n",
    "        \"output_subdirs\": [\"metadata\", \"processed\"],\n",
    "        \"nta_concentration_calibration_factor\": 4.61E+5,  # Default ZetaView calibration\n",
    "        \"project_metadata\": {\n",
    "            \"experimenter\": \"Your_Initials\",\n",
    "            \"location\": \"Your_Lab_Location\",\n",
    "            \"project\": \"Your_Project_Name\",\n",
    "            \"meta_version\": \"v03\",\n",
    "            \"pi\": \"Principal_Investigator_Initials\",\n",
    "            \"funding\": \"Funding_Source\",\n",
    "            \"data_collection_method\": \"NTA\",\n",
    "            \"unit_of_analysis\": '[\"nm\", \"nm^2\", \"nm^3\"]',\n",
    "            \"keywords\": \"particle_size_distribution\",\n",
    "            \"publications\": \"None\",\n",
    "        }\n",
    "    }\n",
    "\n",
    "def set_data_directory(directory_path):\n",
    "    \"\"\"\n",
    "    Set or update the data directory in CONFIG.\n",
    "    \n",
    "    Parameters:\n",
    "    directory_path (str): Path to directory containing NTA data\n",
    "    \n",
    "    Returns:\n",
    "    bool: True if directory exists and was set, False otherwise\n",
    "    \"\"\"\n",
    "    global CONFIG\n",
    "    \n",
    "    # Validate directory exists\n",
    "    if not os.path.exists(directory_path):\n",
    "        print(f\"Error: Directory not found: {directory_path}\")\n",
    "        return False\n",
    "    \n",
    "    # Update CONFIG with new directory\n",
    "    CONFIG[\"directory\"] = directory_path\n",
    "    \n",
    "    # Ensure output subdirectories exist\n",
    "    for subdir in CONFIG[\"output_subdirs\"]:\n",
    "        output_dir = os.path.join(directory_path, subdir)\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    print(f\"Data directory set to: {directory_path}\")\n",
    "    print(f\"Output directories created/verified.\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "def find_nta_files():\n",
    "    \"\"\"\n",
    "    Find NTA data files in the current directory.\n",
    "    \n",
    "    Returns:\n",
    "    list: List of NTA files, or empty list if none found\n",
    "    \"\"\"\n",
    "    directory = CONFIG[\"directory\"]\n",
    "    file_identifier = CONFIG[\"file_identifier\"]\n",
    "    \n",
    "    # Validate directory exists\n",
    "    if not os.path.exists(directory):\n",
    "        print(f\"Error: Directory not found: {directory}\")\n",
    "        return []\n",
    "    \n",
    "    # Find all matching files\n",
    "    all_files = os.listdir(directory)\n",
    "    nta_files = [f for f in all_files if f.endswith(file_identifier)]\n",
    "    \n",
    "    return nta_files\n",
    "\n",
    "def extract_sample_info(filename):\n",
    "    \"\"\"\n",
    "    Extract sample information from filename for more readable display.\n",
    "    \n",
    "    Parameters:\n",
    "    filename (str): Filename to analyze\n",
    "    \n",
    "    Returns:\n",
    "    str: Formatted sample information\n",
    "    \"\"\"\n",
    "    # Remove common prefixes and suffixes\n",
    "    base_name = filename.replace(\"_rawdata.txt\", \"\").replace(\"size_NTA\", \"\")\n",
    "    \n",
    "    if base_name.startswith(\"Data_\"):\n",
    "        base_name = base_name[5:]\n",
    "    \n",
    "    # Look for date pattern (e.g., 20250311)\n",
    "    date_match = re.search(r'_(\\d{8})_', base_name)\n",
    "    date_str = \"\"\n",
    "    if date_match:\n",
    "        date = date_match.group(1)\n",
    "        date_str = f\" (Date: {date[0:4]}-{date[4:6]}-{date[6:8]})\"\n",
    "    \n",
    "    return f\"{base_name}{date_str}\"\n",
    "\n",
    "def parse_indices(indices_input):\n",
    "    \"\"\"\n",
    "    Parse file indices from string input.\n",
    "    \n",
    "    Parameters:\n",
    "    indices_input (str): Comma-separated indices (e.g., \"0,1,2\" or \"0\")\n",
    "    \n",
    "    Returns:\n",
    "    list: List of integer indices, or empty list if invalid\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Remove spaces and split by commas\n",
    "        indices_str = indices_input.replace(\" \", \"\").split(\",\")\n",
    "        indices = [int(idx) for idx in indices_str if idx.strip()]\n",
    "        return indices\n",
    "    except ValueError:\n",
    "        print(f\"Error: Invalid indices format. Use comma-separated numbers (e.g., '0,1,2')\")\n",
    "        return []\n",
    "\n",
    "def select_files(indices_input):\n",
    "    \"\"\"\n",
    "    Select multiple files by indices for replicate analysis.\n",
    "    \n",
    "    Parameters:\n",
    "    indices_input (str): Comma-separated string of indices (e.g., \"0,1,2\")\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (filenames_list, filepaths_list) or (None, None) if invalid\n",
    "    \"\"\"\n",
    "    nta_files = find_nta_files()\n",
    "    \n",
    "    if not nta_files:\n",
    "        print(\"No NTA files found in the current directory.\")\n",
    "        return None, None\n",
    "    \n",
    "    # Parse indices\n",
    "    indices = parse_indices(indices_input)\n",
    "    if not indices:\n",
    "        return None, None\n",
    "    \n",
    "    # Validate indices\n",
    "    invalid_indices = [idx for idx in indices if idx < 0 or idx >= len(nta_files)]\n",
    "    if invalid_indices:\n",
    "        print(f\"Invalid file indices: {invalid_indices}. Must be between 0 and {len(nta_files)-1}\")\n",
    "        return None, None\n",
    "    \n",
    "    # Remove duplicates while preserving order\n",
    "    unique_indices = []\n",
    "    for idx in indices:\n",
    "        if idx not in unique_indices:\n",
    "            unique_indices.append(idx)\n",
    "    \n",
    "    # Select files\n",
    "    selected_filenames = []\n",
    "    selected_filepaths = []\n",
    "    \n",
    "    for idx in unique_indices:\n",
    "        filename = nta_files[idx]\n",
    "        filepath = os.path.join(CONFIG[\"directory\"], filename)\n",
    "        selected_filenames.append(filename)\n",
    "        selected_filepaths.append(filepath)\n",
    "    \n",
    "    # Display selection\n",
    "    print(f\"\\nSelected {len(selected_filenames)} file(s) for analysis:\")\n",
    "    for i, (idx, filename) in enumerate(zip(unique_indices, selected_filenames)):\n",
    "        print(f\"  File {i+1}: #{idx} - {filename}\")\n",
    "    \n",
    "    # Set global variables for use in other cells (always as lists)\n",
    "    global selected_filenames_list, selected_filepaths_list, num_replicates\n",
    "    selected_filenames_list = selected_filenames\n",
    "    selected_filepaths_list = selected_filepaths\n",
    "    num_replicates = len(selected_filenames)\n",
    "    \n",
    "    print(f\"\\nNumber of replicates: {num_replicates}\")\n",
    "    if num_replicates == 1:\n",
    "        print(\"Single file analysis - SD values will be 0\")\n",
    "    else:\n",
    "        print(\"Multi-file analysis - averaging with SD calculations\")\n",
    "    \n",
    "    return selected_filenames, selected_filepaths\n",
    "\n",
    "# Set a custom directory (uncomment and modify if needed)\n",
    "# set_data_directory(\"/path/to/your/data\")\n",
    "\n",
    "# List available files\n",
    "print(\"=\" * 80)\n",
    "print(\"NTA DATA ANALYSIS - MULTI-FILE SELECTION\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Current directory: {CONFIG['directory']}\")\n",
    "print()\n",
    "\n",
    "nta_files = find_nta_files()\n",
    "if nta_files:\n",
    "    print(f\"Found {len(nta_files)} NTA files:\")\n",
    "    for i, file in enumerate(nta_files):\n",
    "        print(f\"  {i}: {file}\")\n",
    "    \n",
    "    print(\"\\nTo select files belonging to the same sample for analysis:\")\n",
    "    print(\"  Single file: select_files('0')\")\n",
    "    print(\"  Multiple files: select_files('0,1,2')\")\n",
    "    print(\"  Example: select_files('0,2,5') for files 0, 2, and 5\")\n",
    "else:\n",
    "    print(\"No NTA files found in the current directory.\")\n",
    "    print(\"Please set a different directory using set_data_directory()\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Initialize global variables for file selection (always as lists)\n",
    "selected_filenames_list = None\n",
    "selected_filepaths_list = None\n",
    "num_replicates = 0\n",
    "\n",
    "# Default selection - uncomment to automatically select the first file\n",
    "# if nta_files:\n",
    "#     select_files('0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa30cfa1-4ff8-4ea1-a68a-913dcb8a01ea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selected 3 file(s) for analysis:\n",
      "  File 1: #0 - 20250527_0004_23052025_SUV_5136664_after6weeks_size_488.txt\n",
      "  File 2: #11 - 20250527_0005_23052025_SUV_5136664_after6weeks_size_488.txt\n",
      "  File 3: #16 - 20250527_0006_23052025_SUV_5136664_after6weeks_size_488.txt\n",
      "\n",
      "Number of replicates: 3\n",
      "Multi-file analysis - averaging with SD calculations\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['20250527_0004_23052025_SUV_5136664_after6weeks_size_488.txt',\n",
       "  '20250527_0005_23052025_SUV_5136664_after6weeks_size_488.txt',\n",
       "  '20250527_0006_23052025_SUV_5136664_after6weeks_size_488.txt'],\n",
       " ['/Users/hboehm/Seafile/LEAF/NTA_data/!Inbox/20250527/20250527_0004_23052025_SUV_5136664_after6weeks_size_488.txt',\n",
       "  '/Users/hboehm/Seafile/LEAF/NTA_data/!Inbox/20250527/20250527_0005_23052025_SUV_5136664_after6weeks_size_488.txt',\n",
       "  '/Users/hboehm/Seafile/LEAF/NTA_data/!Inbox/20250527/20250527_0006_23052025_SUV_5136664_after6weeks_size_488.txt'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select_files('0,11,16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9a60bf4-6497-432a-ad49-366467681613",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PROCESSING SELECTED NTA FILES\n",
      "================================================================================\n",
      "Processing 3 file(s)...\n",
      "  Processing: 20250527_0004_23052025_SUV_5136664_after6weeks_size_488.txt\n",
      "    ‚úì Success\n",
      "  Processing: 20250527_0005_23052025_SUV_5136664_after6weeks_size_488.txt\n",
      "    ‚úì Success\n",
      "  Processing: 20250527_0006_23052025_SUV_5136664_after6weeks_size_488.txt\n",
      "    ‚úì Success\n",
      "\n",
      "Processing complete:\n",
      "  Successful: 3 files\n",
      "  Failed: 0 files\n",
      "\n",
      "Successfully processed 3 file(s):\n",
      "  20250527_0004_23052025_SUV_5136664_after6weeks_size_488.txt\n",
      "    Linear section at position: 850\n",
      "    Logarithmic section at position: 56536\n",
      "  20250527_0005_23052025_SUV_5136664_after6weeks_size_488.txt\n",
      "    Linear section at position: 837\n",
      "    Logarithmic section at position: 56523\n",
      "  20250527_0006_23052025_SUV_5136664_after6weeks_size_488.txt\n",
      "    Linear section at position: 836\n",
      "    Logarithmic section at position: 56522\n",
      "\n",
      "File Preview (20250527_0004_23052025_SUV_5136664_after6weeks_size_488.txt):\n",
      "--------------------------------------------------\n",
      "Original File:\tC:\\Users\\ZetaView\\Desktop\\margeaux\\20250527_0004_23052025_SUV_5136664_after6weeks_size_488.avi\tSection:\t0.1 [0]\n",
      "Operator:\tZetaUser\n",
      "Experiment:\t20250527_0004\n",
      "ZetaView S/N:\t20-601\n",
      "Cell S/N:\t018-020-16-465-014-1\n",
      "Software:\tZetaView (version 8.06.01 SP1)\tAnalyze:\t8.06.01 SP1\n",
      "SOP:\tSH_2025\n",
      "Sample:\tSUV_5136664\n",
      "Electrolyte:\tPBS\n",
      "pH:\t7.000000\tentered\n",
      "Conductivity: 13581.000000\tsensed\n",
      "TempControl:\t1\n",
      "SetTemperature:\t24.000000\n",
      "Temperature:\t24.050000\tsensed\n",
      "Viscosity:\t0.919258\n",
      "Date:\t2025-05-27\n",
      "T\n",
      "--------------------------------------------------\n",
      "\n",
      "File processing completed successfully!\n",
      "Ready for data extraction and averaging (Cell 03)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "NTA Data Analysis - Multi-file I/O Module (Cell 02)\n",
    "\n",
    "This module handles:\n",
    "1. Reading multiple NTA data files\n",
    "2. Identifying key sections in file content\n",
    "3. Validating file structure before processing\n",
    "4. Handling errors gracefully by skipping problematic files\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "\n",
    "\n",
    "def read_nta_file(filepath):\n",
    "    \"\"\"\n",
    "    Read an NTA data file with appropriate encoding.\n",
    "    \n",
    "    Parameters:\n",
    "    filepath (str): Path to the NTA file\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (success_flag, result)\n",
    "        - If successful: (True, file_content)\n",
    "        - If failed: (False, error_message)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='latin1') as file:\n",
    "            content = file.read()\n",
    "        \n",
    "        if not content or len(content) < 100:\n",
    "            return False, f\"File appears to be empty or too small: {filepath}\"\n",
    "        \n",
    "        return True, content\n",
    "    except Exception as e:\n",
    "        return False, f\"Error reading file: {str(e)}\"\n",
    "\n",
    "\n",
    "def identify_sections(content):\n",
    "    \"\"\"\n",
    "    Identify key data sections in the file content.\n",
    "    \n",
    "    Parameters:\n",
    "    content (str): File content to analyze\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (success_flag, result)\n",
    "        - If successful: (True, dict_of_sections)\n",
    "        - If failed: (False, error_message)\n",
    "    \"\"\"\n",
    "    sections = {}\n",
    "    \n",
    "    # Find linear size distribution section\n",
    "    size_lin_start = content.find(\"Size Distribution\")\n",
    "    if size_lin_start == -1:\n",
    "        return False, \"Could not find 'Size Distribution' section for linear data\"\n",
    "    sections['linear_start'] = size_lin_start\n",
    "    \n",
    "    # Find logarithmic data section (starts with -1.000E+0 separator)\n",
    "    size_log_start = content.find(\"-1.000E+0\")\n",
    "    if size_log_start == -1:\n",
    "        # Try alternative approach - look for second \"Size / nm\" header\n",
    "        second_header = content.find(\"Size / nm\", size_lin_start + 100)\n",
    "        if second_header == -1:\n",
    "            return False, \"Could not find logarithmic data section\"\n",
    "        sections['logarithmic_start'] = second_header\n",
    "    else:\n",
    "        sections['logarithmic_start'] = size_log_start\n",
    "    \n",
    "    # Validate section order\n",
    "    if sections['linear_start'] >= sections['logarithmic_start']:\n",
    "        return False, \"Invalid file structure: linear section should come before logarithmic section\"\n",
    "    \n",
    "    return True, sections\n",
    "\n",
    "\n",
    "def validate_file_structure(content, sections):\n",
    "    \"\"\"\n",
    "    Validate file structure to ensure it can be processed.\n",
    "    \n",
    "    Parameters:\n",
    "    content (str): File content\n",
    "    sections (dict): Section positions from identify_sections\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (success_flag, message)\n",
    "    \"\"\"\n",
    "    # Check linear section has expected header pattern\n",
    "    linear_section = content[sections['linear_start']:sections['logarithmic_start']]\n",
    "    if not re.search(r'Size / nm\\s+Number\\s+Concentration', linear_section):\n",
    "        return False, \"Missing expected header pattern in linear section\"\n",
    "    \n",
    "    # Check logarithmic section has data in expected format\n",
    "    log_section = content[sections['logarithmic_start']:]\n",
    "    if not re.search(r'[\\d.-]+E[\\+\\-]\\d+\\s+[\\d.-]+E[\\+\\-]\\d+', log_section):\n",
    "        return False, \"Could not find data rows in logarithmic section\"\n",
    "    \n",
    "    return True, \"File structure is valid\"\n",
    "\n",
    "\n",
    "def process_single_file_content(filepath):\n",
    "    \"\"\"\n",
    "    Complete file processing workflow for a single file.\n",
    "    \n",
    "    Parameters:\n",
    "    filepath (str): Path to the NTA file\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (success_flag, result)\n",
    "        - If successful: (True, (content, sections))\n",
    "        - If failed: (False, error_message)\n",
    "    \"\"\"\n",
    "    # Read the file\n",
    "    success, result = read_nta_file(filepath)\n",
    "    if not success:\n",
    "        return False, result\n",
    "    content = result\n",
    "    \n",
    "    # Identify sections\n",
    "    success, result = identify_sections(content)\n",
    "    if not success:\n",
    "        return False, result\n",
    "    sections = result\n",
    "    \n",
    "    # Validate structure\n",
    "    success, message = validate_file_structure(content, sections)\n",
    "    if not success:\n",
    "        return False, message\n",
    "    \n",
    "    return True, (content, sections)\n",
    "\n",
    "\n",
    "def process_multiple_file_contents(filepaths):\n",
    "    \"\"\"\n",
    "    Process multiple NTA files, skipping any that fail with error messages.\n",
    "    \n",
    "    Parameters:\n",
    "    filepaths (list): List of file paths to process\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (file_data_list, error_summary)\n",
    "        - file_data_list: List of (filename, content, sections) for successful files\n",
    "        - error_summary: Dict with failed files and their error messages\n",
    "    \"\"\"\n",
    "    successful_files = []\n",
    "    failed_files = {}\n",
    "    \n",
    "    print(f\"Processing {len(filepaths)} file(s)...\")\n",
    "    \n",
    "    for filepath in filepaths:\n",
    "        filename = os.path.basename(filepath)\n",
    "        print(f\"  Processing: {filename}\")\n",
    "        \n",
    "        success, result = process_single_file_content(filepath)\n",
    "        \n",
    "        if success:\n",
    "            content, sections = result\n",
    "            successful_files.append((filename, content, sections))\n",
    "            print(f\"    ‚úì Success\")\n",
    "        else:\n",
    "            failed_files[filename] = result\n",
    "            print(f\"    ‚úó Failed: {result}\")\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\nProcessing complete:\")\n",
    "    print(f\"  Successful: {len(successful_files)} files\")\n",
    "    print(f\"  Failed: {len(failed_files)} files\")\n",
    "    \n",
    "    if failed_files:\n",
    "        print(\"\\nFailed files:\")\n",
    "        for filename, error in failed_files.items():\n",
    "            print(f\"  {filename}: {error}\")\n",
    "    \n",
    "    return successful_files, failed_files\n",
    "\n",
    "\n",
    "def preview_file_content(content, max_chars=500):\n",
    "    \"\"\"\n",
    "    Display a preview of the file content to verify it was read correctly.\n",
    "    \n",
    "    Parameters:\n",
    "    content (str): File content to preview\n",
    "    max_chars (int): Maximum number of characters to display\n",
    "    \n",
    "    Returns:\n",
    "    str: Preview text for display\n",
    "    \"\"\"\n",
    "    preview = content[:max_chars]\n",
    "    return \"-\" * 50 + \"\\n\" + preview + \"\\n\" + \"-\" * 50\n",
    "\n",
    "\n",
    "# Execute file processing if files were selected in Cell 00\n",
    "if 'selected_filepaths_list' in globals() and selected_filepaths_list:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"PROCESSING SELECTED NTA FILES\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Process all selected files\n",
    "    successful_files, failed_files = process_multiple_file_contents(selected_filepaths_list)\n",
    "    \n",
    "    if successful_files:\n",
    "        print(f\"\\nSuccessfully processed {len(successful_files)} file(s):\")\n",
    "        for filename, content, sections in successful_files:\n",
    "            print(f\"  {filename}\")\n",
    "            print(f\"    Linear section at position: {sections['linear_start']}\")\n",
    "            print(f\"    Logarithmic section at position: {sections['logarithmic_start']}\")\n",
    "        \n",
    "        # Show preview of first file\n",
    "        if successful_files:\n",
    "            first_filename, first_content, first_sections = successful_files[0]\n",
    "            print(f\"\\nFile Preview ({first_filename}):\")\n",
    "            print(preview_file_content(first_content))\n",
    "        \n",
    "        # Store results for use in subsequent cells\n",
    "        current_files_data = successful_files\n",
    "        current_failed_files = failed_files\n",
    "        \n",
    "        print(\"\\nFile processing completed successfully!\")\n",
    "        print(\"Ready for data extraction and averaging (Cell 03)\")\n",
    "        \n",
    "    else:\n",
    "        print(\"\\nERROR: No files could be processed successfully.\")\n",
    "        if failed_files:\n",
    "            print(\"All files failed with errors (see details above).\")\n",
    "        \n",
    "else:\n",
    "    print(\"No files selected. Please run Cell 00 (file selection) first.\")\n",
    "    print(\"Use: select_files('0,1,2') to select files for analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3155e92e-c640-4fa6-a83a-92a708abf155",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "EXTRACTING AND AVERAGING DISTRIBUTION DATA\n",
      "================================================================================\n",
      "Extracting data from: 20250527_0004_23052025_SUV_5136664_after6weeks_size_488.txt\n",
      "  ‚úì Extracted 1400 data points\n",
      "Extracting data from: 20250527_0005_23052025_SUV_5136664_after6weeks_size_488.txt\n",
      "  ‚úì Extracted 1400 data points\n",
      "Extracting data from: 20250527_0006_23052025_SUV_5136664_after6weeks_size_488.txt\n",
      "  ‚úì Extracted 1400 data points\n",
      "\n",
      "Averaging data from 3 successful file(s)...\n",
      "Averaging data from 3 files:\n",
      "  20250527_0004_23052025_SUV_5136664_after6weeks_size_488.txt\n",
      "  20250527_0005_23052025_SUV_5136664_after6weeks_size_488.txt\n",
      "  20250527_0006_23052025_SUV_5136664_after6weeks_size_488.txt\n",
      "Averaging completed:\n",
      "  Total size bins: 1400\n",
      "  Linear scale bins: 1200\n",
      "  Log scale bins: 200\n",
      "\n",
      "Data extraction and averaging completed successfully!\n",
      "Dataset ID: 20250527_0004_23052025_SUV_5136664_after6weeks_size_488_avg3\n",
      "Ready for metadata extraction (Cell 04)\n",
      "\n",
      "Averaged Data Preview:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>size_nm</th>\n",
       "      <th>number_avg</th>\n",
       "      <th>number_sd</th>\n",
       "      <th>concentration_cm-3_avg</th>\n",
       "      <th>concentration_cm-3_sd</th>\n",
       "      <th>volume_nm^3_avg</th>\n",
       "      <th>volume_nm^3_sd</th>\n",
       "      <th>area_nm^2_avg</th>\n",
       "      <th>area_nm^2_sd</th>\n",
       "      <th>scale</th>\n",
       "      <th>num_replicates</th>\n",
       "      <th>source_files</th>\n",
       "      <th>uniqueID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>linear</td>\n",
       "      <td>3</td>\n",
       "      <td>20250527_0004_23052025_SUV_5136664_after6weeks...</td>\n",
       "      <td>20250527_0004_23052025_SUV_5136664_after6weeks...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.5</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>1.527525</td>\n",
       "      <td>32986.666667</td>\n",
       "      <td>33115.746003</td>\n",
       "      <td>368.166667</td>\n",
       "      <td>337.430324</td>\n",
       "      <td>73.620000</td>\n",
       "      <td>67.468609</td>\n",
       "      <td>linear</td>\n",
       "      <td>3</td>\n",
       "      <td>20250527_0004_23052025_SUV_5136664_after6weeks...</td>\n",
       "      <td>20250527_0004_23052025_SUV_5136664_after6weeks...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12.5</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>0.577350</td>\n",
       "      <td>30893.333333</td>\n",
       "      <td>14263.962750</td>\n",
       "      <td>1704.333333</td>\n",
       "      <td>590.051975</td>\n",
       "      <td>204.500000</td>\n",
       "      <td>70.840878</td>\n",
       "      <td>linear</td>\n",
       "      <td>3</td>\n",
       "      <td>20250527_0004_23052025_SUV_5136664_after6weeks...</td>\n",
       "      <td>20250527_0004_23052025_SUV_5136664_after6weeks...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17.5</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>31970.000000</td>\n",
       "      <td>31596.855856</td>\n",
       "      <td>5610.666667</td>\n",
       "      <td>5610.000119</td>\n",
       "      <td>481.066667</td>\n",
       "      <td>481.050001</td>\n",
       "      <td>linear</td>\n",
       "      <td>3</td>\n",
       "      <td>20250527_0004_23052025_SUV_5136664_after6weeks...</td>\n",
       "      <td>20250527_0004_23052025_SUV_5136664_after6weeks...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22.5</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.732051</td>\n",
       "      <td>15796.666667</td>\n",
       "      <td>27360.629257</td>\n",
       "      <td>5963.333333</td>\n",
       "      <td>10328.796316</td>\n",
       "      <td>397.666667</td>\n",
       "      <td>688.778871</td>\n",
       "      <td>linear</td>\n",
       "      <td>3</td>\n",
       "      <td>20250527_0004_23052025_SUV_5136664_after6weeks...</td>\n",
       "      <td>20250527_0004_23052025_SUV_5136664_after6weeks...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   size_nm  number_avg  number_sd  concentration_cm-3_avg  \\\n",
       "0      2.5    0.000000   0.000000                0.000000   \n",
       "1      7.5    1.666667   1.527525            32986.666667   \n",
       "2     12.5    1.666667   0.577350            30893.333333   \n",
       "3     17.5    2.000000   2.000000            31970.000000   \n",
       "4     22.5    1.000000   1.732051            15796.666667   \n",
       "\n",
       "   concentration_cm-3_sd  volume_nm^3_avg  volume_nm^3_sd  area_nm^2_avg  \\\n",
       "0               0.000000         0.000000        0.000000       0.000000   \n",
       "1           33115.746003       368.166667      337.430324      73.620000   \n",
       "2           14263.962750      1704.333333      590.051975     204.500000   \n",
       "3           31596.855856      5610.666667     5610.000119     481.066667   \n",
       "4           27360.629257      5963.333333    10328.796316     397.666667   \n",
       "\n",
       "   area_nm^2_sd   scale  num_replicates  \\\n",
       "0      0.000000  linear               3   \n",
       "1     67.468609  linear               3   \n",
       "2     70.840878  linear               3   \n",
       "3    481.050001  linear               3   \n",
       "4    688.778871  linear               3   \n",
       "\n",
       "                                        source_files  \\\n",
       "0  20250527_0004_23052025_SUV_5136664_after6weeks...   \n",
       "1  20250527_0004_23052025_SUV_5136664_after6weeks...   \n",
       "2  20250527_0004_23052025_SUV_5136664_after6weeks...   \n",
       "3  20250527_0004_23052025_SUV_5136664_after6weeks...   \n",
       "4  20250527_0004_23052025_SUV_5136664_after6weeks...   \n",
       "\n",
       "                                            uniqueID  \n",
       "0  20250527_0004_23052025_SUV_5136664_after6weeks...  \n",
       "1  20250527_0004_23052025_SUV_5136664_after6weeks...  \n",
       "2  20250527_0004_23052025_SUV_5136664_after6weeks...  \n",
       "3  20250527_0004_23052025_SUV_5136664_after6weeks...  \n",
       "4  20250527_0004_23052025_SUV_5136664_after6weeks...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "NTA Data Analysis - Data Extraction and Averaging Module (Cell 03)\n",
    "\n",
    "This module handles:\n",
    "1. Extracting distribution data from multiple NTA files\n",
    "2. Averaging raw particle counts bin-by-bin across replicates\n",
    "3. Calculating standard deviations for each size bin\n",
    "4. Creating a single averaged dataset for downstream analysis\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "\n",
    "def extract_data_section(content, start_pos, end_pos=None, is_log_section=False):\n",
    "    \"\"\"\n",
    "    Extract tabular data from a section of the file content.\n",
    "    \n",
    "    Parameters:\n",
    "    content (str): File content\n",
    "    start_pos (int): Starting position of the section\n",
    "    end_pos (int, optional): Ending position of the section\n",
    "    is_log_section (bool): Whether this is the logarithmic section\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (success_flag, result)\n",
    "        - If successful: (True, (header, data_lines))\n",
    "        - If failed: (False, error_message)\n",
    "    \"\"\"\n",
    "    # Extract the section\n",
    "    section = content[start_pos:end_pos]\n",
    "    \n",
    "    # For logarithmic section, skip past the separator line\n",
    "    if is_log_section:\n",
    "        sep_pos = section.find(\"-1.000E+0\")\n",
    "        if sep_pos != -1:\n",
    "            sep_line_end = section.find('\\n', sep_pos)\n",
    "            if sep_line_end != -1:\n",
    "                section = section[sep_line_end + 1:]\n",
    "            else:\n",
    "                section = section[sep_pos + 20:]\n",
    "        \n",
    "        # Sometimes there's a second \"Size Distribution\" header\n",
    "        second_header = section.find(\"Size Distribution\")\n",
    "        if second_header != -1:\n",
    "            section = section[second_header:]\n",
    "    \n",
    "    # Find the header line\n",
    "    header_match = re.search(r'Size / nm\\s+Number\\s+Concentration.+', section)\n",
    "    \n",
    "    if not header_match and is_log_section:\n",
    "        # For log section, try more relaxed pattern or use default\n",
    "        header_match = re.search(r'Size.*Number', section)\n",
    "        if not header_match:\n",
    "            header_line = \"Size / nm\\tNumber\\tConcentration / cm-3\\tVolume / nm^3\\tArea / nm^2\"\n",
    "            data_lines = []\n",
    "            for line in section.split('\\n'):\n",
    "                if \"-1.000E+0\" in line:\n",
    "                    continue\n",
    "                if re.match(r'^\\s*[\\d.-]+E[\\+\\-]\\d+\\s+[\\d.-]+E[\\+\\-]\\d+', line):\n",
    "                    data_lines.append(line)\n",
    "            \n",
    "            if not data_lines:\n",
    "                return False, \"Could not find any valid data lines in logarithmic section\"\n",
    "            return True, (header_line, data_lines)\n",
    "        else:\n",
    "            header_line = header_match.group(0)\n",
    "            data_start = section.find(header_line) + len(header_line)\n",
    "            data_section = section[data_start:]\n",
    "    else:\n",
    "        if header_match:\n",
    "            header_line = header_match.group(0)\n",
    "            data_start = section.find(header_line) + len(header_line)\n",
    "            data_section = section[data_start:]\n",
    "        else:\n",
    "            return False, \"Could not find header line in data section\"\n",
    "    \n",
    "    # Extract data lines\n",
    "    all_lines = data_section.split('\\n')\n",
    "    data_lines = []\n",
    "    \n",
    "    for line in all_lines:\n",
    "        if \"-1.000E+0\" in line:\n",
    "            continue\n",
    "        if re.match(r'^\\s*[\\d.-]+E[\\+\\-]\\d+\\s+[\\d.-]+E[\\+\\-]\\d+', line):\n",
    "            data_lines.append(line)\n",
    "        elif len(data_lines) > 0 and (line.strip() == '' or line.strip().startswith('-')):\n",
    "            break\n",
    "    \n",
    "    if not data_lines:\n",
    "        return False, \"No data lines found in section\"\n",
    "    \n",
    "    return True, (header_line, data_lines)\n",
    "\n",
    "\n",
    "def parse_data_lines(header_line, data_lines, scale_type):\n",
    "    \"\"\"\n",
    "    Parse data lines into a structured DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    header_line (str): Header line with column names\n",
    "    data_lines (list): List of data lines to parse\n",
    "    scale_type (str): Type of scale ('linear' or 'logarithmic')\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (success_flag, result)\n",
    "        - If successful: (True, DataFrame)\n",
    "        - If failed: (False, error_message)\n",
    "    \"\"\"\n",
    "    parsed_data = []\n",
    "    \n",
    "    for line in data_lines:\n",
    "        if \"-1.000E+0\" in line:\n",
    "            continue\n",
    "            \n",
    "        values = re.findall(r'[\\d.-]+E[\\+\\-]\\d+', line)\n",
    "        \n",
    "        if len(values) >= 5:\n",
    "            try:\n",
    "                if values[0] == \"-1.000E+0\":\n",
    "                    continue\n",
    "                \n",
    "                row = {\n",
    "                    'size_nm': float(values[0]),\n",
    "                    'number': float(values[1]),\n",
    "                    'concentration_cm-3': float(values[2]),\n",
    "                    'volume_nm^3': float(values[3]),\n",
    "                    'area_nm^2': float(values[4]),\n",
    "                    'scale': scale_type\n",
    "                }\n",
    "                parsed_data.append(row)\n",
    "            except ValueError as e:\n",
    "                print(f\"Error parsing line: {e}\")\n",
    "                continue\n",
    "    \n",
    "    if not parsed_data:\n",
    "        return False, f\"Failed to parse any data lines for {scale_type} scale\"\n",
    "    \n",
    "    return True, pd.DataFrame(parsed_data)\n",
    "\n",
    "\n",
    "def extract_single_file_distribution(content, sections, filename):\n",
    "    \"\"\"\n",
    "    Extract distribution data from a single file.\n",
    "    \n",
    "    Parameters:\n",
    "    content (str): File content\n",
    "    sections (dict): Dictionary with section positions\n",
    "    filename (str): Filename for reference\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (success_flag, result)\n",
    "        - If successful: (True, combined_df)\n",
    "        - If failed: (False, error_message)\n",
    "    \"\"\"\n",
    "    # Extract linear data\n",
    "    lin_start = sections['linear_start']\n",
    "    log_start = sections['logarithmic_start']\n",
    "    \n",
    "    success, lin_result = extract_data_section(content, lin_start, log_start, is_log_section=False)\n",
    "    if not success:\n",
    "        return False, f\"Failed to extract linear data from {filename}: {lin_result}\"\n",
    "    \n",
    "    lin_header, lin_data_lines = lin_result\n",
    "    success, lin_df = parse_data_lines(lin_header, lin_data_lines, 'linear')\n",
    "    if not success:\n",
    "        return False, f\"Failed to parse linear data from {filename}: {lin_df}\"\n",
    "    \n",
    "    # Extract logarithmic data\n",
    "    success, log_result = extract_data_section(content, log_start, is_log_section=True)\n",
    "    if not success:\n",
    "        print(f\"Warning: Could not extract logarithmic data from {filename}: {log_result}\")\n",
    "        combined_df = lin_df.copy()\n",
    "    else:\n",
    "        log_header, log_data_lines = log_result\n",
    "        success, log_df = parse_data_lines(log_header, log_data_lines, 'logarithmic')\n",
    "        if not success:\n",
    "            print(f\"Warning: Could not parse logarithmic data from {filename}: {log_df}\")\n",
    "            combined_df = lin_df.copy()\n",
    "        else:\n",
    "            combined_df = pd.concat([lin_df, log_df], ignore_index=True)\n",
    "    \n",
    "    # Add filename for tracking\n",
    "    combined_df['source_file'] = filename\n",
    "    \n",
    "    return True, combined_df\n",
    "\n",
    "\n",
    "def average_replicate_data(dataframes_list, filenames_list):\n",
    "    \"\"\"\n",
    "    Average distribution data across multiple replicates bin-by-bin.\n",
    "    \n",
    "    Parameters:\n",
    "    dataframes_list (list): List of DataFrames from individual files\n",
    "    filenames_list (list): List of filenames for reference\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (success_flag, result)\n",
    "        - If successful: (True, averaged_df)\n",
    "        - If failed: (False, error_message)\n",
    "    \"\"\"\n",
    "    if not dataframes_list:\n",
    "        return False, \"No dataframes to average\"\n",
    "    \n",
    "    if len(dataframes_list) == 1:\n",
    "        # Single file case - add SD columns with zeros\n",
    "        df = dataframes_list[0].copy()\n",
    "        \n",
    "        # Add SD columns (all zeros for single file)\n",
    "        df['number_sd'] = 0.0\n",
    "        df['concentration_cm-3_sd'] = 0.0\n",
    "        df['volume_nm^3_sd'] = 0.0\n",
    "        df['area_nm^2_sd'] = 0.0\n",
    "        \n",
    "        # Rename value columns to indicate they're averages\n",
    "        df.rename(columns={\n",
    "            'number': 'number_avg',\n",
    "            'concentration_cm-3': 'concentration_cm-3_avg',\n",
    "            'volume_nm^3': 'volume_nm^3_avg',\n",
    "            'area_nm^2': 'area_nm^2_avg'\n",
    "        }, inplace=True)\n",
    "        \n",
    "        # Add replicate info\n",
    "        df['num_replicates'] = 1\n",
    "        df['source_files'] = filenames_list[0]\n",
    "        \n",
    "        print(f\"Single file analysis: {filenames_list[0]}\")\n",
    "        print(\"Standard deviation values set to 0\")\n",
    "        \n",
    "        return True, df\n",
    "    \n",
    "    # Multiple files case - perform averaging\n",
    "    print(f\"Averaging data from {len(dataframes_list)} files:\")\n",
    "    for filename in filenames_list:\n",
    "        print(f\"  {filename}\")\n",
    "    \n",
    "    # Process each scale separately\n",
    "    averaged_dfs = []\n",
    "    \n",
    "    for scale in ['linear', 'logarithmic']:\n",
    "        scale_dfs = []\n",
    "        for df in dataframes_list:\n",
    "            scale_data = df[df['scale'] == scale]\n",
    "            if not scale_data.empty:\n",
    "                scale_dfs.append(scale_data)\n",
    "        \n",
    "        if not scale_dfs:\n",
    "            continue\n",
    "        \n",
    "        # Find common size bins across all files\n",
    "        all_sizes = []\n",
    "        for df in scale_dfs:\n",
    "            all_sizes.extend(df['size_nm'].values)\n",
    "        unique_sizes = sorted(set(all_sizes))\n",
    "        \n",
    "        # Create averaged data for this scale\n",
    "        averaged_data = []\n",
    "        \n",
    "        for size in unique_sizes:\n",
    "            # Collect values for this size across all files\n",
    "            numbers = []\n",
    "            concentrations = []\n",
    "            volumes = []\n",
    "            areas = []\n",
    "            \n",
    "            for df in scale_dfs:\n",
    "                size_row = df[df['size_nm'] == size]\n",
    "                if not size_row.empty:\n",
    "                    numbers.append(size_row['number'].iloc[0])\n",
    "                    concentrations.append(size_row['concentration_cm-3'].iloc[0])\n",
    "                    volumes.append(size_row['volume_nm^3'].iloc[0])\n",
    "                    areas.append(size_row['area_nm^2'].iloc[0])\n",
    "            \n",
    "            # Calculate averages and standard deviations\n",
    "            if numbers:  # Only if we have data for this size\n",
    "                row = {\n",
    "                    'size_nm': size,\n",
    "                    'number_avg': np.mean(numbers),\n",
    "                    'number_sd': np.std(numbers, ddof=1) if len(numbers) > 1 else 0.0,\n",
    "                    'concentration_cm-3_avg': np.mean(concentrations),\n",
    "                    'concentration_cm-3_sd': np.std(concentrations, ddof=1) if len(concentrations) > 1 else 0.0,\n",
    "                    'volume_nm^3_avg': np.mean(volumes),\n",
    "                    'volume_nm^3_sd': np.std(volumes, ddof=1) if len(volumes) > 1 else 0.0,\n",
    "                    'area_nm^2_avg': np.mean(areas),\n",
    "                    'area_nm^2_sd': np.std(areas, ddof=1) if len(areas) > 1 else 0.0,\n",
    "                    'scale': scale,\n",
    "                    'num_replicates': len(numbers),\n",
    "                    'source_files': '; '.join(filenames_list)\n",
    "                }\n",
    "                averaged_data.append(row)\n",
    "        \n",
    "        if averaged_data:\n",
    "            scale_df = pd.DataFrame(averaged_data)\n",
    "            averaged_dfs.append(scale_df)\n",
    "    \n",
    "    if not averaged_dfs:\n",
    "        return False, \"No data could be averaged\"\n",
    "    \n",
    "    # Combine linear and logarithmic scales\n",
    "    final_df = pd.concat(averaged_dfs, ignore_index=True)\n",
    "    \n",
    "    # Sort by scale and size\n",
    "    final_df = final_df.sort_values(['scale', 'size_nm']).reset_index(drop=True)\n",
    "    \n",
    "    print(f\"Averaging completed:\")\n",
    "    print(f\"  Total size bins: {len(final_df)}\")\n",
    "    print(f\"  Linear scale bins: {len(final_df[final_df['scale'] == 'linear'])}\")\n",
    "    print(f\"  Log scale bins: {len(final_df[final_df['scale'] == 'logarithmic'])}\")\n",
    "    \n",
    "    return True, final_df\n",
    "\n",
    "\n",
    "# Execute data extraction and averaging if files were processed in Cell 02\n",
    "if 'current_files_data' in globals() and current_files_data:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"EXTRACTING AND AVERAGING DISTRIBUTION DATA\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Extract data from each file\n",
    "    individual_dataframes = []\n",
    "    filenames = []\n",
    "    extraction_errors = []\n",
    "    \n",
    "    for filename, content, sections in current_files_data:\n",
    "        print(f\"Extracting data from: {filename}\")\n",
    "        \n",
    "        success, result = extract_single_file_distribution(content, sections, filename)\n",
    "        \n",
    "        if success:\n",
    "            individual_dataframes.append(result)\n",
    "            filenames.append(filename)\n",
    "            print(f\"  ‚úì Extracted {len(result)} data points\")\n",
    "        else:\n",
    "            extraction_errors.append((filename, result))\n",
    "            print(f\"  ‚úó Failed: {result}\")\n",
    "    \n",
    "    if individual_dataframes:\n",
    "        # Average the data across replicates\n",
    "        print(f\"\\nAveraging data from {len(individual_dataframes)} successful file(s)...\")\n",
    "        \n",
    "        success, averaged_df = average_replicate_data(individual_dataframes, filenames)\n",
    "        \n",
    "        if success:\n",
    "            # Store results for downstream cells (mimicking old format)\n",
    "            current_distribution_df = averaged_df\n",
    "            current_file_content = None  # Not applicable for averaged data\n",
    "            current_file_sections = None  # Not applicable for averaged data\n",
    "            \n",
    "            # Add uniqueID based on first filename\n",
    "            first_filename = filenames[0]\n",
    "            # Extract base name for uniqueID\n",
    "            base_name = first_filename.replace('_rawdata.txt', '').replace('.txt', '')\n",
    "            if base_name.startswith('Data_'):\n",
    "                base_name = base_name[5:]\n",
    "            \n",
    "            # For multiple files, add indication\n",
    "            if len(filenames) > 1:\n",
    "                uniqueID = f\"{base_name}_avg{len(filenames)}\"\n",
    "            else:\n",
    "                uniqueID = base_name\n",
    "            \n",
    "            current_distribution_df['uniqueID'] = uniqueID\n",
    "            \n",
    "            print(f\"\\nData extraction and averaging completed successfully!\")\n",
    "            print(f\"Dataset ID: {uniqueID}\")\n",
    "            print(f\"Ready for metadata extraction (Cell 04)\")\n",
    "            \n",
    "            # Display preview\n",
    "            print(f\"\\nAveraged Data Preview:\")\n",
    "            display(current_distribution_df.head())\n",
    "            \n",
    "        else:\n",
    "            print(f\"\\nERROR: Failed to average data: {averaged_df}\")\n",
    "    \n",
    "    else:\n",
    "        print(f\"\\nERROR: No files could be processed for data extraction.\")\n",
    "        if extraction_errors:\n",
    "            print(\"Extraction errors:\")\n",
    "            for filename, error in extraction_errors:\n",
    "                print(f\"  {filename}: {error}\")\n",
    "\n",
    "else:\n",
    "    print(\"No processed files found. Please run Cell 02 (file processing) first.\")\n",
    "    print(\"Make sure to run Cell 00 (file selection) before Cell 02.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55b15e38-6bcd-438c-a9d3-dcd7735c608f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "AUTOMATED METADATA EXTRACTION AND ANALYSIS\n",
      "================================================================================\n",
      "Extracting ALL metadata fields from 3 file(s):\n",
      "  Processing: 20250527_0004_23052025_SUV_5136664_after6weeks_size_488.txt\n",
      "    ‚úì Extracted 58 metadata fields\n",
      "  Processing: 20250527_0005_23052025_SUV_5136664_after6weeks_size_488.txt\n",
      "    ‚úì Extracted 58 metadata fields\n",
      "  Processing: 20250527_0006_23052025_SUV_5136664_after6weeks_size_488.txt\n",
      "    ‚úì Extracted 58 metadata fields\n",
      "\n",
      "Extracted metadata from 3 files\n",
      "\n",
      "Analyzing 58 unique metadata fields:\n",
      "  Identical fields: 40\n",
      "  Different fields: 18\n",
      "\n",
      "Automated metadata processing completed successfully!\n",
      "Saved comprehensive metadata to: /Users/hboehm/Seafile/LEAF/NTA_data/!Inbox/20250527/metadata/Data_20250527_0004_23052025_SUV_5136664_after6weeks_size_488_avg3_metadata.txt\n",
      "\n",
      "Metadata Summary:\n",
      "  persistentID: 20250527_0004_23052025_SUV_5136664_after6weeks_size_488_avg3\n",
      "  num_replicates: 3\n",
      "  total_fields_extracted: 33\n",
      "\n",
      "Detailed analysis available in memory:\n",
      "  current_all_fields_metadata (full extraction from all files)\n",
      "  current_original_differences (all field differences)\n",
      "  current_processing_notes (how each field was processed)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "NTA Data Analysis - Automated Modular Metadata System (Cell 04)\n",
    "\n",
    "This module handles:\n",
    "1. Automated extraction of ALL metadata fields from multiple files\n",
    "2. Automatic detection of differences vs. identical values\n",
    "3. Smart array creation for differing values\n",
    "4. Detailed conflict reporting for analysis\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import ntpath  \n",
    "from datetime import date\n",
    "\n",
    "\n",
    "def extract_all_metadata_fields(content, filename):\n",
    "    \"\"\"\n",
    "    Extract ALL possible metadata fields from file content using comprehensive regex patterns.\n",
    "    \n",
    "    Parameters:\n",
    "    content (str): File content to extract metadata from\n",
    "    filename (str): Original filename for reference\n",
    "    \n",
    "    Returns:\n",
    "    dict: Dictionary with all found metadata fields\n",
    "    \"\"\"\n",
    "    # Comprehensive regex patterns for ALL possible NTA metadata fields\n",
    "    metadata_patterns = [\n",
    "        ('original_file', r'Original File:\\s+(.+?)(?:\\s+Section:|$)'),\n",
    "        ('section', r'Section:\\s+(.+)'),\n",
    "        ('operator', r'Operator:\\s+(.+)'),\n",
    "        ('experiment', r'Experiment:\\s+(.+)'),\n",
    "        ('zetaview_sn', r'ZetaView S/N:\\s+(.+)'),\n",
    "        ('cell_sn', r'Cell S/N:\\s+(.+)'),\n",
    "        ('software', r'Software:\\s+(.+?)(?:\\s+Analyze:|$)'),\n",
    "        ('analyze', r'Analyze:\\s+(.+)'),\n",
    "        ('sop', r'SOP:\\s+(.+)'),\n",
    "        ('sample', r'Sample:\\s+(.+)'),\n",
    "        ('electrolyte', r'Electrolyte:(?:\\s*(.*?))?(?:\\r?\\n|$)'),\n",
    "        ('ph', r'pH:\\s+(.+?)(?:\\s+entered|$)'),\n",
    "        ('conductivity', r'Conductivity:\\s+(.+?)(?:\\s+sensed|$)'),\n",
    "        ('temp_control', r'TempControl:\\s+(.+)'),\n",
    "        ('set_temperature', r'SetTemperature:\\s+(.+)'),\n",
    "        ('temperature', r'Temperature:\\s+(.+?)(?:\\s+sensed|$)'),\n",
    "        ('viscosity', r'Viscosity:\\s+(.+)'),\n",
    "        ('date', r'Date:\\s+(.+)'),\n",
    "        ('time', r'Time:\\s+(.+)'),\n",
    "        ('general_remarks', r'General Remarks:\\s+(.+)'),\n",
    "        ('remarks', r'Remarks:\\s+(.+)'),\n",
    "        ('sample_info_1', r'Sample Info 1:\\s+(.+)'),\n",
    "        ('sample_info_2', r'Sample Info 2:\\s+(.+)'),\n",
    "        ('sample_info_3', r'Sample Info 3:\\s*(.*)'),\n",
    "        ('scattering_intensity', r'Scattering Intensity:\\s+(.+)'),\n",
    "        ('detected_particles', r'Detected Particles:\\s+(.+)'),\n",
    "        ('particle_drift_checked', r'Particle Drift Checked:\\s+(.+)'),\n",
    "        ('particle_drift_check_result', r'Particle Drift Check Result:\\s+(.+)'),\n",
    "        ('cell_check_date', r'Cell Checked:\\s+(\\d{4}-\\d{2}-\\d{2})'),\n",
    "        ('cell_check_result', r'Cell Check Result:\\s+(.+)'),\n",
    "        ('type_of_measurement', r'Type of Measurement:\\s+(.+)'),\n",
    "        ('positions', r'Positions:\\s+(.+)'),\n",
    "        ('microscope_position', r'Microscope Position:\\s+(.+)'),\n",
    "        ('number_of_traces', r'Number of Traces:\\s+(\\d+)'),\n",
    "        ('average_number_of_particles', r'Average Number of Particles:\\s+(\\d+\\.\\d+)'),\n",
    "        ('dilution', r'Dilution::\\s+(\\d+\\.\\d+)'),\n",
    "        ('concentration_correction_factor', r'Concentration Correction Factor:\\s+(.+)'),\n",
    "        ('laser_wavelength', r'Laser Wavelength nm:\\s+(\\d+\\.\\d+)'),\n",
    "        ('median_number_d50', r'Median Number \\(D50\\):\\s+(.+)'),\n",
    "        ('median_concentration_d50', r'Median Concentration \\(D50\\):\\s+(.+)'),\n",
    "        ('median_volume_d50', r'Median Volume \\(D50\\):\\s+(.+)'),\n",
    "        ('minimum_brightness', r'Minimum Brightness:\\s+(\\d+)'),\n",
    "        ('minimum_area', r'Minimum Area:\\s+(\\d+)'),\n",
    "        ('maximum_area', r'Maximum Area:\\s+(\\d+)'),\n",
    "        ('maximum_brightness', r'Maximum Brightness:\\s+(\\d+)'),\n",
    "        ('tracking_radius2', r'Tracking Radius2:\\s+(\\d+)'),\n",
    "        ('minimum_tracelength', r'Minimum Tracelength:\\s+(\\d+)'),\n",
    "        ('fps', r'Camera:\\s*FpSec\\s+(\\d+)\\s+#Cycles'),\n",
    "        ('cycles', r'#Cycles\\s+(\\d+)'),\n",
    "        # Additional patterns for more comprehensive extraction\n",
    "        ('camera_settings', r'Camera:\\s+(.+)'),\n",
    "        ('frame_rate', r'FRate\\s+(\\d+\\.\\d+)'),\n",
    "        ('auto_settings', r'Auto:\\s+(.+)'),\n",
    "        ('sensitivity', r'Sensitivity:\\s+(.+)'),\n",
    "        ('shutter', r'Shutter:\\s+(.+)'),\n",
    "        ('gain', r'Gain:\\s+(.+)'),\n",
    "    ]\n",
    "    \n",
    "    # Extract metadata\n",
    "    metadata = {}\n",
    "    \n",
    "    for key, pattern in metadata_patterns:\n",
    "        match = re.search(pattern, content)\n",
    "        if match:\n",
    "            value = match.group(1).strip() if match.group(1) else ''\n",
    "            # Clean up common issues\n",
    "            if value and not value.lower() in ['none', 'null', '']:\n",
    "                metadata[key] = value\n",
    "    \n",
    "    # Add filename and derived fields\n",
    "    metadata['filename'] = filename\n",
    "    \n",
    "    # Extract unique ID from filename\n",
    "    base_name = os.path.splitext(filename)[0]\n",
    "    if base_name.endswith(\"_rawdata\"):\n",
    "        base_name = base_name[:-8]\n",
    "    if base_name.startswith(\"Data_\"):\n",
    "        base_name = base_name[5:]\n",
    "    metadata['uniqueID'] = base_name\n",
    "    \n",
    "    # Try to find AVI file information\n",
    "    original_file = metadata.get('original_file', '')\n",
    "    if original_file:\n",
    "        avi_filename = ntpath.basename(original_file)\n",
    "        metadata['avi_filename'] = avi_filename\n",
    "        \n",
    "        # Try to find AVI file size if file exists\n",
    "        if 'CONFIG' in globals():\n",
    "            directory = CONFIG.get('directory', '')\n",
    "            full_avi_path = os.path.join(directory, avi_filename)\n",
    "            if os.path.exists(full_avi_path) and os.path.isfile(full_avi_path):\n",
    "                size_bytes = os.path.getsize(full_avi_path)\n",
    "                metadata['avi_filesize'] = f\"{size_bytes / (1024 * 1024):.2f} MB\"\n",
    "    \n",
    "    return metadata\n",
    "\n",
    "\n",
    "def extract_metadata_from_all_files(files_data):\n",
    "    \"\"\"\n",
    "    Extract metadata from all files and organize by filename.\n",
    "    \n",
    "    Parameters:\n",
    "    files_data (list): List of (filename, content, sections) tuples\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (success_flag, result)\n",
    "        - If successful: (True, all_files_metadata_dict)\n",
    "        - If failed: (False, error_message)\n",
    "    \"\"\"\n",
    "    if not files_data:\n",
    "        return False, \"No files provided for metadata extraction\"\n",
    "    \n",
    "    all_files_metadata = {}\n",
    "    \n",
    "    print(f\"Extracting ALL metadata fields from {len(files_data)} file(s):\")\n",
    "    \n",
    "    for filename, content, sections in files_data:\n",
    "        print(f\"  Processing: {filename}\")\n",
    "        \n",
    "        metadata = extract_all_metadata_fields(content, filename)\n",
    "        all_files_metadata[filename] = metadata\n",
    "        \n",
    "        print(f\"    ‚úì Extracted {len(metadata)} metadata fields\")\n",
    "    \n",
    "    print(f\"\\nExtracted metadata from {len(all_files_metadata)} files\")\n",
    "    return True, all_files_metadata\n",
    "\n",
    "\n",
    "def analyze_field_differences(all_files_metadata):\n",
    "    \"\"\"\n",
    "    Analyze which fields are identical vs. different across files.\n",
    "    \n",
    "    Parameters:\n",
    "    all_files_metadata (dict): Dictionary of {filename: metadata_dict}\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (identical_fields, different_fields, field_analysis)\n",
    "    \"\"\"\n",
    "    if not all_files_metadata:\n",
    "        return {}, {}, {}\n",
    "    \n",
    "    # Get all unique field names across all files\n",
    "    all_field_names = set()\n",
    "    for metadata in all_files_metadata.values():\n",
    "        all_field_names.update(metadata.keys())\n",
    "    \n",
    "    identical_fields = {}\n",
    "    different_fields = {}\n",
    "    field_analysis = {}\n",
    "    \n",
    "    print(f\"\\nAnalyzing {len(all_field_names)} unique metadata fields:\")\n",
    "    \n",
    "    for field_name in sorted(all_field_names):\n",
    "        # Collect values for this field from all files\n",
    "        values = []\n",
    "        files_with_field = []\n",
    "        \n",
    "        for filename, metadata in all_files_metadata.items():\n",
    "            if field_name in metadata:\n",
    "                values.append(metadata[field_name])\n",
    "                files_with_field.append(filename)\n",
    "        \n",
    "        # Analyze this field\n",
    "        field_info = {\n",
    "            'values': values,\n",
    "            'files_with_field': files_with_field,\n",
    "            'present_in_files': len(files_with_field),\n",
    "            'total_files': len(all_files_metadata)\n",
    "        }\n",
    "        \n",
    "        if len(set(values)) == 1:\n",
    "            # All values are identical\n",
    "            identical_fields[field_name] = values[0]\n",
    "            field_info['status'] = 'identical'\n",
    "        else:\n",
    "            # Values differ\n",
    "            different_fields[field_name] = values\n",
    "            field_info['status'] = 'different'\n",
    "        \n",
    "        field_analysis[field_name] = field_info\n",
    "    \n",
    "    # Report concise summary\n",
    "    print(f\"  Identical fields: {len(identical_fields)}\")\n",
    "    print(f\"  Different fields: {len(different_fields)}\")\n",
    "    \n",
    "    # Only show alerts, not all differences\n",
    "    quality_issues = []\n",
    "    for field_name, values in different_fields.items():\n",
    "        if field_name == 'particle_drift_check_result':\n",
    "            unique_values = list(set(values))\n",
    "            if len(unique_values) > 1:\n",
    "                quality_issues.append(f\"QC Alert - {field_name}: {values}\")\n",
    "    \n",
    "    if quality_issues:\n",
    "        print(f\"\\nQuality Control Issues Detected:\")\n",
    "        for issue in quality_issues:\n",
    "            print(f\"  {issue}\")\n",
    "    \n",
    "    return identical_fields, different_fields, field_analysis\n",
    "\n",
    "\n",
    "def smart_format_field(field_name, values):\n",
    "    \"\"\"\n",
    "    Apply smart formatting rules based on field type and content.\n",
    "    \n",
    "    Parameters:\n",
    "    field_name (str): Name of the metadata field\n",
    "    values (list): List of values from different files\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (formatted_value, notes)\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    \n",
    "    # File-specific info - keep as arrays\n",
    "    file_specific_fields = [\n",
    "        'filename', 'avi_filename', 'experiment', 'original_file', 'uniqueID', \n",
    "        'time', 'particle_drift_checked'\n",
    "    ]\n",
    "    \n",
    "    # Text fields - use first value only\n",
    "    text_first_fields = [\n",
    "        'sample_info_1', 'sample_info_2', 'sample_info_3', 'remarks', 'general_remarks'\n",
    "    ]\n",
    "    \n",
    "    # Sum fields - add them up\n",
    "    sum_fields = [\n",
    "        'number_of_traces', 'detected_particles'\n",
    "    ]\n",
    "    \n",
    "    # Instrument-determined fields - keep as arrays with note\n",
    "    instrument_determined_fields = [\n",
    "        'median_number_d50', 'median_concentration_d50', 'median_volume_d50'\n",
    "    ]\n",
    "    \n",
    "    # Quality control fields - flag differences\n",
    "    quality_control_fields = [\n",
    "        'particle_drift_check_result', 'cell_check_result'\n",
    "    ]\n",
    "    \n",
    "    notes = \"\"\n",
    "    \n",
    "    if field_name in file_specific_fields:\n",
    "        # Keep as JSON array\n",
    "        return json.dumps(values), \"file_specific\"\n",
    "    \n",
    "    elif field_name in text_first_fields:\n",
    "        # Use first value only\n",
    "        return values[0], f\"using_first_of_{len(values)}\"\n",
    "    \n",
    "    elif field_name in sum_fields:\n",
    "        # Sum all values\n",
    "        try:\n",
    "            numeric_values = [float(v) for v in values]\n",
    "            total = sum(numeric_values)\n",
    "            return f\"{total:.0f}\", f\"sum_of_{len(values)}_measurements\"\n",
    "        except ValueError:\n",
    "            return json.dumps(values), \"non_numeric_sum_field\"\n",
    "    \n",
    "    elif field_name in instrument_determined_fields:\n",
    "        # Keep as array with explanatory note\n",
    "        return json.dumps(values), \"instrument_determined_per_measurement\"\n",
    "    \n",
    "    elif field_name in quality_control_fields:\n",
    "        # Check if all values are the same\n",
    "        unique_values = list(set(values))\n",
    "        if len(unique_values) == 1:\n",
    "            return values[0], \"qc_consistent\"\n",
    "        else:\n",
    "            return json.dumps(values), f\"QC_ALERT_inconsistent_values\"\n",
    "    \n",
    "    else:\n",
    "        # Try to calculate mean ¬± SD for numeric fields\n",
    "        try:\n",
    "            # Special handling for file sizes with units\n",
    "            if field_name == 'avi_filesize':\n",
    "                # Extract numeric values from strings like \"219.24 MB\"\n",
    "                numeric_values = []\n",
    "                for v in values:\n",
    "                    if isinstance(v, str) and 'MB' in v:\n",
    "                        numeric_values.append(float(v.replace(' MB', '')))\n",
    "                    else:\n",
    "                        numeric_values.append(float(v))\n",
    "                \n",
    "                mean_val = np.mean(numeric_values)\n",
    "                std_val = np.std(numeric_values, ddof=1) if len(numeric_values) > 1 else 0.0\n",
    "                \n",
    "                cv = (std_val / mean_val * 100) if mean_val != 0 else 0\n",
    "                if cv > 10:\n",
    "                    notes = f\"HIGH_VARIATION_CV_{cv:.1f}%\"\n",
    "                else:\n",
    "                    notes = f\"mean_sd_of_{len(values)}\"\n",
    "                \n",
    "                return f\"{mean_val:.2f} ¬± {std_val:.2f} MB\", notes\n",
    "            \n",
    "            # Regular numeric processing for other fields\n",
    "            numeric_values = [float(v) for v in values]\n",
    "            mean_val = np.mean(numeric_values)\n",
    "            std_val = np.std(numeric_values, ddof=1) if len(numeric_values) > 1 else 0.0\n",
    "            \n",
    "            # Check for concerning variations\n",
    "            cv = (std_val / mean_val * 100) if mean_val != 0 else 0\n",
    "            \n",
    "            if cv > 10:  # More than 10% coefficient of variation\n",
    "                notes = f\"HIGH_VARIATION_CV_{cv:.1f}%\"\n",
    "            else:\n",
    "                notes = f\"mean_sd_of_{len(values)}\"\n",
    "            \n",
    "            return f\"{mean_val:.2f} ¬± {std_val:.2f}\", notes\n",
    "            \n",
    "        except ValueError:\n",
    "            # Non-numeric field - keep as array\n",
    "            return json.dumps(values), \"non_numeric_different\"\n",
    "\n",
    "\n",
    "def create_automated_metadata(all_files_metadata, identical_fields, different_fields, config=None):\n",
    "    \"\"\"\n",
    "    Create essential standardized metadata for multi-file analysis.\n",
    "    \n",
    "    Parameters:\n",
    "    all_files_metadata (dict): All files' metadata\n",
    "    identical_fields (dict): Fields with identical values\n",
    "    different_fields (dict): Fields with different values\n",
    "    config (dict): Configuration dictionary\n",
    "    \n",
    "    Returns:\n",
    "    dict: Essential standardized metadata dictionary with ordered fields\n",
    "    \"\"\"\n",
    "    filenames = list(all_files_metadata.keys())\n",
    "    num_files = len(filenames)\n",
    "    \n",
    "    # Start with ordered metadata structure\n",
    "    metadata = {}\n",
    "    processing_notes = {}\n",
    "    \n",
    "    # Create uniqueID with averaging suffix\n",
    "    if 'uniqueID' in identical_fields:\n",
    "        base_id = identical_fields['uniqueID']\n",
    "    else:\n",
    "        # Take from first file if not identical\n",
    "        base_id = list(all_files_metadata.values())[0].get('uniqueID', 'unknown')\n",
    "    \n",
    "    if num_files > 1:\n",
    "        unique_id = f\"{base_id}_avg{num_files}\"\n",
    "    else:\n",
    "        unique_id = base_id\n",
    "    \n",
    "    # Check for custom persistent_ID from config\n",
    "    if config and \"project_metadata\" in config:\n",
    "        project_meta = config[\"project_metadata\"]\n",
    "        if \"persistent_ID\" in project_meta:\n",
    "            custom_id = project_meta[\"persistent_ID\"]\n",
    "            if num_files > 1:\n",
    "                unique_id = f\"{custom_id}_avg{num_files}\"\n",
    "            else:\n",
    "                unique_id = custom_id\n",
    "            print(f\"Using custom persistent ID: {unique_id}\")\n",
    "    else:\n",
    "        project_meta = {}\n",
    "    \n",
    "    # SECTION 1: CORE IDENTIFICATION (first in file)\n",
    "    metadata['experimenter'] = project_meta.get('experimenter', 'SH/HB')\n",
    "    metadata['location'] = project_meta.get('location', 'HD_MPImF_CBP_R0.106')\n",
    "    metadata['project'] = project_meta.get('project', 'LEAF')\n",
    "    metadata['meta_version'] = project_meta.get('meta_version', 'v02')\n",
    "    metadata['pi'] = project_meta.get('pi', 'HB')\n",
    "    metadata['funding'] = project_meta.get('funding', 'MPG')\n",
    "    metadata['persistentID'] = unique_id\n",
    "    metadata['data_collection_method'] = project_meta.get('data_collection_method', 'NTA')\n",
    "    metadata['nta_instrument'] = 'ZetaView'\n",
    "    \n",
    "    # NTA software version\n",
    "    if 'analyze' in identical_fields:\n",
    "        metadata['nta_software'] = f\"ZetaView {identical_fields['analyze']}\"\n",
    "    elif 'analyze' in different_fields:\n",
    "        metadata['nta_software'] = f\"ZetaView {different_fields['analyze'][0]}\"\n",
    "    else:\n",
    "        metadata['nta_software'] = 'ZetaView'\n",
    "    \n",
    "    metadata['nta_processed_file'] = f\"Data_{unique_id}_PSD.txt\"\n",
    "    \n",
    "    # Sample info\n",
    "    if 'sample' in identical_fields:\n",
    "        metadata['sample'] = identical_fields['sample']\n",
    "    elif 'sample' in different_fields:\n",
    "        metadata['sample'] = different_fields['sample'][0]  # Use first sample name\n",
    "    \n",
    "    # SECTION 2: MULTI-FILE INFO\n",
    "    metadata['num_replicates'] = num_files\n",
    "    metadata['source_files'] = json.dumps(filenames)\n",
    "    \n",
    "    # SECTION 3: MEASUREMENT PARAMETERS\n",
    "    # Define essential fields to save (others will be kept in memory only)\n",
    "    essential_fields = {\n",
    "        # Key measurement parameters\n",
    "        'date', 'temperature', 'ph', 'dilution', 'laser_wavelength', 'electrolyte',\n",
    "        'positions', 'cycles', 'fps', \n",
    "        # Quality control\n",
    "        'particle_drift_check_result', 'cell_check_result',\n",
    "        # Key results (not instrument D50s - we calculate better ones later)\n",
    "        'average_number_of_particles', 'number_of_traces', 'detected_particles',\n",
    "        'conductivity', 'scattering_intensity', 'viscosity',\n",
    "        # File info (simplified)\n",
    "        'avi_filesize'\n",
    "    }\n",
    "    \n",
    "    # Process identical fields (only essential ones)\n",
    "    for field_name, value in identical_fields.items():\n",
    "        if field_name in essential_fields:\n",
    "            # Add nta_ prefix for measurement-related fields\n",
    "            if field_name in ['temperature', 'ph', 'dilution', 'laser_wavelength', 'positions', \n",
    "                             'cycles', 'fps', 'average_number_of_particles', 'number_of_traces', \n",
    "                             'detected_particles', 'particle_drift_check_result', 'cell_check_result',\n",
    "                             'conductivity', 'scattering_intensity', 'viscosity', 'avi_filesize']:\n",
    "                # Special naming for summed fields\n",
    "                if field_name in ['number_of_traces', 'detected_particles']:\n",
    "                    metadata[f'nta_{field_name}_sum'] = value\n",
    "                else:\n",
    "                    metadata[f'nta_{field_name}'] = value\n",
    "            else:\n",
    "                metadata[field_name] = value\n",
    "    \n",
    "    # Process different fields with smart formatting (only essential ones)\n",
    "    quality_alerts = []\n",
    "    high_variation_fields = []\n",
    "    \n",
    "    for field_name, values in different_fields.items():\n",
    "        if field_name in essential_fields:\n",
    "            # Apply smart formatting\n",
    "            formatted_value, notes = smart_format_field(field_name, values)\n",
    "            \n",
    "            # Track quality issues\n",
    "            if \"QC_ALERT\" in notes:\n",
    "                quality_alerts.append(f\"{field_name}: {formatted_value}\")\n",
    "            if \"HIGH_VARIATION\" in notes:\n",
    "                high_variation_fields.append(f\"{field_name}: {notes}\")\n",
    "            \n",
    "            # Add nta_ prefix for measurement-related fields\n",
    "            if field_name in ['temperature', 'ph', 'dilution', 'laser_wavelength', 'positions', \n",
    "                             'cycles', 'fps', 'average_number_of_particles', 'number_of_traces', \n",
    "                             'detected_particles', 'particle_drift_check_result', 'cell_check_result',\n",
    "                             'conductivity', 'scattering_intensity', 'viscosity', 'avi_filesize']:\n",
    "                # Special naming for summed fields\n",
    "                if field_name in ['number_of_traces', 'detected_particles']:\n",
    "                    metadata[f'nta_{field_name}_sum'] = formatted_value\n",
    "                else:\n",
    "                    metadata[f'nta_{field_name}'] = formatted_value\n",
    "            else:\n",
    "                metadata[field_name] = formatted_value\n",
    "            \n",
    "            # Store processing note (in memory only)\n",
    "            processing_notes[field_name] = notes\n",
    "    \n",
    "    # SECTION 4: ADDITIONAL REFERENCES\n",
    "    # Note: nta_plot_file will be added later when plots are actually generated\n",
    "    metadata['python_analysis'] = str(date.today())\n",
    "    \n",
    "    # SECTION 5: QUALITY ALERTS (only if present)\n",
    "    if quality_alerts:\n",
    "        metadata['quality_control_alerts'] = json.dumps(quality_alerts)\n",
    "        print(f\"\\n‚ö† QUALITY CONTROL ALERTS:\")\n",
    "        for alert in quality_alerts:\n",
    "            print(f\"  {alert}\")\n",
    "    \n",
    "    if high_variation_fields:\n",
    "        metadata['high_variation_fields'] = json.dumps(high_variation_fields)\n",
    "        print(f\"\\n‚ö† HIGH VARIATION DETECTED:\")\n",
    "        for field in high_variation_fields:\n",
    "            print(f\"  {field}\")\n",
    "    \n",
    "    # Store detailed analysis in memory only (not in saved file)\n",
    "    global current_processing_notes, current_all_fields_metadata, current_original_differences\n",
    "    current_processing_notes = processing_notes\n",
    "    current_all_fields_metadata = all_files_metadata  # Full detailed metadata\n",
    "    current_original_differences = different_fields    # All original differences\n",
    "    \n",
    "    return metadata\n",
    "\n",
    "\n",
    "def save_metadata_file(metadata, output_dir=None, config=None):\n",
    "    \"\"\"\n",
    "    Save metadata to a file in the specified output directory.\n",
    "    \"\"\"\n",
    "    # Determine output directory\n",
    "    if output_dir is None:\n",
    "        if config is not None and \"directory\" in config:\n",
    "            base_dir = config[\"directory\"]\n",
    "            output_dir = os.path.join(base_dir, \"metadata\")\n",
    "        else:\n",
    "            output_dir = os.path.join(os.getcwd(), \"metadata\")\n",
    "    \n",
    "    # Ensure directory exists\n",
    "    try:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "    except Exception as e:\n",
    "        return False, f\"Failed to create metadata directory: {str(e)}\"\n",
    "    \n",
    "    # Create filepath\n",
    "    unique_id = metadata.get('persistentID', 'unknown')\n",
    "    metadata_path = os.path.join(output_dir, f\"Data_{unique_id}_metadata.txt\")\n",
    "    \n",
    "    try:\n",
    "        # Write metadata\n",
    "        with open(metadata_path, 'w') as f:\n",
    "            for key, value in metadata.items():\n",
    "                f.write(f\"{key}\\t{value}\\t\\n\")\n",
    "        \n",
    "        return True, metadata_path\n",
    "    except Exception as e:\n",
    "        return False, f\"Failed to write metadata file: {str(e)}\"\n",
    "\n",
    "\n",
    "# Execute automated metadata extraction if files were processed\n",
    "if 'current_files_data' in globals() and current_files_data:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"AUTOMATED METADATA EXTRACTION AND ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Step 1: Extract all metadata from all files\n",
    "    success, all_files_metadata = extract_metadata_from_all_files(current_files_data)\n",
    "    \n",
    "    if success:\n",
    "        # Step 2: Analyze which fields are identical vs. different\n",
    "        identical_fields, different_fields, field_analysis = analyze_field_differences(all_files_metadata)\n",
    "        \n",
    "        # Step 3: Create standardized metadata with automated rules\n",
    "        standard_metadata = create_automated_metadata(\n",
    "            all_files_metadata, identical_fields, different_fields, CONFIG\n",
    "        )\n",
    "        \n",
    "        # Step 4: Save metadata file\n",
    "        success, metadata_path = save_metadata_file(standard_metadata, config=CONFIG)\n",
    "        \n",
    "        if success:\n",
    "            print(f\"\\nAutomated metadata processing completed successfully!\")\n",
    "            print(f\"Saved comprehensive metadata to: {metadata_path}\")\n",
    "            \n",
    "            # Store for downstream cells\n",
    "            current_metadata = standard_metadata\n",
    "            \n",
    "            print(f\"\\nMetadata Summary:\")\n",
    "            print(f\"  persistentID: {standard_metadata['persistentID']}\")\n",
    "            print(f\"  num_replicates: {standard_metadata['num_replicates']}\")\n",
    "            print(f\"  total_fields_extracted: {len(standard_metadata)}\")\n",
    "            \n",
    "            # Store detailed analysis for inspection (but don't clutter output)\n",
    "            current_field_analysis = field_analysis\n",
    "            current_identical_fields = identical_fields\n",
    "            current_different_fields = different_fields\n",
    "            # current_processing_notes, current_all_fields_metadata, and current_original_differences\n",
    "            # are stored globally by create_automated_metadata()\n",
    "            \n",
    "            print(f\"\\nDetailed analysis available in memory:\")\n",
    "            print(f\"  current_all_fields_metadata (full extraction from all files)\")\n",
    "            print(f\"  current_original_differences (all field differences)\")\n",
    "            print(f\"  current_processing_notes (how each field was processed)\")\n",
    "            \n",
    "        else:\n",
    "            print(f\"ERROR: Failed to save metadata: {metadata_path}\")\n",
    "    \n",
    "    else:\n",
    "        print(f\"ERROR: Failed to extract metadata: {all_files_metadata}\")\n",
    "\n",
    "else:\n",
    "    print(\"No processed files found. Please run Cell 02 and Cell 03 first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "726df0e5-2b10-4eab-abf5-6f37cc991d11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "NTA Data Analysis - Core Calculations with Uncertainty Propagation (Cell 05)\n",
    "\n",
    "This module provides core calculation functions for NTA data analysis with rigorous\n",
    "uncertainty propagation through averaging of replicates:\n",
    "\n",
    "1. Dilution correction for all measurements\n",
    "2. Normalization of particle distributions with uncertainty propagation\n",
    "3. Calculation of cumulative distributions with proper error propagation\n",
    "4. Total metrics calculation with uncertainty combination\n",
    "\n",
    "These functions provide the mathematical foundation for analyzing averaged\n",
    "number, volume, and surface area distributions from replicate NTA measurements.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import integrate\n",
    "\n",
    "\n",
    "def apply_dilution_correction_with_uncertainty(df, metadata=None, manual_dilution=None):\n",
    "    \"\"\"\n",
    "    Apply dilution correction to all measured values with uncertainty propagation.\n",
    "    \n",
    "    For a dilution factor D, the actual sample concentration = measured √ó D\n",
    "    Uncertainty propagation: œÉ_actual = œÉ_measured √ó D\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): DataFrame containing averaged distribution data\n",
    "    metadata (dict): Metadata dictionary that may contain dilution info\n",
    "    manual_dilution (float): Manual dilution factor override (optional)\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (success_flag, result)\n",
    "        - If successful: (True, updated_df)\n",
    "        - If failed: (False, error_message)\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying the original dataframe\n",
    "    updated_df = df.copy()\n",
    "    \n",
    "    # Determine dilution factor\n",
    "    dilution_factor = 1.0  # Default: no dilution\n",
    "    dilution_source = \"default (no dilution)\"\n",
    "    \n",
    "    if manual_dilution is not None:\n",
    "        try:\n",
    "            dilution_factor = float(manual_dilution)\n",
    "            dilution_source = \"manually specified\"\n",
    "        except (ValueError, TypeError):\n",
    "            return False, f\"Invalid manual dilution factor: {manual_dilution}\"\n",
    "    \n",
    "    elif metadata is not None:\n",
    "        # Try nta_dilution field\n",
    "        if 'nta_dilution' in metadata:\n",
    "            try:\n",
    "                dilution_string = metadata['nta_dilution']\n",
    "                # Handle format like \"10.0\" or other formats\n",
    "                dilution_factor = float(dilution_string.split('¬±')[0].strip()) if '¬±' in dilution_string else float(dilution_string)\n",
    "                dilution_source = \"metadata (nta_dilution)\"\n",
    "            except (ValueError, TypeError):\n",
    "                print(\"Warning: Could not parse dilution factor from metadata, using default (1.0)\")\n",
    "    \n",
    "    print(f\"Applying dilution correction: factor = {dilution_factor} (source: {dilution_source})\")\n",
    "    \n",
    "    # Apply dilution correction to concentration and rename\n",
    "    if 'concentration_cm-3_avg' in updated_df.columns:\n",
    "        updated_df['particles_per_mL_avg'] = updated_df['concentration_cm-3_avg'] * dilution_factor\n",
    "        if 'concentration_cm-3_sd' in updated_df.columns:\n",
    "            updated_df['particles_per_mL_sd'] = updated_df['concentration_cm-3_sd'] * dilution_factor\n",
    "        \n",
    "        # Remove old concentration columns\n",
    "        updated_df = updated_df.drop(['concentration_cm-3_avg', 'concentration_cm-3_sd'], axis=1, errors='ignore')\n",
    "    else:\n",
    "        return False, \"Missing concentration_cm-3_avg column for dilution correction\"\n",
    "    \n",
    "    # Apply dilution correction to volume and rename to per_mL\n",
    "    if 'volume_nm^3_avg' in updated_df.columns:\n",
    "        updated_df['volume_nm^3_per_mL_avg'] = updated_df['volume_nm^3_avg'] * dilution_factor\n",
    "        if 'volume_nm^3_sd' in updated_df.columns:\n",
    "            updated_df['volume_nm^3_per_mL_sd'] = updated_df['volume_nm^3_sd'] * dilution_factor\n",
    "        \n",
    "        # Remove old volume columns\n",
    "        updated_df = updated_df.drop(['volume_nm^3_avg', 'volume_nm^3_sd'], axis=1, errors='ignore')\n",
    "    \n",
    "    # Apply dilution correction to area and rename to per_mL\n",
    "    if 'area_nm^2_avg' in updated_df.columns:\n",
    "        updated_df['area_nm^2_per_mL_avg'] = updated_df['area_nm^2_avg'] * dilution_factor\n",
    "        if 'area_nm^2_sd' in updated_df.columns:\n",
    "            updated_df['area_nm^2_per_mL_sd'] = updated_df['area_nm^2_sd'] * dilution_factor\n",
    "        \n",
    "        # Remove old area columns\n",
    "        updated_df = updated_df.drop(['area_nm^2_avg', 'area_nm^2_sd'], axis=1, errors='ignore')\n",
    "    \n",
    "    return True, updated_df\n",
    "\n",
    "\n",
    "def normalize_distributions_with_uncertainty(df, size_column='size_nm'):\n",
    "    \"\"\"\n",
    "    Normalize particle distributions by area under the curve with uncertainty propagation.\n",
    "    \n",
    "    This creates normalized number distributions from the averaged number data.\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): DataFrame containing averaged particle distribution data\n",
    "    size_column (str): Name of the column containing size values\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: Updated dataframe with normalized columns and uncertainties\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying the original dataframe\n",
    "    normalized_df = df.copy()\n",
    "    \n",
    "    # Process each scale separately\n",
    "    for scale in normalized_df['scale'].unique():\n",
    "        scale_mask = normalized_df['scale'] == scale\n",
    "        scale_data = normalized_df[scale_mask].copy()\n",
    "        \n",
    "        if scale_data.empty or 'number_avg' not in scale_data.columns:\n",
    "            continue\n",
    "        \n",
    "        # Sort by size for correct integration\n",
    "        scale_data = scale_data.sort_values(size_column)\n",
    "        \n",
    "        # Calculate area under the curve using trapezoidal rule\n",
    "        sizes = scale_data[size_column].values\n",
    "        numbers_avg = scale_data['number_avg'].values\n",
    "        \n",
    "        if len(sizes) < 2:\n",
    "            continue\n",
    "        \n",
    "        # Calculate area for normalization\n",
    "        area_avg = np.trapz(numbers_avg, sizes)\n",
    "        \n",
    "        if area_avg > 0:\n",
    "            # Normalize the averages\n",
    "            normalized_df.loc[scale_mask, 'number_normalized_avg'] = numbers_avg / area_avg\n",
    "            \n",
    "            # Normalize the standard deviations (uncertainty propagation)\n",
    "            if 'number_sd' in scale_data.columns:\n",
    "                numbers_sd = scale_data['number_sd'].values\n",
    "                normalized_df.loc[scale_mask, 'number_normalized_sd'] = numbers_sd / area_avg\n",
    "            else:\n",
    "                normalized_df.loc[scale_mask, 'number_normalized_sd'] = 0.0\n",
    "        else:\n",
    "            # If area is zero, set normalized values to zero\n",
    "            normalized_df.loc[scale_mask, 'number_normalized_avg'] = 0.0\n",
    "            normalized_df.loc[scale_mask, 'number_normalized_sd'] = 0.0\n",
    "    \n",
    "    return normalized_df\n",
    "\n",
    "\n",
    "def calculate_cumulative_distributions_with_uncertainty(df, scale_column='scale'):\n",
    "    \"\"\"\n",
    "    Calculate cumulative distributions with proper uncertainty propagation.\n",
    "    \n",
    "    For independent uncertainties, cumulative uncertainties are calculated as:\n",
    "    œÉ_cumsum[j] = ‚àö(Œ£(i=0 to j) œÉ[i]¬≤)\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): DataFrame containing particle distribution data with uncertainties\n",
    "    scale_column (str): Name of the column distinguishing scale types\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: Updated dataframe with cumulative distribution columns and uncertainties\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying the original\n",
    "    result_df = df.copy()\n",
    "    \n",
    "    # Process each scale type separately\n",
    "    for scale in result_df[scale_column].unique():\n",
    "        # Filter for current scale\n",
    "        scale_mask = result_df[scale_column] == scale\n",
    "        scale_indices = result_df[scale_mask].sort_values('size_nm').index\n",
    "        \n",
    "        if len(scale_indices) == 0:\n",
    "            continue\n",
    "        \n",
    "        # 1. Normalized number distribution (shape analysis)\n",
    "        if 'number_normalized_avg' in result_df.columns:\n",
    "            # Calculate cumulative sum\n",
    "            cumsum_avg = result_df.loc[scale_indices, 'number_normalized_avg'].cumsum()\n",
    "            \n",
    "            # Normalize to 0-1 range (should be close to 1 already due to normalization)\n",
    "            if cumsum_avg.iloc[-1] > 0:\n",
    "                result_df.loc[scale_indices, 'number_normalized_cumsum_avg'] = cumsum_avg / cumsum_avg.iloc[-1]\n",
    "            else:\n",
    "                result_df.loc[scale_indices, 'number_normalized_cumsum_avg'] = 0\n",
    "            \n",
    "            # Calculate uncertainty in cumulative sum using error propagation\n",
    "            if 'number_normalized_sd' in result_df.columns:\n",
    "                # For cumulative sum: œÉ_cumsum[j] = ‚àö(Œ£(i=0 to j) œÉ[i]¬≤)\n",
    "                normalized_var_cumsum = (result_df.loc[scale_indices, 'number_normalized_sd'] ** 2).cumsum()\n",
    "                cumsum_sd = np.sqrt(normalized_var_cumsum)\n",
    "                \n",
    "                # Normalize the uncertainty as well\n",
    "                if cumsum_avg.iloc[-1] > 0:\n",
    "                    result_df.loc[scale_indices, 'number_normalized_cumsum_sd'] = cumsum_sd / cumsum_avg.iloc[-1]\n",
    "                else:\n",
    "                    result_df.loc[scale_indices, 'number_normalized_cumsum_sd'] = 0\n",
    "        \n",
    "        # 2. Absolute volume distribution\n",
    "        if 'volume_nm^3_per_mL_avg' in result_df.columns:\n",
    "            # Calculate cumulative sum for averages\n",
    "            cumsum_avg = result_df.loc[scale_indices, 'volume_nm^3_per_mL_avg'].cumsum()\n",
    "            result_df.loc[scale_indices, 'volume_nm^3_per_mL_cumsum_avg'] = cumsum_avg\n",
    "            \n",
    "            # Calculate uncertainty in cumulative sum\n",
    "            if 'volume_nm^3_per_mL_sd' in result_df.columns:\n",
    "                volume_var_cumsum = (result_df.loc[scale_indices, 'volume_nm^3_per_mL_sd'] ** 2).cumsum()\n",
    "                result_df.loc[scale_indices, 'volume_nm^3_per_mL_cumsum_sd'] = np.sqrt(volume_var_cumsum)\n",
    "        \n",
    "        # 3. Absolute surface area distribution\n",
    "        if 'area_nm^2_per_mL_avg' in result_df.columns:\n",
    "            # Calculate cumulative sum for averages\n",
    "            cumsum_avg = result_df.loc[scale_indices, 'area_nm^2_per_mL_avg'].cumsum()\n",
    "            result_df.loc[scale_indices, 'area_nm^2_per_mL_cumsum_avg'] = cumsum_avg\n",
    "            \n",
    "            # Calculate uncertainty in cumulative sum\n",
    "            if 'area_nm^2_per_mL_sd' in result_df.columns:\n",
    "                area_var_cumsum = (result_df.loc[scale_indices, 'area_nm^2_per_mL_sd'] ** 2).cumsum()\n",
    "                result_df.loc[scale_indices, 'area_nm^2_per_mL_cumsum_sd'] = np.sqrt(area_var_cumsum)\n",
    "                \n",
    "    return result_df\n",
    "\n",
    "\n",
    "def calculate_total_metrics_with_uncertainty(df, scale_column='scale'):\n",
    "    \"\"\"\n",
    "    Calculate total metrics for each scale with proper uncertainty propagation.\n",
    "    \n",
    "    For totals across bins, uncertainties are combined as: œÉ_total = ‚àö(Œ£ œÉ_i¬≤)\n",
    "    For derived metrics, simple calculations are used without uncertainty propagation.\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): DataFrame containing particle distribution data with uncertainties\n",
    "    scale_column (str): Name of the column distinguishing scale types\n",
    "    \n",
    "    Returns:\n",
    "    dict: Dictionary with total metrics and uncertainties for each scale\n",
    "    \"\"\"\n",
    "    # Initialize results structure\n",
    "    results = {}\n",
    "    \n",
    "    # Process each scale type separately\n",
    "    for scale in df[scale_column].unique():\n",
    "        # Filter for current scale and sort by size\n",
    "        scale_df = df[df[scale_column] == scale].sort_values('size_nm')\n",
    "        \n",
    "        # Initialize metrics for this scale\n",
    "        scale_metrics = {}\n",
    "        \n",
    "        # Only calculate if we have data for this scale\n",
    "        if not scale_df.empty:\n",
    "            \n",
    "            # 1. Total particles per mL\n",
    "            if 'particles_per_mL_avg' in scale_df.columns:\n",
    "                total_particles_avg = scale_df['particles_per_mL_avg'].sum()\n",
    "                scale_metrics['total_particles_per_mL_avg'] = total_particles_avg\n",
    "                scale_metrics['total_particles_per_uL_avg'] = total_particles_avg / 1000\n",
    "                \n",
    "                # Calculate uncertainty: œÉ_total = ‚àö(Œ£ œÉ_i¬≤)\n",
    "                if 'particles_per_mL_sd' in scale_df.columns:\n",
    "                    total_particles_sd = np.sqrt((scale_df['particles_per_mL_sd'] ** 2).sum())\n",
    "                    scale_metrics['total_particles_per_mL_sd'] = total_particles_sd\n",
    "                    scale_metrics['total_particles_per_uL_sd'] = total_particles_sd / 1000\n",
    "            \n",
    "            # 2. Total volume per mL\n",
    "            if 'volume_nm^3_per_mL_avg' in scale_df.columns:\n",
    "                total_volume_avg = scale_df['volume_nm^3_per_mL_avg'].sum()\n",
    "                scale_metrics['total_volume_nm^3_per_mL_avg'] = total_volume_avg\n",
    "                scale_metrics['total_volume_um^3_per_mL_avg'] = total_volume_avg / 1e9  # nm¬≥ to Œºm¬≥\n",
    "                scale_metrics['total_volume_uL_per_mL_avg'] = total_volume_avg / 1e18  # nm¬≥ to ŒºL\n",
    "                scale_metrics['volume_percentage_avg'] = (total_volume_avg / 1e18) * 0.1  # percentage\n",
    "                \n",
    "                # Calculate uncertainty\n",
    "                if 'volume_nm^3_per_mL_sd' in scale_df.columns:\n",
    "                    total_volume_sd = np.sqrt((scale_df['volume_nm^3_per_mL_sd'] ** 2).sum())\n",
    "                    scale_metrics['total_volume_nm^3_per_mL_sd'] = total_volume_sd\n",
    "                    scale_metrics['total_volume_um^3_per_mL_sd'] = total_volume_sd / 1e9\n",
    "                    scale_metrics['total_volume_uL_per_mL_sd'] = total_volume_sd / 1e18\n",
    "                    scale_metrics['volume_percentage_sd'] = (total_volume_sd / 1e18) * 0.1\n",
    "            \n",
    "            # 3. Total surface area per mL\n",
    "            if 'area_nm^2_per_mL_avg' in scale_df.columns:\n",
    "                total_area_avg = scale_df['area_nm^2_per_mL_avg'].sum()\n",
    "                scale_metrics['total_surface_area_nm^2_per_mL_avg'] = total_area_avg\n",
    "                scale_metrics['total_surface_area_um^2_per_mL_avg'] = total_area_avg / 1e6  # nm¬≤ to Œºm¬≤\n",
    "                scale_metrics['total_surface_area_cm^2_per_mL_avg'] = total_area_avg / 1e14  # nm¬≤ to cm¬≤\n",
    "                \n",
    "                # Calculate uncertainty\n",
    "                if 'area_nm^2_per_mL_sd' in scale_df.columns:\n",
    "                    total_area_sd = np.sqrt((scale_df['area_nm^2_per_mL_sd'] ** 2).sum())\n",
    "                    scale_metrics['total_surface_area_nm^2_per_mL_sd'] = total_area_sd\n",
    "                    scale_metrics['total_surface_area_um^2_per_mL_sd'] = total_area_sd / 1e6\n",
    "                    scale_metrics['total_surface_area_cm^2_per_mL_sd'] = total_area_sd / 1e14\n",
    "                \n",
    "                # 4. Specific surface area (derived metric, no uncertainty propagation for now)\n",
    "                if ('total_volume_nm^3_per_mL_avg' in scale_metrics and \n",
    "                    scale_metrics['total_volume_nm^3_per_mL_avg'] > 0):\n",
    "                    # Surface area (nm¬≤) / volume (nm¬≥) = 1/nm\n",
    "                    ssa_1_per_nm = total_area_avg / scale_metrics['total_volume_nm^3_per_mL_avg']\n",
    "                    # Convert to m¬≤/cm¬≥ (standard unit)\n",
    "                    scale_metrics['specific_surface_area_m^2_per_cm^3_avg'] = ssa_1_per_nm * 10\n",
    "        \n",
    "        # Store metrics for this scale\n",
    "        results[scale] = scale_metrics\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def add_metrics_to_metadata_with_uncertainty(metadata, metrics, scale='linear'):\n",
    "    \"\"\"\n",
    "    Add key metrics with uncertainties to the metadata dictionary.\n",
    "    \n",
    "    Parameters:\n",
    "    metadata (dict): Current metadata dictionary\n",
    "    metrics (dict): Dictionary of calculated metrics with uncertainties\n",
    "    scale (str): Which scale's metrics to use ('linear' or 'logarithmic')\n",
    "    \n",
    "    Returns:\n",
    "    dict: Updated metadata dictionary\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying the original\n",
    "    updated_metadata = metadata.copy()\n",
    "    \n",
    "    # Use linear scale metrics by default, but check if available\n",
    "    if scale not in metrics:\n",
    "        # Fall back to any available scale\n",
    "        if metrics:\n",
    "            scale = list(metrics.keys())[0]\n",
    "        else:\n",
    "            return updated_metadata  # Return unchanged if no metrics available\n",
    "    \n",
    "    scale_metrics = metrics[scale]\n",
    "    \n",
    "    # Add key metrics to metadata with uncertainties\n",
    "    if 'total_particles_per_mL_avg' in scale_metrics:\n",
    "        avg_val = scale_metrics['total_particles_per_mL_avg']\n",
    "        sd_val = scale_metrics.get('total_particles_per_mL_sd', 0)\n",
    "        updated_metadata['nta_total_particles_per_mL'] = f\"{avg_val:.2E} ¬± {sd_val:.2E}\"\n",
    "    \n",
    "    if 'total_volume_uL_per_mL_avg' in scale_metrics:\n",
    "        avg_val = scale_metrics['total_volume_uL_per_mL_avg']\n",
    "        sd_val = scale_metrics.get('total_volume_uL_per_mL_sd', 0)\n",
    "        updated_metadata['nta_total_volume_uL_per_mL'] = f\"{avg_val:.4E} ¬± {sd_val:.4E}\"\n",
    "    \n",
    "    if 'volume_percentage_avg' in scale_metrics:\n",
    "        avg_val = scale_metrics['volume_percentage_avg']\n",
    "        sd_val = scale_metrics.get('volume_percentage_sd', 0)\n",
    "        updated_metadata['nta_volume_percentage'] = f\"{avg_val:.6f} ¬± {sd_val:.6f}\"\n",
    "    \n",
    "    if 'total_surface_area_cm^2_per_mL_avg' in scale_metrics:\n",
    "        avg_val = scale_metrics['total_surface_area_cm^2_per_mL_avg']\n",
    "        sd_val = scale_metrics.get('total_surface_area_cm^2_per_mL_sd', 0)\n",
    "        updated_metadata['nta_total_surface_area_cm^2_per_mL'] = f\"{avg_val:.4E} ¬± {sd_val:.4E}\"\n",
    "    \n",
    "    if 'specific_surface_area_m^2_per_cm^3_avg' in scale_metrics:\n",
    "        avg_val = scale_metrics['specific_surface_area_m^2_per_cm^3_avg']\n",
    "        # No uncertainty for derived metrics yet\n",
    "        updated_metadata['nta_specific_surface_area_m^2_per_cm^3'] = f\"{avg_val:.2f}\"\n",
    "    \n",
    "    # Add a scale indicator and number of replicates\n",
    "    updated_metadata['nta_metrics_scale'] = scale\n",
    "    if 'num_replicates' in updated_metadata:\n",
    "        updated_metadata['nta_metrics_replicates'] = updated_metadata['num_replicates']\n",
    "    \n",
    "    return updated_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17989377-a304-41b4-8772-a83c90bd6eb1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "EXECUTING CORE CALCULATIONS WITH UNCERTAINTY PROPAGATION\n",
      "================================================================================\n",
      "\n",
      "1. Applying dilution correction with uncertainty propagation...\n",
      "Applying dilution correction: factor = 100000.0 (source: metadata (nta_dilution))\n",
      "‚úì Dilution correction completed successfully\n",
      "\n",
      "2. Normalizing distributions with uncertainty propagation...\n",
      "‚úì Normalization completed successfully\n",
      "\n",
      "3. Calculating cumulative distributions with uncertainty...\n",
      "‚úì Cumulative distributions completed successfully\n",
      "\n",
      "4. Calculating total metrics with uncertainty...\n",
      "\n",
      "LINEAR SCALE - TOTAL METRICS WITH UNCERTAINTIES:\n",
      "  Particle concentration: 4.92E+12 ¬± 9.95E+10 particles/mL\n",
      "                         4.92E+09 ¬± 9.95E+07 particles/¬µL\n",
      "  Volume Information:\n",
      "    3.70E+14 ¬± 2.50E+13 nm¬≥/mL\n",
      "    3.70E+05 ¬± 2.50E+04 ¬µm¬≥/mL\n",
      "    3.6953E-04 ¬± 2.4993E-05 ¬µL/mL\n",
      "    0.000037 ¬± 0.000002% of sample volume\n",
      "  Surface Area Information:\n",
      "    3.66E+12 ¬± 1.66E+11 nm¬≤/mL\n",
      "    3.66E+06 ¬± 1.66E+05 ¬µm¬≤/mL\n",
      "    3.6562E-02 ¬± 1.6587E-03 cm¬≤/mL\n",
      "  Specific Surface Area:\n",
      "    0.10 m¬≤/cm¬≥\n",
      "\n",
      "LOGARITHMIC SCALE - TOTAL METRICS WITH UNCERTAINTIES:\n",
      "  Particle concentration: 4.92E+12 ¬± 1.14E+11 particles/mL\n",
      "                         4.92E+09 ¬± 1.14E+08 particles/¬µL\n",
      "  Volume Information:\n",
      "    3.70E+14 ¬± 2.65E+13 nm¬≥/mL\n",
      "    3.70E+05 ¬± 2.65E+04 ¬µm¬≥/mL\n",
      "    3.6982E-04 ¬± 2.6530E-05 ¬µL/mL\n",
      "    0.000037 ¬± 0.000003% of sample volume\n",
      "  Surface Area Information:\n",
      "    3.66E+12 ¬± 1.67E+11 nm¬≤/mL\n",
      "    3.66E+06 ¬± 1.67E+05 ¬µm¬≤/mL\n",
      "    3.6560E-02 ¬± 1.6740E-03 cm¬≤/mL\n",
      "  Specific Surface Area:\n",
      "    0.10 m¬≤/cm¬≥\n",
      "\n",
      "5. Updating metadata with calculated metrics...\n",
      "‚úì Metadata updated with enhanced metrics including uncertainties\n",
      "\n",
      "6. Updating existing metadata file...\n",
      "‚úì Updated existing metadata file: /Users/hboehm/Seafile/LEAF/NTA_data/!Inbox/20250527/metadata/Data_20250527_0004_23052025_SUV_5136664_after6weeks_size_488_avg3_metadata.txt\n",
      "\n",
      "7. Saving processed distribution data...\n",
      "‚úì Saved processed distribution data to: /Users/hboehm/Seafile/LEAF/NTA_data/!Inbox/20250527/processed/Data_20250527_0004_23052025_SUV_5136664_after6weeks_size_488_avg3_PSD.txt\n",
      "  Columns saved: 21\n",
      "  Data points: 1400 (1200 linear, 200 log)\n",
      "\n",
      "================================================================================\n",
      "CORE CALCULATIONS COMPLETED SUCCESSFULLY!\n",
      "================================================================================\n",
      "Dataset ready for statistics calculation and visualization.\n",
      "Enhanced data stored in: current_distribution_df\n",
      "Total metrics with uncertainties stored in: current_total_metrics\n",
      "Files saved:\n",
      "  Metadata: Data_20250527_0004_23052025_SUV_5136664_after6weeks_size_488_avg3_metadata.txt\n",
      "  Distribution: Data_20250527_0004_23052025_SUV_5136664_after6weeks_size_488_avg3_PSD.txt\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# EXECUTE CORE CALCULATIONS WITH UNCERTAINTY PROPAGATION (Cell 05.2)\n",
    "# ================================================================\n",
    "\n",
    "# Execute the full calculation workflow if we have current data\n",
    "if 'current_distribution_df' in globals() and current_distribution_df is not None:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"EXECUTING CORE CALCULATIONS WITH UNCERTAINTY PROPAGATION\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Step 1: Apply dilution correction first (creates proper per_mL columns)\n",
    "    print(\"\\n1. Applying dilution correction with uncertainty propagation...\")\n",
    "    try:\n",
    "        success, dilution_corrected_df = apply_dilution_correction_with_uncertainty(\n",
    "            current_distribution_df, \n",
    "            metadata=current_metadata if 'current_metadata' in globals() else None\n",
    "        )\n",
    "        if success:\n",
    "            print(\"‚úì Dilution correction completed successfully\")\n",
    "            current_distribution_df = dilution_corrected_df  # Update global variable\n",
    "        else:\n",
    "            print(f\"‚úó Dilution correction failed: {dilution_corrected_df}\")\n",
    "            dilution_corrected_df = current_distribution_df\n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Dilution correction failed: {e}\")\n",
    "        dilution_corrected_df = current_distribution_df\n",
    "    \n",
    "    # Step 2: Normalize distributions with uncertainty propagation\n",
    "    print(\"\\n2. Normalizing distributions with uncertainty propagation...\")\n",
    "    try:\n",
    "        normalized_df = normalize_distributions_with_uncertainty(dilution_corrected_df)\n",
    "        print(\"‚úì Normalization completed successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Normalization failed: {e}\")\n",
    "        normalized_df = dilution_corrected_df\n",
    "    \n",
    "    # Step 3: Calculate cumulative distributions with uncertainty (includes volume and surface area)\n",
    "    print(\"\\n3. Calculating cumulative distributions with uncertainty...\")\n",
    "    try:\n",
    "        final_df = calculate_cumulative_distributions_with_uncertainty(normalized_df)\n",
    "        print(\"‚úì Cumulative distributions completed successfully\")\n",
    "        current_distribution_df = final_df  # Update global variable\n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Cumulative distributions failed: {e}\")\n",
    "        current_distribution_df = normalized_df\n",
    "    \n",
    "    # Step 4: Calculate total metrics with uncertainty\n",
    "    print(\"\\n4. Calculating total metrics with uncertainty...\")\n",
    "    try:\n",
    "        total_metrics = calculate_total_metrics_with_uncertainty(current_distribution_df)\n",
    "        \n",
    "        # Display enhanced metrics for each scale\n",
    "        for scale, metrics in total_metrics.items():\n",
    "            if metrics:\n",
    "                print(f\"\\n{scale.upper()} SCALE - TOTAL METRICS WITH UNCERTAINTIES:\")\n",
    "                \n",
    "                # Particle count metrics\n",
    "                if 'total_particles_per_mL_avg' in metrics:\n",
    "                    avg_val = metrics['total_particles_per_mL_avg']\n",
    "                    sd_val = metrics.get('total_particles_per_mL_sd', 0)\n",
    "                    print(f\"  Particle concentration: {avg_val:.2E} ¬± {sd_val:.2E} particles/mL\")\n",
    "                    \n",
    "                    if 'total_particles_per_uL_avg' in metrics:\n",
    "                        avg_val_ul = metrics['total_particles_per_uL_avg']\n",
    "                        sd_val_ul = metrics.get('total_particles_per_uL_sd', 0)\n",
    "                        print(f\"                         {avg_val_ul:.2E} ¬± {sd_val_ul:.2E} particles/¬µL\")\n",
    "                \n",
    "                # Volume metrics\n",
    "                if 'total_volume_nm^3_per_mL_avg' in metrics:\n",
    "                    print(f\"  Volume Information:\")\n",
    "                    \n",
    "                    avg_val = metrics['total_volume_nm^3_per_mL_avg']\n",
    "                    sd_val = metrics.get('total_volume_nm^3_per_mL_sd', 0)\n",
    "                    print(f\"    {avg_val:.2E} ¬± {sd_val:.2E} nm¬≥/mL\")\n",
    "                    \n",
    "                    if 'total_volume_um^3_per_mL_avg' in metrics:\n",
    "                        avg_val_um = metrics['total_volume_um^3_per_mL_avg']\n",
    "                        sd_val_um = metrics.get('total_volume_um^3_per_mL_sd', 0)\n",
    "                        print(f\"    {avg_val_um:.2E} ¬± {sd_val_um:.2E} ¬µm¬≥/mL\")\n",
    "                    \n",
    "                    if 'total_volume_uL_per_mL_avg' in metrics:\n",
    "                        avg_val_ul = metrics['total_volume_uL_per_mL_avg']\n",
    "                        sd_val_ul = metrics.get('total_volume_uL_per_mL_sd', 0)\n",
    "                        print(f\"    {avg_val_ul:.4E} ¬± {sd_val_ul:.4E} ¬µL/mL\")\n",
    "                    \n",
    "                    if 'volume_percentage_avg' in metrics:\n",
    "                        avg_val_pct = metrics['volume_percentage_avg']\n",
    "                        sd_val_pct = metrics.get('volume_percentage_sd', 0)\n",
    "                        print(f\"    {avg_val_pct:.6f} ¬± {sd_val_pct:.6f}% of sample volume\")\n",
    "                \n",
    "                # Surface area metrics\n",
    "                if 'total_surface_area_nm^2_per_mL_avg' in metrics:\n",
    "                    print(f\"  Surface Area Information:\")\n",
    "                    \n",
    "                    avg_val = metrics['total_surface_area_nm^2_per_mL_avg']\n",
    "                    sd_val = metrics.get('total_surface_area_nm^2_per_mL_sd', 0)\n",
    "                    print(f\"    {avg_val:.2E} ¬± {sd_val:.2E} nm¬≤/mL\")\n",
    "                    \n",
    "                    if 'total_surface_area_um^2_per_mL_avg' in metrics:\n",
    "                        avg_val_um = metrics['total_surface_area_um^2_per_mL_avg']\n",
    "                        sd_val_um = metrics.get('total_surface_area_um^2_per_mL_sd', 0)\n",
    "                        print(f\"    {avg_val_um:.2E} ¬± {sd_val_um:.2E} ¬µm¬≤/mL\")\n",
    "                    \n",
    "                    if 'total_surface_area_cm^2_per_mL_avg' in metrics:\n",
    "                        avg_val_cm = metrics['total_surface_area_cm^2_per_mL_avg']\n",
    "                        sd_val_cm = metrics.get('total_surface_area_cm^2_per_mL_sd', 0)\n",
    "                        print(f\"    {avg_val_cm:.4E} ¬± {sd_val_cm:.4E} cm¬≤/mL\")\n",
    "                \n",
    "                # Specific surface area (no uncertainty for now)\n",
    "                if 'specific_surface_area_m^2_per_cm^3_avg' in metrics:\n",
    "                    print(f\"  Specific Surface Area:\")\n",
    "                    print(f\"    {metrics['specific_surface_area_m^2_per_cm^3_avg']:.2f} m¬≤/cm¬≥\")\n",
    "        \n",
    "        # Step 5: Update metadata with enhanced metrics\n",
    "        if 'current_metadata' in globals():\n",
    "            print(\"\\n5. Updating metadata with calculated metrics...\")\n",
    "            try:\n",
    "                current_metadata = add_metrics_to_metadata_with_uncertainty(\n",
    "                    current_metadata, total_metrics\n",
    "                )\n",
    "                print(\"‚úì Metadata updated with enhanced metrics including uncertainties\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚úó Metadata update failed: {e}\")\n",
    "        \n",
    "        # Step 6: Update existing metadata file (don't overwrite)\n",
    "        print(\"\\n6. Updating existing metadata file...\")\n",
    "        try:\n",
    "            # Get output directory\n",
    "            if 'CONFIG' in globals() and \"directory\" in CONFIG:\n",
    "                metadata_dir = os.path.join(CONFIG[\"directory\"], \"metadata\")\n",
    "            else:\n",
    "                metadata_dir = os.path.join(os.getcwd(), \"metadata\")\n",
    "            \n",
    "            # Create metadata file path\n",
    "            unique_id = current_metadata.get('persistentID', 'unknown')\n",
    "            metadata_path = os.path.join(metadata_dir, f\"Data_{unique_id}_metadata.txt\")\n",
    "            \n",
    "            # Read existing metadata file to preserve all fields\n",
    "            existing_metadata = {}\n",
    "            if os.path.exists(metadata_path):\n",
    "                with open(metadata_path, 'r') as f:\n",
    "                    for line in f:\n",
    "                        parts = line.strip().split('\\t')\n",
    "                        if len(parts) >= 2:\n",
    "                            existing_metadata[parts[0]] = parts[1]\n",
    "            \n",
    "            # Update existing metadata with our enhanced values\n",
    "            existing_metadata.update(current_metadata)\n",
    "            \n",
    "            # Write back the combined metadata\n",
    "            with open(metadata_path, 'w') as f:\n",
    "                for key, value in existing_metadata.items():\n",
    "                    f.write(f\"{key}\\t{value}\\t\\n\")\n",
    "            \n",
    "            print(f\"‚úì Updated existing metadata file: {metadata_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚úó Failed to update metadata file: {e}\")\n",
    "        \n",
    "        # Step 7: Save processed distribution data (clean, analysis-ready data only)\n",
    "        print(\"\\n7. Saving processed distribution data...\")\n",
    "        try:\n",
    "            # Get output directory\n",
    "            if 'CONFIG' in globals() and \"directory\" in CONFIG:\n",
    "                processed_dir = os.path.join(CONFIG[\"directory\"], \"processed\")\n",
    "            else:\n",
    "                processed_dir = os.path.join(os.getcwd(), \"processed\")\n",
    "            \n",
    "            # Ensure processed directory exists\n",
    "            os.makedirs(processed_dir, exist_ok=True)\n",
    "            \n",
    "            # The dataframe is already cleaned in the calculation step\n",
    "            # Just save it directly with the correct column order\n",
    "            clean_df = current_distribution_df.copy()\n",
    "            \n",
    "            # Create PSD file path\n",
    "            unique_id = current_metadata.get('persistentID', 'unknown')\n",
    "            psd_path = os.path.join(processed_dir, f\"Data_{unique_id}_PSD.txt\")\n",
    "            \n",
    "            # Save the clean distribution data as tab-separated file\n",
    "            clean_df.to_csv(psd_path, sep='\\t', index=False)\n",
    "            \n",
    "            print(f\"‚úì Saved processed distribution data to: {psd_path}\")\n",
    "            print(f\"  Columns saved: {len(clean_df.columns)}\")\n",
    "            print(f\"  Data points: {len(clean_df)} ({len(clean_df[clean_df['scale']=='linear'])} linear, {len(clean_df[clean_df['scale']=='logarithmic'])} log)\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚úó Failed to save distribution data: {e}\")\n",
    "        \n",
    "        # Store results for downstream analysis\n",
    "        current_total_metrics = total_metrics\n",
    "        \n",
    "        print(f\"\\n\" + \"=\" * 80)\n",
    "        print(\"CORE CALCULATIONS COMPLETED SUCCESSFULLY!\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"Dataset ready for statistics calculation and visualization.\")\n",
    "        print(f\"Enhanced data stored in: current_distribution_df\")\n",
    "        print(f\"Total metrics with uncertainties stored in: current_total_metrics\")\n",
    "        print(f\"Files saved:\")\n",
    "        print(f\"  Metadata: Data_{unique_id}_metadata.txt\")\n",
    "        print(f\"  Distribution: Data_{unique_id}_PSD.txt\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Total metrics calculation failed: {e}\")\n",
    "\n",
    "else:\n",
    "    print(\"No distribution data found in 'current_distribution_df'.\")\n",
    "    print(\"Please run the data extraction and averaging workflow first:\")\n",
    "    print(\"  cell00_multipleFiles ‚Üí cell02_multipleFiles ‚Üí cell03_multipleFiles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab390cbe-ed8e-4128-86e6-2aced16dd881",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CALCULATING PARTICLE SIZE DISTRIBUTION STATISTICS\n",
      "================================================================================\n",
      "\n",
      "Calculating statistics for LINEAR scale:\n",
      "  Processing number-weighted distribution...\n",
      "    ‚úì D10: 78.98 nm (77.95 - 80.29)\n",
      "    ‚úì D50: 116.80 nm (114.23 - 119.79)\n",
      "    ‚úì D90: 166.65 nm (157.24 - 180.97)\n",
      "    ‚úì Span: 0.751 (0.642 - 0.902)\n",
      "  Processing volume-weighted distribution...\n",
      "    ‚úì D10: 104.30 nm (103.17 - 105.52)\n",
      "    ‚úì D50: 150.93 nm (147.65 - 154.32)\n",
      "    ‚úì D90: 245.41 nm (220.59 - 351.05)\n",
      "    ‚úì Span: 0.935 (0.746 - 1.679)\n",
      "  Processing surface_area-weighted distribution...\n",
      "    ‚úì D10: 95.57 nm (94.29 - 97.22)\n",
      "    ‚úì D50: 137.88 nm (135.04 - 140.98)\n",
      "    ‚úì D90: 200.61 nm (187.06 - 225.34)\n",
      "    ‚úì Span: 0.762 (0.637 - 0.970)\n",
      "\n",
      "Calculating statistics for LOGARITHMIC scale:\n",
      "  Processing number-weighted distribution...\n",
      "    ‚úì D10: 79.89 nm (78.89 - 81.11)\n",
      "    ‚úì D50: 117.24 nm (114.82 - 119.94)\n",
      "    ‚úì D90: 165.96 nm (157.23 - 179.95)\n",
      "    ‚úì Span: 0.734 (0.635 - 0.880)\n",
      "  Processing volume-weighted distribution...\n",
      "    ‚úì D10: 104.93 nm (103.95 - 105.96)\n",
      "    ‚úì D50: 150.77 nm (147.67 - 154.12)\n",
      "    ‚úì D90: 246.32 nm (219.57 - 352.48)\n",
      "    ‚úì Span: 0.938 (0.737 - 1.683)\n",
      "  Processing surface_area-weighted distribution...\n",
      "    ‚úì D10: 96.22 nm (95.00 - 97.54)\n",
      "    ‚úì D50: 137.85 nm (135.20 - 140.90)\n",
      "    ‚úì D90: 200.00 nm (185.97 - 224.46)\n",
      "    ‚úì Span: 0.753 (0.628 - 0.957)\n",
      "\n",
      "Creating comprehensive statistics table...\n",
      "Statistics Table:\n",
      "========================================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scale</th>\n",
       "      <th>distribution</th>\n",
       "      <th>D10</th>\n",
       "      <th>D10_lower</th>\n",
       "      <th>D10_upper</th>\n",
       "      <th>D50</th>\n",
       "      <th>D50_lower</th>\n",
       "      <th>D50_upper</th>\n",
       "      <th>D90</th>\n",
       "      <th>D90_lower</th>\n",
       "      <th>...</th>\n",
       "      <th>total_particles_per_mL</th>\n",
       "      <th>total_particles_sd</th>\n",
       "      <th>total_volume_uL_per_mL</th>\n",
       "      <th>total_volume_sd</th>\n",
       "      <th>total_surface_area_cm2_per_mL</th>\n",
       "      <th>total_surface_area_sd</th>\n",
       "      <th>volume_percentage</th>\n",
       "      <th>volume_percentage_sd</th>\n",
       "      <th>specific_surface_area_m2_per_cm3</th>\n",
       "      <th>specific_surface_area_sd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>linear</td>\n",
       "      <td>number_normalized</td>\n",
       "      <td>78.978723</td>\n",
       "      <td>77.947254</td>\n",
       "      <td>80.285095</td>\n",
       "      <td>116.798780</td>\n",
       "      <td>114.225104</td>\n",
       "      <td>119.793059</td>\n",
       "      <td>166.653846</td>\n",
       "      <td>157.243913</td>\n",
       "      <td>...</td>\n",
       "      <td>4.922490e+12</td>\n",
       "      <td>9.949400e+10</td>\n",
       "      <td>0.00037</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.036562</td>\n",
       "      <td>0.001659</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>989.407046</td>\n",
       "      <td>80.577841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>linear</td>\n",
       "      <td>volume_per_mL</td>\n",
       "      <td>104.301224</td>\n",
       "      <td>103.169434</td>\n",
       "      <td>105.521429</td>\n",
       "      <td>150.933352</td>\n",
       "      <td>147.646185</td>\n",
       "      <td>154.318915</td>\n",
       "      <td>245.413208</td>\n",
       "      <td>220.592249</td>\n",
       "      <td>...</td>\n",
       "      <td>4.922490e+12</td>\n",
       "      <td>9.949400e+10</td>\n",
       "      <td>0.00037</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.036562</td>\n",
       "      <td>0.001659</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>989.407046</td>\n",
       "      <td>80.577841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>linear</td>\n",
       "      <td>surface_area_per_mL</td>\n",
       "      <td>95.567250</td>\n",
       "      <td>94.286978</td>\n",
       "      <td>97.222066</td>\n",
       "      <td>137.877915</td>\n",
       "      <td>135.038201</td>\n",
       "      <td>140.980536</td>\n",
       "      <td>200.608232</td>\n",
       "      <td>187.055296</td>\n",
       "      <td>...</td>\n",
       "      <td>4.922490e+12</td>\n",
       "      <td>9.949400e+10</td>\n",
       "      <td>0.00037</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.036562</td>\n",
       "      <td>0.001659</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>989.407046</td>\n",
       "      <td>80.577841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>logarithmic</td>\n",
       "      <td>number_normalized</td>\n",
       "      <td>79.893173</td>\n",
       "      <td>78.894075</td>\n",
       "      <td>81.107207</td>\n",
       "      <td>117.244893</td>\n",
       "      <td>114.821925</td>\n",
       "      <td>119.943386</td>\n",
       "      <td>165.962791</td>\n",
       "      <td>157.226531</td>\n",
       "      <td>...</td>\n",
       "      <td>4.922460e+12</td>\n",
       "      <td>1.141483e+11</td>\n",
       "      <td>0.00037</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.036560</td>\n",
       "      <td>0.001674</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>988.577094</td>\n",
       "      <td>84.132768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>logarithmic</td>\n",
       "      <td>volume_per_mL</td>\n",
       "      <td>104.933955</td>\n",
       "      <td>103.946855</td>\n",
       "      <td>105.958702</td>\n",
       "      <td>150.771969</td>\n",
       "      <td>147.667036</td>\n",
       "      <td>154.117406</td>\n",
       "      <td>246.322597</td>\n",
       "      <td>219.565049</td>\n",
       "      <td>...</td>\n",
       "      <td>4.922460e+12</td>\n",
       "      <td>1.141483e+11</td>\n",
       "      <td>0.00037</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.036560</td>\n",
       "      <td>0.001674</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>988.577094</td>\n",
       "      <td>84.132768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>logarithmic</td>\n",
       "      <td>surface_area_per_mL</td>\n",
       "      <td>96.224558</td>\n",
       "      <td>95.004443</td>\n",
       "      <td>97.543132</td>\n",
       "      <td>137.848523</td>\n",
       "      <td>135.201294</td>\n",
       "      <td>140.901644</td>\n",
       "      <td>199.995709</td>\n",
       "      <td>185.973398</td>\n",
       "      <td>...</td>\n",
       "      <td>4.922460e+12</td>\n",
       "      <td>1.141483e+11</td>\n",
       "      <td>0.00037</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.036560</td>\n",
       "      <td>0.001674</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>988.577094</td>\n",
       "      <td>84.132768</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows √ó 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         scale         distribution         D10   D10_lower   D10_upper  \\\n",
       "0       linear    number_normalized   78.978723   77.947254   80.285095   \n",
       "1       linear        volume_per_mL  104.301224  103.169434  105.521429   \n",
       "2       linear  surface_area_per_mL   95.567250   94.286978   97.222066   \n",
       "3  logarithmic    number_normalized   79.893173   78.894075   81.107207   \n",
       "4  logarithmic        volume_per_mL  104.933955  103.946855  105.958702   \n",
       "5  logarithmic  surface_area_per_mL   96.224558   95.004443   97.543132   \n",
       "\n",
       "          D50   D50_lower   D50_upper         D90   D90_lower  ...  \\\n",
       "0  116.798780  114.225104  119.793059  166.653846  157.243913  ...   \n",
       "1  150.933352  147.646185  154.318915  245.413208  220.592249  ...   \n",
       "2  137.877915  135.038201  140.980536  200.608232  187.055296  ...   \n",
       "3  117.244893  114.821925  119.943386  165.962791  157.226531  ...   \n",
       "4  150.771969  147.667036  154.117406  246.322597  219.565049  ...   \n",
       "5  137.848523  135.201294  140.901644  199.995709  185.973398  ...   \n",
       "\n",
       "   total_particles_per_mL  total_particles_sd  total_volume_uL_per_mL  \\\n",
       "0            4.922490e+12        9.949400e+10                 0.00037   \n",
       "1            4.922490e+12        9.949400e+10                 0.00037   \n",
       "2            4.922490e+12        9.949400e+10                 0.00037   \n",
       "3            4.922460e+12        1.141483e+11                 0.00037   \n",
       "4            4.922460e+12        1.141483e+11                 0.00037   \n",
       "5            4.922460e+12        1.141483e+11                 0.00037   \n",
       "\n",
       "   total_volume_sd  total_surface_area_cm2_per_mL  total_surface_area_sd  \\\n",
       "0         0.000025                       0.036562               0.001659   \n",
       "1         0.000025                       0.036562               0.001659   \n",
       "2         0.000025                       0.036562               0.001659   \n",
       "3         0.000027                       0.036560               0.001674   \n",
       "4         0.000027                       0.036560               0.001674   \n",
       "5         0.000027                       0.036560               0.001674   \n",
       "\n",
       "   volume_percentage  volume_percentage_sd  specific_surface_area_m2_per_cm3  \\\n",
       "0           0.000037              0.000002                        989.407046   \n",
       "1           0.000037              0.000002                        989.407046   \n",
       "2           0.000037              0.000002                        989.407046   \n",
       "3           0.000037              0.000003                        988.577094   \n",
       "4           0.000037              0.000003                        988.577094   \n",
       "5           0.000037              0.000003                        988.577094   \n",
       "\n",
       "   specific_surface_area_sd  \n",
       "0                 80.577841  \n",
       "1                 80.577841  \n",
       "2                 80.577841  \n",
       "3                 84.132768  \n",
       "4                 84.132768  \n",
       "5                 84.132768  \n",
       "\n",
       "[6 rows x 24 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Saved comprehensive statistics table to: /Users/hboehm/Seafile/LEAF/NTA_data/!Inbox/20250527/processed/Stats_20250527_0004_23052025_SUV_5136664_after6weeks_size_488_avg3_comprehensive.txt\n",
      "\n",
      "Adding key D-values to metadata...\n",
      "‚úì Updated metadata with key D-values: /Users/hboehm/Seafile/LEAF/NTA_data/!Inbox/20250527/metadata/Data_20250527_0004_23052025_SUV_5136664_after6weeks_size_488_avg3_metadata.txt\n",
      "\n",
      "================================================================================\n",
      "STATISTICS CALCULATION COMPLETED SUCCESSFULLY!\n",
      "================================================================================\n",
      "Statistics with uncertainties stored in: current_stats\n",
      "Comprehensive statistics table stored in: current_stats_table\n",
      "Key D-values added to metadata.\n",
      "Ready for visualization and further analysis.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "NTA Data Analysis - Statistics Module with Uncertainty Propagation (Cell 06)\n",
    "\n",
    "This module calculates key statistics (D10, D50, D90, span) from\n",
    "pre-calculated cumulative distributions with proper uncertainty propagation.\n",
    "Uses bounds-based approach: interpolate cumsum¬±SD to get D-value uncertainties.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "def interpolate_d_value_with_bounds(sizes, cumsum_avg, cumsum_sd, target_fraction):\n",
    "    \"\"\"\n",
    "    Calculate D-value with asymmetric confidence bounds using bounds approach.\n",
    "    \n",
    "    Parameters:\n",
    "    sizes (array): Size values (nm)\n",
    "    cumsum_avg (array): Average cumulative distribution values\n",
    "    cumsum_sd (array): Standard deviation of cumulative distribution values\n",
    "    target_fraction (float): Target fraction (e.g., 0.1 for D10, 0.5 for D50)\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (d_value_avg, d_value_lower, d_value_upper)\n",
    "    \"\"\"\n",
    "    # Ensure arrays are sorted by size\n",
    "    sorted_indices = np.argsort(sizes)\n",
    "    sizes_sorted = sizes[sorted_indices]\n",
    "    cumsum_avg_sorted = cumsum_avg[sorted_indices]\n",
    "    cumsum_sd_sorted = cumsum_sd[sorted_indices]\n",
    "    \n",
    "    # Calculate bounds\n",
    "    cumsum_lower = cumsum_avg_sorted - cumsum_sd_sorted\n",
    "    cumsum_upper = cumsum_avg_sorted + cumsum_sd_sorted\n",
    "    \n",
    "    # Ensure cumulative distributions are monotonic and in valid range [0,1]\n",
    "    cumsum_avg_sorted = np.clip(cumsum_avg_sorted, 0, 1)\n",
    "    cumsum_lower = np.clip(cumsum_lower, 0, 1)\n",
    "    cumsum_upper = np.clip(cumsum_upper, 0, 1)\n",
    "    \n",
    "    # Ensure monotonicity by taking cumulative maximum\n",
    "    cumsum_avg_sorted = np.maximum.accumulate(cumsum_avg_sorted)\n",
    "    cumsum_lower = np.maximum.accumulate(cumsum_lower)\n",
    "    cumsum_upper = np.maximum.accumulate(cumsum_upper)\n",
    "    \n",
    "    # Check if target fraction is achievable\n",
    "    if target_fraction < cumsum_avg_sorted[0] or target_fraction > cumsum_avg_sorted[-1]:\n",
    "        return np.nan, np.nan, np.nan\n",
    "    \n",
    "    # Interpolate D-values\n",
    "    try:\n",
    "        d_value_avg = np.interp(target_fraction, cumsum_avg_sorted, sizes_sorted)\n",
    "        d_value_lower = np.interp(target_fraction, cumsum_upper, sizes_sorted)  # Note: swapped!\n",
    "        d_value_upper = np.interp(target_fraction, cumsum_lower, sizes_sorted)  # Note: swapped!\n",
    "        \n",
    "        # The swapping is because:\n",
    "        # - When cumsum is higher (cumsum + sd), we reach target fraction at smaller size ‚Üí lower bound\n",
    "        # - When cumsum is lower (cumsum - sd), we reach target fraction at larger size ‚Üí upper bound\n",
    "        \n",
    "        return d_value_avg, d_value_lower, d_value_upper\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in interpolation for target fraction {target_fraction}: {e}\")\n",
    "        return np.nan, np.nan, np.nan\n",
    "\n",
    "\n",
    "def calculate_percentile_statistics_with_uncertainty(df, size_column='size_nm'):\n",
    "    \"\"\"\n",
    "    Calculate percentile statistics (D10, D50, D90, span) with uncertainties for all\n",
    "    cumulative distributions (number, volume, surface area) and scales.\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): DataFrame containing size and cumulative distribution data with uncertainties\n",
    "    size_column (str): Column name for particle sizes\n",
    "    \n",
    "    Returns:\n",
    "    dict: Nested dictionary of statistics by scale, distribution type, and metric\n",
    "    \"\"\"\n",
    "    # Initialize results dictionary\n",
    "    stats = {'linear': {}, 'logarithmic': {}}\n",
    "    \n",
    "    # Define cumulative distribution configurations\n",
    "    cumsum_configs = [\n",
    "        {\n",
    "            'name': 'number',\n",
    "            'avg_column': 'number_normalized_cumsum_avg',\n",
    "            'sd_column': 'number_normalized_cumsum_sd'\n",
    "        },\n",
    "        {\n",
    "            'name': 'volume',\n",
    "            'avg_column': 'volume_nm^3_per_mL_cumsum_avg', \n",
    "            'sd_column': 'volume_nm^3_per_mL_cumsum_sd'\n",
    "        },\n",
    "        {\n",
    "            'name': 'surface_area',\n",
    "            'avg_column': 'area_nm^2_per_mL_cumsum_avg',\n",
    "            'sd_column': 'area_nm^2_per_mL_cumsum_sd'\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Process each scale\n",
    "    for scale in ['linear', 'logarithmic']:\n",
    "        print(f\"\\nCalculating statistics for {scale.upper()} scale:\")\n",
    "        \n",
    "        # Filter data for this scale\n",
    "        scale_df = df[df['scale'] == scale].copy()\n",
    "        if scale_df.empty:\n",
    "            print(f\"  No data available for {scale} scale\")\n",
    "            continue\n",
    "            \n",
    "        # Sort data by size\n",
    "        scale_df = scale_df.sort_values(size_column)\n",
    "        \n",
    "        # Process each cumulative distribution type\n",
    "        for config in cumsum_configs:\n",
    "            name = config['name']\n",
    "            avg_column = config['avg_column']\n",
    "            sd_column = config['sd_column']\n",
    "            \n",
    "            print(f\"  Processing {name}-weighted distribution...\")\n",
    "            \n",
    "            # Skip if required columns don't exist\n",
    "            if avg_column not in scale_df.columns or sd_column not in scale_df.columns:\n",
    "                print(f\"    Skipping - missing columns: {avg_column} or {sd_column}\")\n",
    "                continue\n",
    "            \n",
    "            # Extract data arrays\n",
    "            sizes = scale_df[size_column].values\n",
    "            cumsum_avg = scale_df[avg_column].values\n",
    "            cumsum_sd = scale_df[sd_column].values\n",
    "            \n",
    "            # Skip if all values are zero or NaN\n",
    "            if np.all(cumsum_avg == 0) or np.all(np.isnan(cumsum_avg)):\n",
    "                print(f\"    Skipping - all cumsum values are zero or NaN\")\n",
    "                continue\n",
    "            \n",
    "            # For absolute distributions (volume, surface area), normalize to 0-1 for D-value calculation\n",
    "            # We want to know \"at what size do we have X% of the total volume/surface area\"\n",
    "            if name in ['volume', 'surface_area']:\n",
    "                max_cumsum = np.nanmax(cumsum_avg)\n",
    "                if max_cumsum > 0:\n",
    "                    cumsum_avg = cumsum_avg / max_cumsum\n",
    "                    cumsum_sd = cumsum_sd / max_cumsum\n",
    "                else:\n",
    "                    print(f\"    Skipping - maximum cumsum is zero\")\n",
    "                    continue\n",
    "            \n",
    "            # Calculate D10, D50, D90 with asymmetric bounds\n",
    "            try:\n",
    "                d10_avg, d10_lower, d10_upper = interpolate_d_value_with_bounds(sizes, cumsum_avg, cumsum_sd, 0.1)\n",
    "                d50_avg, d50_lower, d50_upper = interpolate_d_value_with_bounds(sizes, cumsum_avg, cumsum_sd, 0.5)\n",
    "                d90_avg, d90_lower, d90_upper = interpolate_d_value_with_bounds(sizes, cumsum_avg, cumsum_sd, 0.9)\n",
    "                \n",
    "                # Calculate span with bounds\n",
    "                # span = (D90 - D10) / D50\n",
    "                if not np.isnan(d10_avg) and not np.isnan(d50_avg) and not np.isnan(d90_avg) and d50_avg > 0:\n",
    "                    span_avg = (d90_avg - d10_avg) / d50_avg\n",
    "                    \n",
    "                    # For span bounds, we need to consider which combinations give min/max\n",
    "                    # span_lower = min of all possible combinations\n",
    "                    # span_upper = max of all possible combinations\n",
    "                    possible_spans = [\n",
    "                        (d90_lower - d10_upper) / d50_upper,  # All values that make span smaller\n",
    "                        (d90_lower - d10_upper) / d50_lower,\n",
    "                        (d90_upper - d10_lower) / d50_upper,\n",
    "                        (d90_upper - d10_lower) / d50_lower   # All values that make span larger\n",
    "                    ]\n",
    "                    \n",
    "                    # Filter out invalid spans (negative denominators)\n",
    "                    valid_spans = [s for s in possible_spans if not np.isnan(s) and np.isfinite(s)]\n",
    "                    \n",
    "                    if valid_spans:\n",
    "                        span_lower = min(valid_spans)\n",
    "                        span_upper = max(valid_spans)\n",
    "                    else:\n",
    "                        span_lower, span_upper = np.nan, np.nan\n",
    "                else:\n",
    "                    span_avg, span_lower, span_upper = np.nan, np.nan, np.nan\n",
    "                \n",
    "                # Store results with bounds\n",
    "                metrics = {\n",
    "                    'D10_avg': d10_avg,\n",
    "                    'D10_lower': d10_lower,\n",
    "                    'D10_upper': d10_upper,\n",
    "                    'D50_avg': d50_avg,\n",
    "                    'D50_lower': d50_lower,\n",
    "                    'D50_upper': d50_upper,\n",
    "                    'D90_avg': d90_avg,\n",
    "                    'D90_lower': d90_lower,\n",
    "                    'D90_upper': d90_upper,\n",
    "                    'span_avg': span_avg,\n",
    "                    'span_lower': span_lower,\n",
    "                    'span_upper': span_upper\n",
    "                }\n",
    "                \n",
    "                stats[scale][name] = metrics\n",
    "                \n",
    "                print(f\"    ‚úì D10: {d10_avg:.2f} nm ({d10_lower:.2f} - {d10_upper:.2f})\")\n",
    "                print(f\"    ‚úì D50: {d50_avg:.2f} nm ({d50_lower:.2f} - {d50_upper:.2f})\")\n",
    "                print(f\"    ‚úì D90: {d90_avg:.2f} nm ({d90_lower:.2f} - {d90_upper:.2f})\")\n",
    "                print(f\"    ‚úì Span: {span_avg:.3f} ({span_lower:.3f} - {span_upper:.3f})\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"    Error calculating statistics for {name}: {str(e)}\")\n",
    "    \n",
    "    return stats\n",
    "\n",
    "\n",
    "def create_comprehensive_statistics_table(stats, total_metrics):\n",
    "    \"\"\"\n",
    "    Create a comprehensive wide-format table with percentile statistics and total metrics.\n",
    "    \n",
    "    Parameters:\n",
    "    stats (dict): Dictionary of calculated percentile statistics\n",
    "    total_metrics (dict): Dictionary of total metrics from cell05.2\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: Wide-format table with all statistics\n",
    "    \"\"\"\n",
    "    # Initialize list to store table rows\n",
    "    table_data = []\n",
    "    \n",
    "    # Process each scale\n",
    "    for scale in ['linear', 'logarithmic']:\n",
    "        if scale not in stats or not stats[scale]:\n",
    "            continue\n",
    "            \n",
    "        # Get total metrics for this scale\n",
    "        scale_totals = total_metrics.get(scale, {})\n",
    "        \n",
    "        # Process each distribution type\n",
    "        for dist_type in ['number', 'volume', 'surface_area']:\n",
    "            if dist_type not in stats[scale]:\n",
    "                continue\n",
    "                \n",
    "            metrics = stats[scale][dist_type]\n",
    "            \n",
    "            # Prepare distribution name for table\n",
    "            if dist_type == 'number':\n",
    "                dist_name = 'number_normalized'\n",
    "            elif dist_type == 'volume':\n",
    "                dist_name = 'volume_per_mL' \n",
    "            else:\n",
    "                dist_name = 'surface_area_per_mL'\n",
    "            \n",
    "            # Create row data\n",
    "            row = {\n",
    "                'scale': scale,\n",
    "                'distribution': dist_name,\n",
    "                'D10': metrics.get('D10_avg', np.nan),\n",
    "                'D10_lower': metrics.get('D10_lower', np.nan),\n",
    "                'D10_upper': metrics.get('D10_upper', np.nan),\n",
    "                'D50': metrics.get('D50_avg', np.nan),\n",
    "                'D50_lower': metrics.get('D50_lower', np.nan),\n",
    "                'D50_upper': metrics.get('D50_upper', np.nan),\n",
    "                'D90': metrics.get('D90_avg', np.nan),\n",
    "                'D90_lower': metrics.get('D90_lower', np.nan),\n",
    "                'D90_upper': metrics.get('D90_upper', np.nan),\n",
    "                'span': metrics.get('span_avg', np.nan),\n",
    "                'span_lower': metrics.get('span_lower', np.nan),\n",
    "                'span_upper': metrics.get('span_upper', np.nan)\n",
    "            }\n",
    "            \n",
    "            # Add total metrics (same for all distribution types within a scale)\n",
    "            row.update({\n",
    "                'total_particles_per_mL': scale_totals.get('total_particles_per_mL_avg', np.nan),\n",
    "                'total_particles_sd': scale_totals.get('total_particles_per_mL_sd', np.nan),\n",
    "                'total_volume_uL_per_mL': scale_totals.get('total_volume_uL_per_mL_avg', np.nan),\n",
    "                'total_volume_sd': scale_totals.get('total_volume_uL_per_mL_sd', np.nan),\n",
    "                'total_surface_area_cm2_per_mL': scale_totals.get('total_surface_area_cm^2_per_mL_avg', np.nan),\n",
    "                'total_surface_area_sd': scale_totals.get('total_surface_area_cm^2_per_mL_sd', np.nan),\n",
    "                'volume_percentage': scale_totals.get('volume_percentage_avg', np.nan),\n",
    "                'volume_percentage_sd': scale_totals.get('volume_percentage_sd', np.nan)\n",
    "            })\n",
    "            \n",
    "            # Calculate specific surface area with uncertainty\n",
    "            total_area = scale_totals.get('total_surface_area_cm^2_per_mL_avg', np.nan)\n",
    "            total_area_sd = scale_totals.get('total_surface_area_cm^2_per_mL_sd', np.nan)\n",
    "            total_volume = scale_totals.get('total_volume_uL_per_mL_avg', np.nan)  # in ¬µL/mL\n",
    "            total_volume_sd = scale_totals.get('total_volume_uL_per_mL_sd', np.nan)\n",
    "            \n",
    "            if not np.isnan(total_area) and not np.isnan(total_volume) and total_volume > 0:\n",
    "                # Convert volume from ¬µL/mL to cm¬≥/mL: 1 ¬µL = 1e-3 cm¬≥\n",
    "                volume_cm3_per_mL = total_volume * 1e-3\n",
    "                volume_cm3_per_mL_sd = total_volume_sd * 1e-3\n",
    "                \n",
    "                # Specific surface area = area (cm¬≤/mL) / volume (cm¬≥/mL) = cm‚Åª¬π\n",
    "                # Convert to m¬≤/cm¬≥: 1 cm‚Åª¬π = 100 m¬≤/cm¬≥\n",
    "                ssa_avg = (total_area / volume_cm3_per_mL) / 100  # Convert to m¬≤/cm¬≥\n",
    "                \n",
    "                # Uncertainty propagation for ratio: œÉ_R = R √ó ‚àö((œÉ_A/A)¬≤ + (œÉ_V/V)¬≤)\n",
    "                if not np.isnan(total_area_sd) and not np.isnan(volume_cm3_per_mL_sd):\n",
    "                    rel_area_error = total_area_sd / total_area\n",
    "                    rel_volume_error = volume_cm3_per_mL_sd / volume_cm3_per_mL\n",
    "                    ssa_sd = ssa_avg * np.sqrt(rel_area_error**2 + rel_volume_error**2)\n",
    "                else:\n",
    "                    ssa_sd = np.nan\n",
    "                \n",
    "                row['specific_surface_area_m2_per_cm3'] = ssa_avg\n",
    "                row['specific_surface_area_sd'] = ssa_sd\n",
    "            else:\n",
    "                row['specific_surface_area_m2_per_cm3'] = np.nan\n",
    "                row['specific_surface_area_sd'] = np.nan\n",
    "            \n",
    "            table_data.append(row)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    stats_table = pd.DataFrame(table_data)\n",
    "    \n",
    "    return stats_table\n",
    "\n",
    "\n",
    "def save_statistics_table(stats_table, uniqueID, output_dir=None, config=None):\n",
    "    \"\"\"\n",
    "    Save the comprehensive statistics table to a file.\n",
    "    \n",
    "    Parameters:\n",
    "    stats_table (DataFrame): Statistics table to save\n",
    "    uniqueID (str): Unique identifier for the dataset\n",
    "    output_dir (str): Directory to save the file (optional)\n",
    "    config (dict): Configuration dictionary (optional)\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (success_flag, filepath)\n",
    "    \"\"\"\n",
    "    # Determine output directory\n",
    "    if output_dir is None:\n",
    "        if config is not None and \"directory\" in config:\n",
    "            base_dir = config[\"directory\"]\n",
    "            output_dir = os.path.join(base_dir, \"processed\")\n",
    "        else:\n",
    "            output_dir = os.path.join(os.getcwd(), \"processed\")\n",
    "    \n",
    "    # Ensure output directory exists\n",
    "    try:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "    except Exception as e:\n",
    "        return False, f\"Failed to create output directory: {str(e)}\"\n",
    "    \n",
    "    # Create filepath\n",
    "    stats_path = os.path.join(output_dir, f\"Stats_{uniqueID}_comprehensive.txt\")\n",
    "    \n",
    "    try:\n",
    "        # Save as tab-separated file\n",
    "        stats_table.to_csv(stats_path, sep='\\t', index=False)\n",
    "        return True, stats_path\n",
    "    except Exception as e:\n",
    "        return False, f\"Failed to save statistics table: {str(e)}\"\n",
    "\n",
    "\n",
    "def add_key_statistics_to_metadata(metadata, stats):\n",
    "    \"\"\"\n",
    "    Add key D-values (linear number-weighted D10, D50, D90) to metadata.\n",
    "    \n",
    "    Parameters:\n",
    "    metadata (dict): Current metadata dictionary\n",
    "    stats (dict): Dictionary with calculated statistics\n",
    "    \n",
    "    Returns:\n",
    "    dict: Updated metadata dictionary\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying the original\n",
    "    updated_metadata = metadata.copy()\n",
    "    \n",
    "    # Get linear number-weighted statistics\n",
    "    if ('linear' in stats and \n",
    "        'number' in stats['linear'] and \n",
    "        stats['linear']['number']):\n",
    "        \n",
    "        linear_number_stats = stats['linear']['number']\n",
    "        \n",
    "        # Add D-values with bounds to metadata\n",
    "        for param in ['D10', 'D50', 'D90']:\n",
    "            avg_val = linear_number_stats.get(f'{param}_avg', np.nan)\n",
    "            lower_val = linear_number_stats.get(f'{param}_lower', np.nan)\n",
    "            upper_val = linear_number_stats.get(f'{param}_upper', np.nan)\n",
    "            \n",
    "            if not np.isnan(avg_val):\n",
    "                updated_metadata[f'nta_linear_number_{param.lower()}'] = f\"{avg_val:.2f} nm ({lower_val:.2f} - {upper_val:.2f})\"\n",
    "            else:\n",
    "                updated_metadata[f'nta_linear_number_{param.lower()}'] = \"Not available\"\n",
    "        \n",
    "        # Add span with bounds\n",
    "        span_avg = linear_number_stats.get('span_avg', np.nan)\n",
    "        span_lower = linear_number_stats.get('span_lower', np.nan)\n",
    "        span_upper = linear_number_stats.get('span_upper', np.nan)\n",
    "        if not np.isnan(span_avg):\n",
    "            updated_metadata['nta_linear_number_span'] = f\"{span_avg:.3f} ({span_lower:.3f} - {span_upper:.3f})\"\n",
    "        else:\n",
    "            updated_metadata['nta_linear_number_span'] = \"Not available\"\n",
    "    \n",
    "    return updated_metadata\n",
    "\n",
    "\n",
    "# Execute statistics calculation if we have the required data\n",
    "if 'current_distribution_df' in globals() and current_distribution_df is not None:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"CALCULATING PARTICLE SIZE DISTRIBUTION STATISTICS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Check if we have the required cumulative distribution columns\n",
    "    required_columns = [\n",
    "        'number_normalized_cumsum_avg', 'number_normalized_cumsum_sd',\n",
    "        'volume_nm^3_per_mL_cumsum_avg', 'volume_nm^3_per_mL_cumsum_sd',\n",
    "        'area_nm^2_per_mL_cumsum_avg', 'area_nm^2_per_mL_cumsum_sd'\n",
    "    ]\n",
    "    \n",
    "    missing_columns = [col for col in required_columns if col not in current_distribution_df.columns]\n",
    "    \n",
    "    if missing_columns:\n",
    "        print(f\"ERROR: Missing required cumulative distribution columns:\")\n",
    "        for col in missing_columns:\n",
    "            print(f\"  - {col}\")\n",
    "        print(\"\\nPlease run the core calculations (cell05.2) first to generate cumulative distributions.\")\n",
    "    else:\n",
    "        # Calculate statistics with uncertainties\n",
    "        current_stats = calculate_percentile_statistics_with_uncertainty(current_distribution_df)\n",
    "        \n",
    "        # Create comprehensive statistics table\n",
    "        if 'current_total_metrics' in globals():\n",
    "            print(\"\\nCreating comprehensive statistics table...\")\n",
    "            stats_table = create_comprehensive_statistics_table(current_stats, current_total_metrics)\n",
    "            \n",
    "            print(\"Statistics Table:\")\n",
    "            print(\"=\" * 120)\n",
    "            display(stats_table)\n",
    "            \n",
    "            # Save the statistics table\n",
    "            uniqueID = current_metadata.get('persistentID', 'unknown') if 'current_metadata' in globals() else 'unknown'\n",
    "            success, stats_path = save_statistics_table(\n",
    "                stats_table, \n",
    "                uniqueID, \n",
    "                config=CONFIG if 'CONFIG' in globals() else None\n",
    "            )\n",
    "            \n",
    "            if success:\n",
    "                print(f\"\\n‚úì Saved comprehensive statistics table to: {stats_path}\")\n",
    "            else:\n",
    "                print(f\"\\n‚úó Failed to save statistics table: {stats_path}\")\n",
    "            \n",
    "            # Add key statistics to metadata\n",
    "            if 'current_metadata' in globals():\n",
    "                print(\"\\nAdding key D-values to metadata...\")\n",
    "                try:\n",
    "                    current_metadata = add_key_statistics_to_metadata(current_metadata, current_stats)\n",
    "                    \n",
    "                    # Save updated metadata file\n",
    "                    if 'CONFIG' in globals() and \"directory\" in CONFIG:\n",
    "                        metadata_dir = os.path.join(CONFIG[\"directory\"], \"metadata\")\n",
    "                        metadata_path = os.path.join(metadata_dir, f\"Data_{uniqueID}_metadata.txt\")\n",
    "                        \n",
    "                        # Read existing metadata to preserve all fields\n",
    "                        existing_metadata = {}\n",
    "                        if os.path.exists(metadata_path):\n",
    "                            with open(metadata_path, 'r') as f:\n",
    "                                for line in f:\n",
    "                                    parts = line.strip().split('\\t')\n",
    "                                    if len(parts) >= 2:\n",
    "                                        existing_metadata[parts[0]] = parts[1]\n",
    "                        \n",
    "                        # Update with our enhanced metadata\n",
    "                        existing_metadata.update(current_metadata)\n",
    "                        \n",
    "                        # Write back\n",
    "                        with open(metadata_path, 'w') as f:\n",
    "                            for key, value in existing_metadata.items():\n",
    "                                f.write(f\"{key}\\t{value}\\t\\n\")\n",
    "                        \n",
    "                        print(f\"‚úì Updated metadata with key D-values: {metadata_path}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"‚úó Failed to update metadata: {e}\")\n",
    "            \n",
    "            # Store the table for further use\n",
    "            current_stats_table = stats_table\n",
    "            \n",
    "        else:\n",
    "            print(\"\\nWarning: 'current_total_metrics' not found. Creating table with percentile statistics only.\")\n",
    "            # Could create a simplified table here if needed\n",
    "        \n",
    "        print(f\"\\n\" + \"=\" * 80)\n",
    "        print(\"STATISTICS CALCULATION COMPLETED SUCCESSFULLY!\")\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Statistics with uncertainties stored in: current_stats\")\n",
    "        print(\"Comprehensive statistics table stored in: current_stats_table\")\n",
    "        print(\"Key D-values added to metadata.\")\n",
    "        print(\"Ready for visualization and further analysis.\")\n",
    "\n",
    "else:\n",
    "    print(\"No distribution data found in 'current_distribution_df'.\")\n",
    "    print(\"Please run the complete workflow first:\")\n",
    "    print(\"  cell00_multipleFiles ‚Üí cell02_multipleFiles ‚Üí cell03_multipleFiles ‚Üí\")\n",
    "    print(\"  cell04_multipleFiles ‚Üí cell05 ‚Üí cell05.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96adf5d4-2a13-44ba-a80e-d981c7b3c055",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "GENERATING NUMBER-WEIGHTED DISTRIBUTION PLOTS (FIXED VERSION)\n",
      "================================================================================\n",
      "Creating number-weighted plots for: 20250527_0004_23052025_SUV_5136664_after6weeks_size_488_avg3\n",
      "Includes: Linear + Logarithmic (Lognormal fits for both)\n",
      "Creating linear number-weighted plot...\n",
      "  ‚úì Saved fit to: Fits_20250527_0004_23052025_SUV_5136664_after6weeks_size_488_avg3_all.json\n",
      "  ‚úì Saved: Plot_20250527_0004_23052025_SUV_5136664_after6weeks_size_488_avg3_linear_number.pdf/.png\n",
      "Creating log number-weighted plot...\n",
      "  ‚úì Saved fit to: Fits_20250527_0004_23052025_SUV_5136664_after6weeks_size_488_avg3_all.json\n",
      "  ‚úì Saved: Plot_20250527_0004_23052025_SUV_5136664_after6weeks_size_488_avg3_log_number.pdf/.png\n",
      "\n",
      "‚úì Successfully created 2 number-weighted plots!\n",
      "  - Plot_20250527_0004_23052025_SUV_5136664_after6weeks_size_488_avg3_linear_number.pdf\n",
      "  - Plot_20250527_0004_23052025_SUV_5136664_after6weeks_size_488_avg3_log_number.pdf\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "NTA Data Analysis - Number-Weighted Distribution Plots (Cell 10a)\n",
    "\n",
    "This module creates two-subplot plots for number-weighted distributions:\n",
    "- Linear scale with lognormal fits (displayed in linear space)\n",
    "- Logarithmic scale with lognormal fits (displayed in log space)\n",
    "\n",
    "Layout: 60% main distribution (top) + 40% cumulative (bottom)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.lines import Line2D\n",
    "from scipy import stats as scipy_stats\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "\n",
    "def lognormal_pdf(x, mu, sigma, amplitude):\n",
    "    \"\"\"Calculate lognormal probability density function.\"\"\"\n",
    "    return amplitude * (1 / (x * sigma * np.sqrt(2 * np.pi))) * np.exp(-(np.log(x) - mu)**2 / (2 * sigma**2))\n",
    "\n",
    "\n",
    "def fit_lognormal_distribution(sizes, weights):\n",
    "    \"\"\"Fit a lognormal distribution to size distribution data.\"\"\"\n",
    "    try:\n",
    "        # Initial parameter estimates\n",
    "        size_log = np.log(sizes)\n",
    "        initial_mu = np.average(size_log, weights=weights)\n",
    "        initial_sigma = np.sqrt(np.average((size_log - initial_mu)**2, weights=weights))\n",
    "        initial_amplitude = np.max(weights) * initial_sigma * np.sqrt(2 * np.pi) * np.exp(initial_mu)\n",
    "        \n",
    "        initial_params = [initial_mu, initial_sigma, initial_amplitude]\n",
    "        \n",
    "        # Perform curve fitting\n",
    "        params, _ = curve_fit(\n",
    "            lognormal_pdf, sizes, weights, p0=initial_params,\n",
    "            bounds=([0, 0, 0], [np.inf, np.inf, np.inf]), maxfev=10000\n",
    "        )\n",
    "        \n",
    "        # Generate fitted curve\n",
    "        size_range = np.linspace(sizes.min(), sizes.max(), 200)\n",
    "        fitted_curve = lognormal_pdf(size_range, *params)\n",
    "        \n",
    "        return True, (size_range, fitted_curve, params)\n",
    "        \n",
    "    except Exception as e:\n",
    "        return False, f\"Lognormal fit failed: {str(e)}\"\n",
    "\n",
    "\n",
    "def add_number_fit_curve_fixed(ax, plot_df, is_log_scale, fit_color='#F25C54'):\n",
    "    \"\"\"Add lognormal fits for number distributions (both linear and log scale).\"\"\"\n",
    "    fit_legend_elements = []\n",
    "    \n",
    "    sizes = plot_df['size_nm'].values\n",
    "    weights = plot_df['number_normalized_avg'].values\n",
    "    \n",
    "    # Remove any zero or negative weights\n",
    "    valid_mask = (weights > 0) & (sizes > 0)\n",
    "    if not np.any(valid_mask):\n",
    "        return fit_legend_elements, None\n",
    "    \n",
    "    sizes, weights = sizes[valid_mask], weights[valid_mask]\n",
    "    \n",
    "    # Use lognormal fit for both linear and log scales\n",
    "    success, result = fit_lognormal_distribution(sizes, weights)\n",
    "    if success:\n",
    "        size_range, fitted_curve, params = result\n",
    "        ax.plot(size_range, fitted_curve, '-', color=fit_color, linewidth=2.5, \n",
    "               alpha=0.9, label='Lognormal Fit', zorder=4)\n",
    "        \n",
    "        geometric_mean = np.exp(params[0])\n",
    "        geometric_std = np.exp(params[1])\n",
    "        \n",
    "        # Add fit info to legend (consistent for both scales)\n",
    "        fit_legend_elements.append(\n",
    "            Line2D([0], [0], color=fit_color, linestyle='-', linewidth=2.5,\n",
    "                  label=f'Lognormal: geo_mean={geometric_mean:.1f} nm, geo_std={geometric_std:.2f}')\n",
    "        )\n",
    "        \n",
    "        return fit_legend_elements, ('lognormal', {'mu': params[0], 'sigma': params[1], 'amplitude': params[2], \n",
    "                                                   'geo_mean': geometric_mean, 'geo_std': geometric_std})\n",
    "    else:\n",
    "        print(f\"    Lognormal fit failed: {result}\")\n",
    "        return fit_legend_elements, None\n",
    "\n",
    "\n",
    "def add_d_value_lines_and_bands(ax, stats):\n",
    "    \"\"\"Add D-value lines and uncertainty bands to a subplot.\"\"\"\n",
    "    legend_elements = []\n",
    "    \n",
    "    if not stats or 'D10_avg' not in stats:\n",
    "        return legend_elements\n",
    "    \n",
    "    d10_avg = stats['D10_avg']\n",
    "    d10_lower = stats.get('D10_lower', d10_avg)\n",
    "    d10_upper = stats.get('D10_upper', d10_avg)\n",
    "    \n",
    "    d50_avg = stats['D50_avg'] \n",
    "    d50_lower = stats.get('D50_lower', d50_avg)\n",
    "    d50_upper = stats.get('D50_upper', d50_avg)\n",
    "    \n",
    "    d90_avg = stats['D90_avg']\n",
    "    d90_lower = stats.get('D90_lower', d90_avg)\n",
    "    d90_upper = stats.get('D90_upper', d90_avg)\n",
    "    \n",
    "    span = stats.get('span_avg', (d90_avg-d10_avg)/d50_avg if d50_avg > 0 else 0)\n",
    "    \n",
    "    # Add D-value lines and bands\n",
    "    for d_val, d_lower, d_upper, style, width, alpha_band in [\n",
    "        (d10_avg, d10_lower, d10_upper, '--', 1.5, 0.15),\n",
    "        (d50_avg, d50_lower, d50_upper, '-', 2.5, 0.25), \n",
    "        (d90_avg, d90_lower, d90_upper, '--', 1.5, 0.15)\n",
    "    ]:\n",
    "        if not np.isnan(d_val):\n",
    "            ax.axvline(x=d_val, color='gray', linestyle=style, alpha=0.8, linewidth=width, zorder=5)\n",
    "            if not np.isnan(d_lower) and not np.isnan(d_upper) and (d_lower != d_val or d_upper != d_val):\n",
    "                ax.axvspan(d_lower, d_upper, alpha=alpha_band, color='gray', zorder=1)\n",
    "    \n",
    "    # Create legend elements\n",
    "    legend_elements.extend([\n",
    "        Line2D([0], [0], color='gray', linestyle='--', linewidth=1.5, \n",
    "              label=f'D10: {d10_avg:.1f} nm ({d10_lower:.1f}-{d10_upper:.1f})'),\n",
    "        Line2D([0], [0], color='gray', linestyle='-', linewidth=2.5, \n",
    "              label=f'D50: {d50_avg:.1f} nm ({d50_lower:.1f}-{d50_upper:.1f})'),\n",
    "        Line2D([0], [0], color='gray', linestyle='--', linewidth=1.5, \n",
    "              label=f'D90: {d90_avg:.1f} nm ({d90_lower:.1f}-{d90_upper:.1f})'),\n",
    "        Line2D([0], [0], color='white', linestyle='', \n",
    "              label=f'Span: {span:.3f}')\n",
    "    ])\n",
    "    \n",
    "    return legend_elements\n",
    "\n",
    "\n",
    "def create_number_plot(plot_df, is_log_scale, stats=None, uniqueID=None, metadata=None):\n",
    "    \"\"\"Create a two-subplot plot for number-weighted distribution.\"\"\"\n",
    "    \n",
    "    scale_name = \"Logarithmic\" if is_log_scale else \"Linear\"\n",
    "    xscale = 'log' if is_log_scale else 'linear'\n",
    "    color = '#4C5B5C'  # Slate gray for number-weighted data\n",
    "    \n",
    "    # Sort by size\n",
    "    plot_df = plot_df.sort_values('size_nm')\n",
    "    \n",
    "    # Create figure\n",
    "    fig = plt.figure(figsize=(7, 9))\n",
    "    gs = gridspec.GridSpec(2, 1, height_ratios=[0.6, 0.4], hspace=0.3, \n",
    "                          top=0.82, bottom=0.08)\n",
    "    \n",
    "    # =================================================================\n",
    "    # TOP SUBPLOT: MAIN DISTRIBUTION WITH ERROR BARS AND FITS\n",
    "    # =================================================================\n",
    "    ax1 = fig.add_subplot(gs[0])\n",
    "    \n",
    "    # Plot main distribution with error bars\n",
    "    if 'number_normalized_sd' in plot_df.columns:\n",
    "        ax1.errorbar(plot_df['size_nm'], plot_df['number_normalized_avg'], \n",
    "                    yerr=plot_df['number_normalized_sd'],\n",
    "                    fmt='o', color=color, ecolor=color, alpha=0.7,\n",
    "                    capsize=3, capthick=1, markersize=6, linewidth=1.5,\n",
    "                    label='Number Distribution')\n",
    "    else:\n",
    "        ax1.scatter(plot_df['size_nm'], plot_df['number_normalized_avg'], \n",
    "                   color=color, s=60, alpha=0.8, label='Number Distribution')\n",
    "    \n",
    "    # Add lognormal fit curve\n",
    "    fit_result = add_number_fit_curve_fixed(ax1, plot_df, is_log_scale)\n",
    "    if isinstance(fit_result, tuple):\n",
    "        fit_legend_elements, fit_results = fit_result\n",
    "    else:\n",
    "        fit_legend_elements = fit_result\n",
    "        fit_results = None\n",
    "    \n",
    "    # Format top subplot\n",
    "    ax1.set_ylabel('Normalized Number', color=color, fontsize=14, labelpad=10)\n",
    "    ax1.tick_params(axis='y', labelcolor=color, labelsize=12)\n",
    "    ax1.tick_params(axis='x', labelsize=12)\n",
    "    ax1.spines['left'].set_color(color)\n",
    "    \n",
    "    # Set x-axis scale and limits\n",
    "    ax1.set_xscale(xscale)\n",
    "    if is_log_scale:\n",
    "        # Log scale: start at 30 nm, focus on signal range\n",
    "        min_size = max(30, plot_df['size_nm'].min())\n",
    "        percentile_90 = np.percentile(plot_df['size_nm'], 90)\n",
    "        max_size = min(percentile_90 * 1.3, 300)\n",
    "        ax1.set_xlim([min_size, max_size])\n",
    "    else:\n",
    "        # Linear scale: start at 0, focus on main signal\n",
    "        percentile_85 = np.percentile(plot_df['size_nm'], 85)\n",
    "        max_size = min(percentile_85 * 1.2, 250)\n",
    "        ax1.set_xlim([0, max_size])\n",
    "    \n",
    "    # Set y-axis to start from 0\n",
    "    y_min, y_max = ax1.get_ylim()\n",
    "    ax1.set_ylim([0, y_max])\n",
    "    \n",
    "    # Add D-value lines and bands\n",
    "    d_legend_elements = add_d_value_lines_and_bands(ax1, stats)\n",
    "    \n",
    "    # Create comprehensive legend for top plot - PLACE OUTSIDE\n",
    "    main_legend = [Line2D([0], [0], marker='o', color='w', markerfacecolor=color, \n",
    "                          markersize=8, label='Number Distribution')]\n",
    "    \n",
    "    all_legend_elements = main_legend + fit_legend_elements + d_legend_elements\n",
    "    leg1 = ax1.legend(handles=all_legend_elements, fontsize=9, frameon=True, \n",
    "                     bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    leg1.get_frame().set_alpha(0.95)\n",
    "    leg1.get_frame().set_edgecolor('lightgray')\n",
    "    \n",
    "    ax1.grid(True, linestyle='--', alpha=0.4)\n",
    "    ax1.set_xlabel('')  # No x-label on top plot\n",
    "    \n",
    "    # =================================================================\n",
    "    # BOTTOM SUBPLOT: CUMULATIVE DISTRIBUTION WITH UNCERTAINTY BANDS\n",
    "    # =================================================================\n",
    "    ax2 = fig.add_subplot(gs[1])\n",
    "    \n",
    "    if 'number_normalized_cumsum_avg' in plot_df.columns:\n",
    "        # Plot cumulative curve as percentage\n",
    "        cumsum_values = plot_df['number_normalized_cumsum_avg'] * 100\n",
    "        ax2.plot(plot_df['size_nm'], cumsum_values, '-', \n",
    "                color=color, linewidth=3, alpha=0.9, label='Cumulative %')\n",
    "        \n",
    "        # Add uncertainty bands if available\n",
    "        if 'number_normalized_cumsum_sd' in plot_df.columns:\n",
    "            cumsum_sd = plot_df['number_normalized_cumsum_sd'] * 100\n",
    "            ax2.fill_between(plot_df['size_nm'], \n",
    "                           cumsum_values - cumsum_sd,\n",
    "                           cumsum_values + cumsum_sd,\n",
    "                           color=color, alpha=0.25, zorder=1, label='¬± SD')\n",
    "        \n",
    "        ax2.set_ylim([0, 110])\n",
    "        ax2.set_ylabel('Cumulative Percentage (%)', color=color, fontsize=14, labelpad=10)\n",
    "        ax2.tick_params(axis='y', labelcolor=color, labelsize=12)\n",
    "        ax2.spines['left'].set_color(color)\n",
    "    \n",
    "    # Format bottom subplot\n",
    "    ax2.set_xlabel('Size (nm)', fontsize=14, labelpad=10)\n",
    "    ax2.tick_params(axis='x', labelsize=12)\n",
    "    ax2.set_xscale(xscale)\n",
    "    ax2.set_xlim(ax1.get_xlim())  # Match top plot limits\n",
    "    \n",
    "    # Add D-value lines to bottom plot\n",
    "    if 'number_normalized_cumsum_avg' in plot_df.columns:\n",
    "        add_d_value_lines_and_bands(ax2, stats)\n",
    "    \n",
    "    # Legend for bottom plot - PLACE OUTSIDE\n",
    "    cumulative_legend = [\n",
    "        Line2D([0], [0], color=color, linewidth=3, label='Cumulative %'),\n",
    "        Line2D([0], [0], color=color, alpha=0.25, linewidth=8, label='¬± SD')\n",
    "    ]\n",
    "    \n",
    "    leg2 = ax2.legend(handles=cumulative_legend, fontsize=10, frameon=True, \n",
    "                     bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    leg2.get_frame().set_alpha(0.95)\n",
    "    leg2.get_frame().set_edgecolor('lightgray')\n",
    "    \n",
    "    ax2.grid(True, linestyle='--', alpha=0.4)\n",
    "    \n",
    "    # =================================================================\n",
    "    # TITLE AND METADATA\n",
    "    # =================================================================\n",
    "    \n",
    "    # Extract replicate info\n",
    "    replicate_info = \"\"\n",
    "    if metadata and 'num_replicates' in metadata:\n",
    "        num_reps = metadata['num_replicates']\n",
    "        if num_reps and str(num_reps) != '1':\n",
    "            replicate_info = f\" (n={num_reps})\"\n",
    "    \n",
    "    # Set main title\n",
    "    main_title = f'{scale_name} Number-Weighted\\nDistribution: {uniqueID}{replicate_info}'\n",
    "    fig.suptitle(main_title, fontsize=14, fontweight='bold', y=0.94)\n",
    "    \n",
    "    # Add subtitle\n",
    "    subtitle = f\"Error bars/bands: ¬± SD | Fits: Lognormal\"\n",
    "    fig.text(0.5, 0.87, subtitle, ha='center', fontsize=11, style='italic')\n",
    "    \n",
    "    return fig, fit_results\n",
    "\n",
    "\n",
    "def generate_number_plots(distribution_df, stats_dict=None, uniqueID=None, \n",
    "                         metadata=None, output_dir=None, config=None):\n",
    "    \"\"\"Generate number-weighted distribution plots for both linear and log scales.\"\"\"\n",
    "    \n",
    "    if distribution_df is None or distribution_df.empty:\n",
    "        return False, \"No data available for plotting\"\n",
    "    \n",
    "    plt.style.use('default')\n",
    "    \n",
    "    # Determine output directory\n",
    "    if output_dir is None:\n",
    "        if config is not None and \"directory\" in config:\n",
    "            base_dir = config[\"directory\"]\n",
    "            output_dir = os.path.join(base_dir, \"processed\")\n",
    "        else:\n",
    "            output_dir = os.path.join(os.getcwd(), \"processed\")\n",
    "    \n",
    "    try:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "    except Exception as e:\n",
    "        return False, f\"Failed to create output directory: {str(e)}\"\n",
    "    \n",
    "    created_files = []\n",
    "    \n",
    "    # Generate linear and logarithmic plots\n",
    "    for is_log_scale in [False, True]:\n",
    "        scale_type = 'logarithmic' if is_log_scale else 'linear'\n",
    "        scale_name = 'log' if is_log_scale else 'linear'\n",
    "        \n",
    "        print(f\"Creating {scale_name} number-weighted plot...\")\n",
    "        \n",
    "        # Filter data for this scale\n",
    "        plot_df = distribution_df[distribution_df['scale'] == scale_type].copy()\n",
    "        \n",
    "        if plot_df.empty:\n",
    "            print(f\"  Warning: No {scale_type} scale data available\")\n",
    "            continue\n",
    "        \n",
    "        # Get statistics\n",
    "        stats = None\n",
    "        if stats_dict and scale_type in stats_dict and 'number' in stats_dict[scale_type]:\n",
    "            stats = stats_dict[scale_type]['number']\n",
    "        \n",
    "        # Create the plot\n",
    "        fig, fit_results = create_number_plot(plot_df, is_log_scale, stats, uniqueID, metadata)\n",
    "        \n",
    "        if fig is None:\n",
    "            print(f\"  Failed to create plot\")\n",
    "            continue\n",
    "        \n",
    "        # Save fit results to comprehensive fits file\n",
    "        if fit_results:\n",
    "            fit_type, fit_data = fit_results\n",
    "            try:\n",
    "                import json\n",
    "                \n",
    "                # Load existing fits file or create new one\n",
    "                fits_filename = f\"Fits_{uniqueID}_all.json\"\n",
    "                fits_path = os.path.join(output_dir, fits_filename)\n",
    "                \n",
    "                if os.path.exists(fits_path):\n",
    "                    with open(fits_path, 'r') as f:\n",
    "                        all_fits = json.load(f)\n",
    "                else:\n",
    "                    all_fits = {'dataset': uniqueID, 'fits': {}}\n",
    "                \n",
    "                # Add this fit to the collection\n",
    "                fit_key = f\"number_{scale_name}\"\n",
    "                all_fits['fits'][fit_key] = {\n",
    "                    'distribution_type': 'number',\n",
    "                    'scale': scale_name,\n",
    "                    'fit_type': fit_type,\n",
    "                    'parameters': fit_data\n",
    "                }\n",
    "                \n",
    "                # Save updated fits file\n",
    "                with open(fits_path, 'w') as f:\n",
    "                    json.dump(all_fits, f, indent=2, default=str)\n",
    "                \n",
    "                print(f\"  ‚úì Saved fit to: {fits_filename}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  ‚ö† Failed to save fit: {str(e)}\")\n",
    "        \n",
    "        # Save the plot\n",
    "        try:\n",
    "            base_filename = f\"Plot_{uniqueID}_{scale_name}_number\"\n",
    "            \n",
    "            pdf_path = os.path.join(output_dir, f\"{base_filename}.pdf\")\n",
    "            fig.savefig(pdf_path, bbox_inches='tight', dpi=300)\n",
    "            \n",
    "            png_path = os.path.join(output_dir, f\"{base_filename}.png\")\n",
    "            fig.savefig(png_path, bbox_inches='tight', dpi=300)\n",
    "            \n",
    "            created_files.append(pdf_path)\n",
    "            print(f\"  ‚úì Saved: {base_filename}.pdf/.png\")\n",
    "            \n",
    "            plt.close(fig)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚úó Failed to save plot: {str(e)}\")\n",
    "            plt.close(fig)\n",
    "            continue\n",
    "    \n",
    "    return True, created_files\n",
    "\n",
    "\n",
    "# Execute if we have the required data\n",
    "if 'current_distribution_df' in globals() and current_distribution_df is not None:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"GENERATING NUMBER-WEIGHTED DISTRIBUTION PLOTS (FIXED VERSION)\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    uniqueID = current_metadata.get('persistentID', 'unknown') if 'current_metadata' in globals() else 'unknown'\n",
    "    stats = current_stats if 'current_stats' in globals() else None\n",
    "    metadata = current_metadata if 'current_metadata' in globals() else None\n",
    "    config = CONFIG if 'CONFIG' in globals() else None\n",
    "    \n",
    "    print(f\"Creating number-weighted plots for: {uniqueID}\")\n",
    "    print(\"Includes: Linear + Logarithmic (Lognormal fits for both)\")\n",
    "    \n",
    "    success, plot_files = generate_number_plots(\n",
    "        current_distribution_df,\n",
    "        stats_dict=stats,\n",
    "        uniqueID=uniqueID,\n",
    "        metadata=metadata,\n",
    "        config=config\n",
    "    )\n",
    "    \n",
    "    if not success:\n",
    "        print(f\"ERROR: {plot_files}\")\n",
    "    else:\n",
    "        print(f\"\\n‚úì Successfully created {len(plot_files)} number-weighted plots!\")\n",
    "        for filepath in plot_files:\n",
    "            print(f\"  - {os.path.basename(filepath)}\")\n",
    "        \n",
    "        current_number_plots = plot_files\n",
    "\n",
    "else:\n",
    "    print(\"No data found. Run the complete workflow first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d164de49-a5cd-4e0a-9006-a5b479f939d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "GENERATING VOLUME-WEIGHTED DISTRIBUTION PLOTS (FIXED VERSION)\n",
      "================================================================================\n",
      "Creating volume-weighted plots for: 20250527_0004_23052025_SUV_5136664_after6weeks_size_488_avg3\n",
      "Includes: Linear + Logarithmic (Lognormal fits for both)\n",
      "Creating linear volume-weighted plot...\n",
      "  ‚úì Saved fit to: Fits_20250527_0004_23052025_SUV_5136664_after6weeks_size_488_avg3_all.json\n",
      "  ‚úì Saved: Plot_20250527_0004_23052025_SUV_5136664_after6weeks_size_488_avg3_linear_volume.pdf/.png\n",
      "Creating log volume-weighted plot...\n",
      "  ‚úì Saved fit to: Fits_20250527_0004_23052025_SUV_5136664_after6weeks_size_488_avg3_all.json\n",
      "  ‚úì Saved: Plot_20250527_0004_23052025_SUV_5136664_after6weeks_size_488_avg3_log_volume.pdf/.png\n",
      "\n",
      "‚úì Successfully created 2 volume-weighted plots!\n",
      "  - Plot_20250527_0004_23052025_SUV_5136664_after6weeks_size_488_avg3_linear_volume.pdf\n",
      "  - Plot_20250527_0004_23052025_SUV_5136664_after6weeks_size_488_avg3_log_volume.pdf\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "NTA Data Analysis - Volume-Weighted Distribution Plots (Cell 10b) \n",
    "\n",
    "This module creates two-subplot plots for volume-weighted distributions:\n",
    "- Linear scale with direct GMM fits (no artificial datasets)\n",
    "- Logarithmic scale with lognormal distribution fits\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.lines import Line2D\n",
    "from scipy import stats as scipy_stats\n",
    "from scipy.optimize import curve_fit, minimize\n",
    "\n",
    "\n",
    "def lognormal_pdf(x, mu, sigma, amplitude):\n",
    "    \"\"\"Calculate lognormal probability density function.\"\"\"\n",
    "    return amplitude * (1 / (x * sigma * np.sqrt(2 * np.pi))) * np.exp(-(np.log(x) - mu)**2 / (2 * sigma**2))\n",
    "\n",
    "\n",
    "def fit_lognormal_distribution(sizes, weights):\n",
    "    \"\"\"Fit a lognormal distribution to size distribution data.\"\"\"\n",
    "    try:\n",
    "        # Initial parameter estimates\n",
    "        size_log = np.log(sizes)\n",
    "        initial_mu = np.average(size_log, weights=weights)\n",
    "        initial_sigma = np.sqrt(np.average((size_log - initial_mu)**2, weights=weights))\n",
    "        initial_amplitude = np.max(weights) * initial_sigma * np.sqrt(2 * np.pi) * np.exp(initial_mu)\n",
    "        \n",
    "        initial_params = [initial_mu, initial_sigma, initial_amplitude]\n",
    "        \n",
    "        # Perform curve fitting\n",
    "        params, _ = curve_fit(\n",
    "            lognormal_pdf, sizes, weights, p0=initial_params,\n",
    "            bounds=([0, 0, 0], [np.inf, np.inf, np.inf]), maxfev=10000\n",
    "        )\n",
    "        \n",
    "        # Generate fitted curve\n",
    "        size_range = np.linspace(sizes.min(), sizes.max(), 200)\n",
    "        fitted_curve = lognormal_pdf(size_range, *params)\n",
    "        \n",
    "        return True, (size_range, fitted_curve, params)\n",
    "        \n",
    "    except Exception as e:\n",
    "        return False, f\"Lognormal fit failed: {str(e)}\"\n",
    "\n",
    "\n",
    "def fit_gaussian_mixture_direct(sizes, volumes, n_components_range=range(1, 4)):\n",
    "    \"\"\"\n",
    "    Fit GMM directly to the distribution data without creating artificial datasets.\n",
    "    \n",
    "    Parameters:\n",
    "    sizes (array): Size values (nm)\n",
    "    volumes (array): Volume values (nm¬≥/mL) \n",
    "    n_components_range: Range of component numbers to try\n",
    "    \n",
    "    Returns:\n",
    "    success, (size_range, total_fit, individual_components, model_info)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Remove zeros and sort by size\n",
    "        valid_mask = (volumes > 0) & (sizes > 0)\n",
    "        if not np.any(valid_mask):\n",
    "            return False, \"No valid data points\"\n",
    "        \n",
    "        sizes_clean = sizes[valid_mask]\n",
    "        volumes_clean = volumes[valid_mask]\n",
    "        \n",
    "        # Sort by size for consistency\n",
    "        sort_idx = np.argsort(sizes_clean)\n",
    "        sizes_clean = sizes_clean[sort_idx]\n",
    "        volumes_clean = volumes_clean[sort_idx]\n",
    "        \n",
    "        if len(sizes_clean) < 6:  # Need minimum data points\n",
    "            return False, \"Insufficient data points for GMM\"\n",
    "        \n",
    "        best_model = None\n",
    "        best_aic = np.inf\n",
    "        \n",
    "        # Try different numbers of components\n",
    "        for n_comp in n_components_range:\n",
    "            try:\n",
    "                # Initial parameter guess\n",
    "                size_min, size_max = sizes_clean.min(), sizes_clean.max()\n",
    "                initial_means = np.linspace(size_min + 0.1*(size_max-size_min), \n",
    "                                          size_max - 0.1*(size_max-size_min), n_comp)\n",
    "                initial_stds = np.full(n_comp, (size_max - size_min) / (n_comp * 3))\n",
    "                initial_weights = np.full(n_comp, 1.0 / n_comp)\n",
    "                \n",
    "                # Pack parameters: [means, stds, weights[:-1]]\n",
    "                initial_params = np.concatenate([initial_means, initial_stds, initial_weights[:-1]])\n",
    "                \n",
    "                def unpack_params(params, n_comp):\n",
    "                    means = params[:n_comp]\n",
    "                    stds = params[n_comp:2*n_comp]\n",
    "                    weights = np.zeros(n_comp)\n",
    "                    weights[:-1] = params[2*n_comp:]\n",
    "                    weights[-1] = 1.0 - np.sum(weights[:-1])  # Ensure sum = 1\n",
    "                    return means, stds, weights\n",
    "                \n",
    "                def gmm_objective(params):\n",
    "                    try:\n",
    "                        means, stds, weights = unpack_params(params, n_comp)\n",
    "                        \n",
    "                        # Check constraints\n",
    "                        if np.any(stds <= 0) or np.any(weights <= 0) or weights[-1] <= 0:\n",
    "                            return 1e10\n",
    "                        \n",
    "                        # Calculate predicted volumes using GMM\n",
    "                        predicted = np.zeros_like(sizes_clean)\n",
    "                        for i in range(n_comp):\n",
    "                            predicted += weights[i] * scipy_stats.norm.pdf(sizes_clean, means[i], stds[i])\n",
    "                        \n",
    "                        # Scale to match data magnitude\n",
    "                        if np.max(predicted) > 0:\n",
    "                            scale_factor = np.sum(volumes_clean) / np.sum(predicted)\n",
    "                            predicted *= scale_factor\n",
    "                        \n",
    "                        # Calculate squared error\n",
    "                        mse = np.mean((volumes_clean - predicted) ** 2)\n",
    "                        return mse\n",
    "                        \n",
    "                    except:\n",
    "                        return 1e10\n",
    "                \n",
    "                # Set bounds\n",
    "                bounds = []\n",
    "                # Bounds for means\n",
    "                for _ in range(n_comp):\n",
    "                    bounds.append((size_min, size_max))\n",
    "                # Bounds for stds\n",
    "                for _ in range(n_comp):\n",
    "                    bounds.append((1.0, size_max - size_min))\n",
    "                # Bounds for weights\n",
    "                for _ in range(n_comp-1):\n",
    "                    bounds.append((0.01, 0.98))\n",
    "                \n",
    "                # Optimize\n",
    "                result = minimize(gmm_objective, initial_params, method='L-BFGS-B', \n",
    "                                bounds=bounds, options={'maxiter': 1000})\n",
    "                \n",
    "                if result.success:\n",
    "                    final_means, final_stds, final_weights = unpack_params(result.x, n_comp)\n",
    "                    \n",
    "                    # Calculate final predicted curve\n",
    "                    predicted_final = np.zeros_like(sizes_clean)\n",
    "                    for i in range(n_comp):\n",
    "                        predicted_final += final_weights[i] * scipy_stats.norm.pdf(sizes_clean, final_means[i], final_stds[i])\n",
    "                    \n",
    "                    scale_factor = np.sum(volumes_clean) / np.sum(predicted_final) if np.sum(predicted_final) > 0 else 1.0\n",
    "                    \n",
    "                    # Calculate AIC\n",
    "                    mse = np.mean((volumes_clean - predicted_final * scale_factor) ** 2)\n",
    "                    n_params = 3 * n_comp - 1\n",
    "                    aic = len(sizes_clean) * np.log(mse) + 2 * n_params\n",
    "                    \n",
    "                    if aic < best_aic:\n",
    "                        best_aic = aic\n",
    "                        best_model = {\n",
    "                            'n_components': n_comp,\n",
    "                            'means': final_means,\n",
    "                            'stds': final_stds,\n",
    "                            'weights': final_weights,\n",
    "                            'scale_factor': scale_factor,\n",
    "                            'aic': aic,\n",
    "                            'mse': mse\n",
    "                        }\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"    Failed to fit {n_comp} components: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if best_model is None:\n",
    "            return False, \"No successful fits\"\n",
    "        \n",
    "        # Generate smooth curves for plotting\n",
    "        size_range = np.linspace(sizes_clean.min(), sizes_clean.max(), 200)\n",
    "        \n",
    "        # Calculate total fitted curve and individual components\n",
    "        total_fit = np.zeros_like(size_range)\n",
    "        individual_components = []\n",
    "        \n",
    "        for i in range(best_model['n_components']):\n",
    "            # Individual component\n",
    "            component = (best_model['weights'][i] * \n",
    "                        scipy_stats.norm.pdf(size_range, best_model['means'][i], best_model['stds'][i]) *\n",
    "                        best_model['scale_factor'])\n",
    "            individual_components.append(component)\n",
    "            total_fit += component\n",
    "        \n",
    "        return True, (size_range, total_fit, individual_components, best_model)\n",
    "        \n",
    "    except Exception as e:\n",
    "        return False, f\"GMM fitting failed: {str(e)}\"\n",
    "\n",
    "\n",
    "def add_volume_fit_curve_fixed(ax, plot_df, is_log_scale, fit_color='#F25C54'):\n",
    "    \"\"\"Add lognormal fits for volume distributions (both linear and log scale).\"\"\"\n",
    "    fit_legend_elements = []\n",
    "    \n",
    "    sizes = plot_df['size_nm'].values\n",
    "    volumes = plot_df['volume_nm^3_per_mL_avg'].values\n",
    "    \n",
    "    # Remove any zero or negative values\n",
    "    valid_mask = (volumes > 0) & (sizes > 0)\n",
    "    if not np.any(valid_mask):\n",
    "        return fit_legend_elements, None\n",
    "    \n",
    "    sizes, volumes = sizes[valid_mask], volumes[valid_mask]\n",
    "    \n",
    "    # Use lognormal fit for both linear and log scales\n",
    "    success, result = fit_lognormal_distribution(sizes, volumes)\n",
    "    if success:\n",
    "        size_range, fitted_curve, params = result\n",
    "        ax.plot(size_range, fitted_curve, '-', color=fit_color, linewidth=2.5, \n",
    "               alpha=0.9, label='Lognormal Fit', zorder=4)\n",
    "        \n",
    "        geometric_mean = np.exp(params[0])\n",
    "        geometric_std = np.exp(params[1])\n",
    "        fit_legend_elements.append(\n",
    "            Line2D([0], [0], color=fit_color, linestyle='-', linewidth=2.5,\n",
    "                  label=f'Lognormal: geo_mean={geometric_mean:.1f} nm, geo_std={geometric_std:.2f}')\n",
    "        )\n",
    "        \n",
    "        return fit_legend_elements, ('lognormal', {'mu': params[0], 'sigma': params[1], 'amplitude': params[2]})\n",
    "    else:\n",
    "        print(f\"    Lognormal fit failed: {result}\")\n",
    "        return fit_legend_elements, None\n",
    "\n",
    "\n",
    "def add_d_value_lines_and_bands(ax, stats):\n",
    "    \"\"\"Add D-value lines and uncertainty bands to a subplot.\"\"\"\n",
    "    legend_elements = []\n",
    "    \n",
    "    if not stats or 'D10_avg' not in stats:\n",
    "        return legend_elements\n",
    "    \n",
    "    d10_avg = stats['D10_avg']\n",
    "    d10_lower = stats.get('D10_lower', d10_avg)\n",
    "    d10_upper = stats.get('D10_upper', d10_avg)\n",
    "    \n",
    "    d50_avg = stats['D50_avg'] \n",
    "    d50_lower = stats.get('D50_lower', d50_avg)\n",
    "    d50_upper = stats.get('D50_upper', d50_avg)\n",
    "    \n",
    "    d90_avg = stats['D90_avg']\n",
    "    d90_lower = stats.get('D90_lower', d90_avg)\n",
    "    d90_upper = stats.get('D90_upper', d90_avg)\n",
    "    \n",
    "    span = stats.get('span_avg', (d90_avg-d10_avg)/d50_avg if d50_avg > 0 else 0)\n",
    "    \n",
    "    # Add D-value lines and bands\n",
    "    for d_val, d_lower, d_upper, style, width, alpha_band in [\n",
    "        (d10_avg, d10_lower, d10_upper, '--', 1.5, 0.15),\n",
    "        (d50_avg, d50_lower, d50_upper, '-', 2.5, 0.25), \n",
    "        (d90_avg, d90_lower, d90_upper, '--', 1.5, 0.15)\n",
    "    ]:\n",
    "        if not np.isnan(d_val):\n",
    "            ax.axvline(x=d_val, color='gray', linestyle=style, alpha=0.8, linewidth=width, zorder=5)\n",
    "            if not np.isnan(d_lower) and not np.isnan(d_upper) and (d_lower != d_val or d_upper != d_val):\n",
    "                ax.axvspan(d_lower, d_upper, alpha=alpha_band, color='gray', zorder=1)\n",
    "    \n",
    "    # Create legend elements\n",
    "    legend_elements.extend([\n",
    "        Line2D([0], [0], color='gray', linestyle='--', linewidth=1.5, \n",
    "              label=f'D10: {d10_avg:.1f} nm ({d10_lower:.1f}-{d10_upper:.1f})'),\n",
    "        Line2D([0], [0], color='gray', linestyle='-', linewidth=2.5, \n",
    "              label=f'D50: {d50_avg:.1f} nm ({d50_lower:.1f}-{d50_upper:.1f})'),\n",
    "        Line2D([0], [0], color='gray', linestyle='--', linewidth=1.5, \n",
    "              label=f'D90: {d90_avg:.1f} nm ({d90_lower:.1f}-{d90_upper:.1f})'),\n",
    "        Line2D([0], [0], color='white', linestyle='', \n",
    "              label=f'Span: {span:.3f}')\n",
    "    ])\n",
    "    \n",
    "    return legend_elements\n",
    "\n",
    "\n",
    "def create_volume_plot(plot_df, is_log_scale, stats=None, uniqueID=None, metadata=None):\n",
    "    \"\"\"Create a two-subplot plot for volume-weighted distribution.\"\"\"\n",
    "    \n",
    "    scale_name = \"Logarithmic\" if is_log_scale else \"Linear\"\n",
    "    xscale = 'log' if is_log_scale else 'linear'\n",
    "    color = '#2E7D32'  # Forest green for volume-weighted\n",
    "    \n",
    "    # Sort by size\n",
    "    plot_df = plot_df.sort_values('size_nm')\n",
    "    \n",
    "    # Create figure\n",
    "    fig = plt.figure(figsize=(7, 9))\n",
    "    gs = gridspec.GridSpec(2, 1, height_ratios=[0.6, 0.4], hspace=0.3, \n",
    "                          top=0.82, bottom=0.08)\n",
    "    \n",
    "    # TOP SUBPLOT: MAIN DISTRIBUTION WITH ERROR BARS AND FITS\n",
    "    ax1 = fig.add_subplot(gs[0])\n",
    "    \n",
    "    # Plot main distribution with error bars\n",
    "    if 'volume_nm^3_per_mL_sd' in plot_df.columns:\n",
    "        ax1.errorbar(plot_df['size_nm'], plot_df['volume_nm^3_per_mL_avg'], \n",
    "                    yerr=plot_df['volume_nm^3_per_mL_sd'],\n",
    "                    fmt='o', color=color, ecolor=color, alpha=0.7,\n",
    "                    capsize=3, capthick=1, markersize=6, linewidth=1.5,\n",
    "                    label='Volume Distribution')\n",
    "    else:\n",
    "        ax1.scatter(plot_df['size_nm'], plot_df['volume_nm^3_per_mL_avg'], \n",
    "                   color=color, s=60, alpha=0.8, label='Volume Distribution')\n",
    "    \n",
    "    # Add fit curve and get fit results\n",
    "    fit_result = add_volume_fit_curve_fixed(ax1, plot_df, is_log_scale)\n",
    "    if isinstance(fit_result, tuple):\n",
    "        fit_legend_elements, fit_results = fit_result\n",
    "    else:\n",
    "        fit_legend_elements = fit_result\n",
    "        fit_results = None\n",
    "    \n",
    "    # Format top subplot\n",
    "    ax1.set_ylabel('Volume (nm¬≥/mL)', color=color, fontsize=14, labelpad=10)\n",
    "    ax1.tick_params(axis='y', labelcolor=color, labelsize=12)\n",
    "    ax1.tick_params(axis='x', labelsize=12)\n",
    "    ax1.spines['left'].set_color(color)\n",
    "    \n",
    "    # Set x-axis scale and add better tick labels for log scale\n",
    "    ax1.set_xscale(xscale)\n",
    "    if is_log_scale:\n",
    "        # Add more detailed log scale ticks\n",
    "        from matplotlib.ticker import LogLocator, LogFormatter\n",
    "        ax1.xaxis.set_major_locator(LogLocator(base=10, numticks=12))\n",
    "        ax1.xaxis.set_minor_locator(LogLocator(base=10, subs=(0.2, 0.4, 0.6, 0.8), numticks=12))\n",
    "        ax1.xaxis.set_major_formatter(LogFormatter(base=10, labelOnlyBase=False))\n",
    "    \n",
    "    # Smart volume-weighted range calculation (restored from original)\n",
    "    weights_for_range = plot_df['volume_nm^3_per_mL_avg'].values\n",
    "    sizes_for_range = plot_df['size_nm'].values\n",
    "    \n",
    "    # Find where 99% of the volume signal is contained\n",
    "    cumsum_weights = np.cumsum(weights_for_range)\n",
    "    total_weight = cumsum_weights[-1]\n",
    "    \n",
    "    # Find 1st and 99th percentiles of the volume-weighted distribution\n",
    "    p1_idx = np.searchsorted(cumsum_weights, 0.01 * total_weight)\n",
    "    p99_idx = np.searchsorted(cumsum_weights, 0.99 * total_weight)\n",
    "    \n",
    "    signal_min = sizes_for_range[max(0, p1_idx)]\n",
    "    signal_max = sizes_for_range[min(len(sizes_for_range)-1, p99_idx)]\n",
    "    data_max = plot_df['size_nm'].max()\n",
    "    \n",
    "    if is_log_scale:\n",
    "        # Log scale: focus on the volume-weighted signal range with some padding\n",
    "        min_size = max(signal_min * 0.7, 20)  # Don't go below 20 nm\n",
    "        max_size = min(signal_max * 2.0, data_max * 1.2)  # Cap at reasonable range\n",
    "        ax1.set_xlim([min_size, max_size])\n",
    "    else:\n",
    "        # Linear scale: tighten the range more, focus on main signal\n",
    "        min_size = 0\n",
    "        max_size = min(signal_max * 1.2, 400)  # Tighter range, cap at 400nm for most cases\n",
    "        ax1.set_xlim([min_size, max_size])\n",
    "    \n",
    "    # Set y-axis to start from 0\n",
    "    y_min, y_max = ax1.get_ylim()\n",
    "    ax1.set_ylim([0, y_max])\n",
    "    \n",
    "    # Add D-value lines and bands\n",
    "    d_legend_elements = add_d_value_lines_and_bands(ax1, stats)\n",
    "    \n",
    "    # Create comprehensive legend for top plot - PLACE OUTSIDE\n",
    "    main_legend = [Line2D([0], [0], marker='o', color='w', markerfacecolor=color, \n",
    "                          markersize=8, label='Volume Distribution')]\n",
    "    \n",
    "    all_legend_elements = main_legend + fit_legend_elements + d_legend_elements\n",
    "    leg1 = ax1.legend(handles=all_legend_elements, fontsize=9, frameon=True, \n",
    "                     bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    leg1.get_frame().set_alpha(0.95)\n",
    "    leg1.get_frame().set_edgecolor('lightgray')\n",
    "    \n",
    "    ax1.grid(True, linestyle='--', alpha=0.4)\n",
    "    ax1.set_xlabel('')\n",
    "    \n",
    "    # BOTTOM SUBPLOT: CUMULATIVE DISTRIBUTION\n",
    "    ax2 = fig.add_subplot(gs[1])\n",
    "    \n",
    "    if 'volume_nm^3_per_mL_cumsum_avg' in plot_df.columns:\n",
    "        cumsum_values = plot_df['volume_nm^3_per_mL_cumsum_avg']\n",
    "        max_cumsum = np.max(cumsum_values)\n",
    "        if max_cumsum > 0:\n",
    "            cumsum_percentage = (cumsum_values / max_cumsum) * 100\n",
    "            ax2.plot(plot_df['size_nm'], cumsum_percentage, '-', \n",
    "                    color=color, linewidth=3, alpha=0.9, label='Cumulative %')\n",
    "            \n",
    "            if 'volume_nm^3_per_mL_cumsum_sd' in plot_df.columns:\n",
    "                cumsum_sd = plot_df['volume_nm^3_per_mL_cumsum_sd']\n",
    "                cumsum_sd_percentage = (cumsum_sd / max_cumsum) * 100\n",
    "                ax2.fill_between(plot_df['size_nm'], \n",
    "                               cumsum_percentage - cumsum_sd_percentage,\n",
    "                               cumsum_percentage + cumsum_sd_percentage,\n",
    "                               color=color, alpha=0.25, zorder=1, label='¬± SD')\n",
    "        \n",
    "        ax2.set_ylim([0, 110])\n",
    "        ax2.set_ylabel('Cumulative Percentage (%)', color=color, fontsize=14, labelpad=10)\n",
    "        ax2.tick_params(axis='y', labelcolor=color, labelsize=12)\n",
    "        ax2.spines['left'].set_color(color)\n",
    "    \n",
    "    # Format bottom subplot\n",
    "    ax2.set_xlabel('Size (nm)', fontsize=14, labelpad=10)\n",
    "    ax2.tick_params(axis='x', labelsize=12)\n",
    "    ax2.set_xscale(xscale)\n",
    "    ax2.set_xlim(ax1.get_xlim())\n",
    "    \n",
    "    # Add D-value lines to bottom plot\n",
    "    if 'volume_nm^3_per_mL_cumsum_avg' in plot_df.columns:\n",
    "        add_d_value_lines_and_bands(ax2, stats)\n",
    "    \n",
    "    # Legend for bottom plot\n",
    "    cumulative_legend = [\n",
    "        Line2D([0], [0], color=color, linewidth=3, label='Cumulative %'),\n",
    "        Line2D([0], [0], color=color, alpha=0.25, linewidth=8, label='¬± SD')\n",
    "    ]\n",
    "    \n",
    "    leg2 = ax2.legend(handles=cumulative_legend, fontsize=10, frameon=True, \n",
    "                     bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    leg2.get_frame().set_alpha(0.95)\n",
    "    leg2.get_frame().set_edgecolor('lightgray')\n",
    "    \n",
    "    ax2.grid(True, linestyle='--', alpha=0.4)\n",
    "    \n",
    "    # TITLE AND METADATA\n",
    "    replicate_info = \"\"\n",
    "    if metadata and 'num_replicates' in metadata:\n",
    "        num_reps = metadata['num_replicates']\n",
    "        if num_reps and str(num_reps) != '1':\n",
    "            replicate_info = f\" (n={num_reps})\"\n",
    "    \n",
    "    main_title = f'{scale_name} Volume-Weighted\\nDistribution: {uniqueID}{replicate_info}'\n",
    "    fig.suptitle(main_title, fontsize=14, fontweight='bold', y=0.94)\n",
    "    \n",
    "    subtitle = f\"Error bars/bands: ¬± SD | Fits: Lognormal\"\n",
    "    fig.text(0.5, 0.87, subtitle, ha='center', fontsize=11, style='italic')\n",
    "    \n",
    "    return fig, fit_results\n",
    "\n",
    "\n",
    "def generate_volume_plots(distribution_df, stats_dict=None, uniqueID=None, \n",
    "                         metadata=None, output_dir=None, config=None):\n",
    "    \"\"\"Generate volume-weighted distribution plots for both linear and log scales.\"\"\"\n",
    "    \n",
    "    if distribution_df is None or distribution_df.empty:\n",
    "        return False, \"No data available for plotting\"\n",
    "    \n",
    "    plt.style.use('default')\n",
    "    \n",
    "    # Determine output directory\n",
    "    if output_dir is None:\n",
    "        if config is not None and \"directory\" in config:\n",
    "            base_dir = config[\"directory\"]\n",
    "            output_dir = os.path.join(base_dir, \"processed\")\n",
    "        else:\n",
    "            output_dir = os.path.join(os.getcwd(), \"processed\")\n",
    "    \n",
    "    try:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "    except Exception as e:\n",
    "        return False, f\"Failed to create output directory: {str(e)}\"\n",
    "    \n",
    "    created_files = []\n",
    "    \n",
    "    # Generate linear and logarithmic plots\n",
    "    for is_log_scale in [False, True]:\n",
    "        scale_type = 'logarithmic' if is_log_scale else 'linear'\n",
    "        scale_name = 'log' if is_log_scale else 'linear'\n",
    "        \n",
    "        print(f\"Creating {scale_name} volume-weighted plot...\")\n",
    "        \n",
    "        # Filter data for this scale\n",
    "        plot_df = distribution_df[distribution_df['scale'] == scale_type].copy()\n",
    "        \n",
    "        if plot_df.empty:\n",
    "            print(f\"  Warning: No {scale_type} scale data available\")\n",
    "            continue\n",
    "        \n",
    "        # Get statistics\n",
    "        stats = None\n",
    "        if stats_dict and scale_type in stats_dict and 'volume' in stats_dict[scale_type]:\n",
    "            stats = stats_dict[scale_type]['volume']\n",
    "        \n",
    "        # Create the plot\n",
    "        fig, fit_results = create_volume_plot(plot_df, is_log_scale, stats, uniqueID, metadata)\n",
    "        \n",
    "        if fig is None:\n",
    "            print(f\"  Failed to create plot\")\n",
    "            continue\n",
    "        \n",
    "        # Save fit results to comprehensive fits file\n",
    "        if fit_results:\n",
    "            fit_type, fit_data = fit_results\n",
    "            try:\n",
    "                import json\n",
    "                \n",
    "                # Load existing fits file or create new one\n",
    "                fits_filename = f\"Fits_{uniqueID}_all.json\"\n",
    "                fits_path = os.path.join(output_dir, fits_filename)\n",
    "                \n",
    "                if os.path.exists(fits_path):\n",
    "                    with open(fits_path, 'r') as f:\n",
    "                        all_fits = json.load(f)\n",
    "                else:\n",
    "                    all_fits = {'dataset': uniqueID, 'fits': {}}\n",
    "                \n",
    "                # Add this fit to the collection\n",
    "                fit_key = f\"volume_{scale_name}\"\n",
    "                all_fits['fits'][fit_key] = {\n",
    "                    'distribution_type': 'volume',\n",
    "                    'scale': scale_name,\n",
    "                    'fit_type': fit_type,\n",
    "                    'parameters': fit_data\n",
    "                }\n",
    "                \n",
    "                # Save updated fits file\n",
    "                with open(fits_path, 'w') as f:\n",
    "                    json.dump(all_fits, f, indent=2, default=str)\n",
    "                \n",
    "                print(f\"  ‚úì Saved fit to: {fits_filename}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  ‚ö† Failed to save fit: {str(e)}\")\n",
    "        \n",
    "        # Save the plot\n",
    "        try:\n",
    "            base_filename = f\"Plot_{uniqueID}_{scale_name}_volume\"\n",
    "            \n",
    "            pdf_path = os.path.join(output_dir, f\"{base_filename}.pdf\")\n",
    "            fig.savefig(pdf_path, bbox_inches='tight', dpi=300)\n",
    "            \n",
    "            png_path = os.path.join(output_dir, f\"{base_filename}.png\")\n",
    "            fig.savefig(png_path, bbox_inches='tight', dpi=300)\n",
    "            \n",
    "            created_files.append(pdf_path)\n",
    "            print(f\"  ‚úì Saved: {base_filename}.pdf/.png\")\n",
    "            \n",
    "            plt.close(fig)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚úó Failed to save plot: {str(e)}\")\n",
    "            plt.close(fig)\n",
    "            continue\n",
    "    \n",
    "    return True, created_files\n",
    "\n",
    "\n",
    "# Execute if we have the required data\n",
    "if 'current_distribution_df' in globals() and current_distribution_df is not None:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"GENERATING VOLUME-WEIGHTED DISTRIBUTION PLOTS (FIXED VERSION)\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    uniqueID = current_metadata.get('persistentID', 'unknown') if 'current_metadata' in globals() else 'unknown'\n",
    "    stats = current_stats if 'current_stats' in globals() else None\n",
    "    metadata = current_metadata if 'current_metadata' in globals() else None\n",
    "    config = CONFIG if 'CONFIG' in globals() else None\n",
    "    \n",
    "    print(f\"Creating volume-weighted plots for: {uniqueID}\")\n",
    "    print(\"Includes: Linear + Logarithmic (Lognormal fits for both)\")\n",
    "    \n",
    "    success, plot_files = generate_volume_plots(\n",
    "        current_distribution_df,\n",
    "        stats_dict=stats,\n",
    "        uniqueID=uniqueID,\n",
    "        metadata=metadata,\n",
    "        config=config\n",
    "    )\n",
    "    \n",
    "    if not success:\n",
    "        print(f\"ERROR: {plot_files}\")\n",
    "    else:\n",
    "        print(f\"\\n‚úì Successfully created {len(plot_files)} volume-weighted plots!\")\n",
    "        for filepath in plot_files:\n",
    "            print(f\"  - {os.path.basename(filepath)}\")\n",
    "        \n",
    "        current_volume_plots = plot_files\n",
    "\n",
    "else:\n",
    "    print(\"No data found. Run the complete workflow first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85099e83-1edc-47a8-b89d-a4635c9fb43c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "GENERATING SURFACE AREA-WEIGHTED DISTRIBUTION PLOTS\n",
      "================================================================================\n",
      "Creating surface area-weighted plots for: 20250527_0004_23052025_SUV_5136664_after6weeks_size_488_avg3\n",
      "Includes: Linear + Logarithmic (Lognormal fits for both)\n",
      "Creating linear surface area-weighted plot...\n",
      "  ‚úì Saved fit to: Fits_20250527_0004_23052025_SUV_5136664_after6weeks_size_488_avg3_all.json\n",
      "  ‚úì Saved: Plot_20250527_0004_23052025_SUV_5136664_after6weeks_size_488_avg3_linear_surface_area.pdf/.png\n",
      "Creating log surface area-weighted plot...\n",
      "  ‚úì Saved fit to: Fits_20250527_0004_23052025_SUV_5136664_after6weeks_size_488_avg3_all.json\n",
      "  ‚úì Saved: Plot_20250527_0004_23052025_SUV_5136664_after6weeks_size_488_avg3_log_surface_area.pdf/.png\n",
      "\n",
      "‚úì Successfully created 2 surface area-weighted plots!\n",
      "  - Plot_20250527_0004_23052025_SUV_5136664_after6weeks_size_488_avg3_linear_surface_area.pdf\n",
      "  - Plot_20250527_0004_23052025_SUV_5136664_after6weeks_size_488_avg3_log_surface_area.pdf\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "NTA Data Analysis - Surface Area-Weighted Distribution Plots (Cell 10c)\n",
    "\n",
    "This module creates two-subplot plots for surface area-weighted distributions:\n",
    "- Linear scale with lognormal fits (displayed in linear space)\n",
    "- Logarithmic scale with lognormal fits (displayed in log space)\n",
    "\n",
    "Layout: 60% main distribution (top) + 40% cumulative (bottom)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.lines import Line2D\n",
    "from scipy import stats as scipy_stats\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "\n",
    "def lognormal_pdf(x, mu, sigma, amplitude):\n",
    "    \"\"\"Calculate lognormal probability density function.\"\"\"\n",
    "    return amplitude * (1 / (x * sigma * np.sqrt(2 * np.pi))) * np.exp(-(np.log(x) - mu)**2 / (2 * sigma**2))\n",
    "\n",
    "\n",
    "def fit_lognormal_distribution(sizes, weights):\n",
    "    \"\"\"Fit a lognormal distribution to size distribution data.\"\"\"\n",
    "    try:\n",
    "        # Initial parameter estimates\n",
    "        size_log = np.log(sizes)\n",
    "        initial_mu = np.average(size_log, weights=weights)\n",
    "        initial_sigma = np.sqrt(np.average((size_log - initial_mu)**2, weights=weights))\n",
    "        initial_amplitude = np.max(weights) * initial_sigma * np.sqrt(2 * np.pi) * np.exp(initial_mu)\n",
    "        \n",
    "        initial_params = [initial_mu, initial_sigma, initial_amplitude]\n",
    "        \n",
    "        # Perform curve fitting\n",
    "        params, _ = curve_fit(\n",
    "            lognormal_pdf, sizes, weights, p0=initial_params,\n",
    "            bounds=([0, 0, 0], [np.inf, np.inf, np.inf]), maxfev=10000\n",
    "        )\n",
    "        \n",
    "        # Generate fitted curve\n",
    "        size_range = np.linspace(sizes.min(), sizes.max(), 200)\n",
    "        fitted_curve = lognormal_pdf(size_range, *params)\n",
    "        \n",
    "        return True, (size_range, fitted_curve, params)\n",
    "        \n",
    "    except Exception as e:\n",
    "        return False, f\"Lognormal fit failed: {str(e)}\"\n",
    "\n",
    "\n",
    "def add_surface_area_fit_curve(ax, plot_df, is_log_scale, fit_color='#F25C54'):\n",
    "    \"\"\"Add lognormal fits for surface area distributions (both linear and log scale).\"\"\"\n",
    "    fit_legend_elements = []\n",
    "    \n",
    "    sizes = plot_df['size_nm'].values\n",
    "    surface_areas = plot_df['area_nm^2_per_mL_avg'].values\n",
    "    \n",
    "    # Remove any zero or negative values\n",
    "    valid_mask = (surface_areas > 0) & (sizes > 0)\n",
    "    if not np.any(valid_mask):\n",
    "        return fit_legend_elements, None\n",
    "    \n",
    "    sizes, surface_areas = sizes[valid_mask], surface_areas[valid_mask]\n",
    "    \n",
    "    # Use lognormal fit for both linear and log scales\n",
    "    success, result = fit_lognormal_distribution(sizes, surface_areas)\n",
    "    if success:\n",
    "        size_range, fitted_curve, params = result\n",
    "        ax.plot(size_range, fitted_curve, '-', color=fit_color, linewidth=2.5, \n",
    "               alpha=0.9, label='Lognormal Fit', zorder=4)\n",
    "        \n",
    "        geometric_mean = np.exp(params[0])\n",
    "        geometric_std = np.exp(params[1])\n",
    "        \n",
    "        # Add fit info to legend (consistent for both scales)\n",
    "        fit_legend_elements.append(\n",
    "            Line2D([0], [0], color=fit_color, linestyle='-', linewidth=2.5,\n",
    "                  label=f'Lognormal: geo_mean={geometric_mean:.1f} nm, geo_std={geometric_std:.2f}')\n",
    "        )\n",
    "        \n",
    "        return fit_legend_elements, ('lognormal', {'mu': params[0], 'sigma': params[1], 'amplitude': params[2]})\n",
    "    else:\n",
    "        print(f\"    Lognormal fit failed: {result}\")\n",
    "        return fit_legend_elements, None\n",
    "\n",
    "\n",
    "def add_d_value_lines_and_bands(ax, stats):\n",
    "    \"\"\"Add D-value lines and uncertainty bands to a subplot.\"\"\"\n",
    "    legend_elements = []\n",
    "    \n",
    "    if not stats or 'D10_avg' not in stats:\n",
    "        return legend_elements\n",
    "    \n",
    "    d10_avg = stats['D10_avg']\n",
    "    d10_lower = stats.get('D10_lower', d10_avg)\n",
    "    d10_upper = stats.get('D10_upper', d10_avg)\n",
    "    \n",
    "    d50_avg = stats['D50_avg'] \n",
    "    d50_lower = stats.get('D50_lower', d50_avg)\n",
    "    d50_upper = stats.get('D50_upper', d50_avg)\n",
    "    \n",
    "    d90_avg = stats['D90_avg']\n",
    "    d90_lower = stats.get('D90_lower', d90_avg)\n",
    "    d90_upper = stats.get('D90_upper', d90_avg)\n",
    "    \n",
    "    span = stats.get('span_avg', (d90_avg-d10_avg)/d50_avg if d50_avg > 0 else 0)\n",
    "    \n",
    "    # Add D-value lines and bands\n",
    "    for d_val, d_lower, d_upper, style, width, alpha_band in [\n",
    "        (d10_avg, d10_lower, d10_upper, '--', 1.5, 0.15),\n",
    "        (d50_avg, d50_lower, d50_upper, '-', 2.5, 0.25), \n",
    "        (d90_avg, d90_lower, d90_upper, '--', 1.5, 0.15)\n",
    "    ]:\n",
    "        if not np.isnan(d_val):\n",
    "            ax.axvline(x=d_val, color='gray', linestyle=style, alpha=0.8, linewidth=width, zorder=5)\n",
    "            if not np.isnan(d_lower) and not np.isnan(d_upper) and (d_lower != d_val or d_upper != d_val):\n",
    "                ax.axvspan(d_lower, d_upper, alpha=alpha_band, color='gray', zorder=1)\n",
    "    \n",
    "    # Create legend elements\n",
    "    legend_elements.extend([\n",
    "        Line2D([0], [0], color='gray', linestyle='--', linewidth=1.5, \n",
    "              label=f'D10: {d10_avg:.1f} nm ({d10_lower:.1f}-{d10_upper:.1f})'),\n",
    "        Line2D([0], [0], color='gray', linestyle='-', linewidth=2.5, \n",
    "              label=f'D50: {d50_avg:.1f} nm ({d50_lower:.1f}-{d50_upper:.1f})'),\n",
    "        Line2D([0], [0], color='gray', linestyle='--', linewidth=1.5, \n",
    "              label=f'D90: {d90_avg:.1f} nm ({d90_lower:.1f}-{d90_upper:.1f})'),\n",
    "        Line2D([0], [0], color='white', linestyle='', \n",
    "              label=f'Span: {span:.3f}')\n",
    "    ])\n",
    "    \n",
    "    return legend_elements\n",
    "\n",
    "\n",
    "def create_surface_area_plot(plot_df, is_log_scale, stats=None, uniqueID=None, metadata=None):\n",
    "    \"\"\"Create a two-subplot plot for surface area-weighted distribution.\"\"\"\n",
    "    \n",
    "    scale_name = \"Logarithmic\" if is_log_scale else \"Linear\"\n",
    "    xscale = 'log' if is_log_scale else 'linear'\n",
    "    color = '#4059AD'  # Indigo blue for surface area-weighted\n",
    "    \n",
    "    # Sort by size\n",
    "    plot_df = plot_df.sort_values('size_nm')\n",
    "    \n",
    "    # Create figure\n",
    "    fig = plt.figure(figsize=(7, 9))\n",
    "    gs = gridspec.GridSpec(2, 1, height_ratios=[0.6, 0.4], hspace=0.3, \n",
    "                          top=0.82, bottom=0.08)\n",
    "    \n",
    "    # =================================================================\n",
    "    # TOP SUBPLOT: MAIN DISTRIBUTION WITH ERROR BARS AND FITS\n",
    "    # =================================================================\n",
    "    ax1 = fig.add_subplot(gs[0])\n",
    "    \n",
    "    # Plot main distribution with error bars\n",
    "    if 'area_nm^2_per_mL_sd' in plot_df.columns:\n",
    "        ax1.errorbar(plot_df['size_nm'], plot_df['area_nm^2_per_mL_avg'], \n",
    "                    yerr=plot_df['area_nm^2_per_mL_sd'],\n",
    "                    fmt='o', color=color, ecolor=color, alpha=0.7,\n",
    "                    capsize=3, capthick=1, markersize=6, linewidth=1.5,\n",
    "                    label='Surface Area Distribution')\n",
    "    else:\n",
    "        ax1.scatter(plot_df['size_nm'], plot_df['area_nm^2_per_mL_avg'], \n",
    "                   color=color, s=60, alpha=0.8, label='Surface Area Distribution')\n",
    "    \n",
    "    # Add lognormal fit curve\n",
    "    fit_result = add_surface_area_fit_curve(ax1, plot_df, is_log_scale)\n",
    "    if isinstance(fit_result, tuple):\n",
    "        fit_legend_elements, fit_results = fit_result\n",
    "    else:\n",
    "        fit_legend_elements = fit_result\n",
    "        fit_results = None\n",
    "    \n",
    "    # Format top subplot\n",
    "    ax1.set_ylabel('Surface Area (nm¬≤/mL)', color=color, fontsize=14, labelpad=10)\n",
    "    ax1.tick_params(axis='y', labelcolor=color, labelsize=12)\n",
    "    ax1.tick_params(axis='x', labelsize=12)\n",
    "    ax1.spines['left'].set_color(color)\n",
    "    \n",
    "    # Set x-axis scale and add better tick labels for log scale\n",
    "    ax1.set_xscale(xscale)\n",
    "    if is_log_scale:\n",
    "        from matplotlib.ticker import LogLocator, LogFormatter\n",
    "        ax1.xaxis.set_major_locator(LogLocator(base=10, numticks=12))\n",
    "        ax1.xaxis.set_minor_locator(LogLocator(base=10, subs=(0.2, 0.4, 0.6, 0.8), numticks=12))\n",
    "        ax1.xaxis.set_major_formatter(LogFormatter(base=10, labelOnlyBase=False))\n",
    "    \n",
    "    # Smart surface area-weighted range calculation\n",
    "    weights_for_range = plot_df['area_nm^2_per_mL_avg'].values\n",
    "    sizes_for_range = plot_df['size_nm'].values\n",
    "    \n",
    "    # Find where 99% of the surface area signal is contained\n",
    "    cumsum_weights = np.cumsum(weights_for_range)\n",
    "    total_weight = cumsum_weights[-1]\n",
    "    \n",
    "    # Find 1st and 99th percentiles of the surface area-weighted distribution\n",
    "    p1_idx = np.searchsorted(cumsum_weights, 0.01 * total_weight)\n",
    "    p99_idx = np.searchsorted(cumsum_weights, 0.99 * total_weight)\n",
    "    \n",
    "    signal_min = sizes_for_range[max(0, p1_idx)]\n",
    "    signal_max = sizes_for_range[min(len(sizes_for_range)-1, p99_idx)]\n",
    "    data_max = plot_df['size_nm'].max()\n",
    "    \n",
    "    if is_log_scale:\n",
    "        # Log scale: focus on the surface area-weighted signal range with some padding\n",
    "        min_size = max(signal_min * 0.7, 20)  # Don't go below 20 nm\n",
    "        max_size = min(signal_max * 1.8, data_max * 1.2)  # Surface area less skewed than volume\n",
    "        ax1.set_xlim([min_size, max_size])\n",
    "    else:\n",
    "        # Linear scale: tighten the range more, focus on main signal\n",
    "        min_size = 0\n",
    "        max_size = min(signal_max * 1.15, 350)  # Tighter range for surface area\n",
    "        ax1.set_xlim([min_size, max_size])\n",
    "    \n",
    "    # Set y-axis to start from 0\n",
    "    y_min, y_max = ax1.get_ylim()\n",
    "    ax1.set_ylim([0, y_max])\n",
    "    \n",
    "    # Add D-value lines and bands\n",
    "    d_legend_elements = add_d_value_lines_and_bands(ax1, stats)\n",
    "    \n",
    "    # Create comprehensive legend for top plot - PLACE OUTSIDE\n",
    "    main_legend = [Line2D([0], [0], marker='o', color='w', markerfacecolor=color, \n",
    "                          markersize=8, label='Surface Area Distribution')]\n",
    "    \n",
    "    all_legend_elements = main_legend + fit_legend_elements + d_legend_elements\n",
    "    leg1 = ax1.legend(handles=all_legend_elements, fontsize=9, frameon=True, \n",
    "                     bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    leg1.get_frame().set_alpha(0.95)\n",
    "    leg1.get_frame().set_edgecolor('lightgray')\n",
    "    \n",
    "    ax1.grid(True, linestyle='--', alpha=0.4)\n",
    "    ax1.set_xlabel('')  # No x-label on top plot\n",
    "    \n",
    "    # =================================================================\n",
    "    # BOTTOM SUBPLOT: CUMULATIVE DISTRIBUTION WITH UNCERTAINTY BANDS\n",
    "    # =================================================================\n",
    "    ax2 = fig.add_subplot(gs[1])\n",
    "    \n",
    "    if 'area_nm^2_per_mL_cumsum_avg' in plot_df.columns:\n",
    "        # For surface area cumulative, normalize to percentage\n",
    "        cumsum_values = plot_df['area_nm^2_per_mL_cumsum_avg']\n",
    "        max_cumsum = np.max(cumsum_values)\n",
    "        if max_cumsum > 0:\n",
    "            cumsum_percentage = (cumsum_values / max_cumsum) * 100\n",
    "            ax2.plot(plot_df['size_nm'], cumsum_percentage, '-', \n",
    "                    color=color, linewidth=3, alpha=0.9, label='Cumulative %')\n",
    "            \n",
    "            # Add uncertainty bands if available\n",
    "            if 'area_nm^2_per_mL_cumsum_sd' in plot_df.columns:\n",
    "                cumsum_sd = plot_df['area_nm^2_per_mL_cumsum_sd']\n",
    "                cumsum_sd_percentage = (cumsum_sd / max_cumsum) * 100\n",
    "                ax2.fill_between(plot_df['size_nm'], \n",
    "                               cumsum_percentage - cumsum_sd_percentage,\n",
    "                               cumsum_percentage + cumsum_sd_percentage,\n",
    "                               color=color, alpha=0.25, zorder=1, label='¬± SD')\n",
    "        \n",
    "        ax2.set_ylim([0, 110])\n",
    "        ax2.set_ylabel('Cumulative Percentage (%)', color=color, fontsize=14, labelpad=10)\n",
    "        ax2.tick_params(axis='y', labelcolor=color, labelsize=12)\n",
    "        ax2.spines['left'].set_color(color)\n",
    "    \n",
    "    # Format bottom subplot\n",
    "    ax2.set_xlabel('Size (nm)', fontsize=14, labelpad=10)\n",
    "    ax2.tick_params(axis='x', labelsize=12)\n",
    "    ax2.set_xscale(xscale)\n",
    "    ax2.set_xlim(ax1.get_xlim())  # Match top plot limits\n",
    "    \n",
    "    if is_log_scale:\n",
    "        # Add same detailed log scale ticks to bottom plot\n",
    "        from matplotlib.ticker import LogLocator, LogFormatter\n",
    "        ax2.xaxis.set_major_locator(LogLocator(base=10, numticks=12))\n",
    "        ax2.xaxis.set_minor_locator(LogLocator(base=10, subs=(0.2, 0.4, 0.6, 0.8), numticks=12))\n",
    "        ax2.xaxis.set_major_formatter(LogFormatter(base=10, labelOnlyBase=False))\n",
    "    \n",
    "    # Add D-value lines to bottom plot\n",
    "    if 'area_nm^2_per_mL_cumsum_avg' in plot_df.columns:\n",
    "        add_d_value_lines_and_bands(ax2, stats)\n",
    "    \n",
    "    # Legend for bottom plot - PLACE OUTSIDE\n",
    "    cumulative_legend = [\n",
    "        Line2D([0], [0], color=color, linewidth=3, label='Cumulative %'),\n",
    "        Line2D([0], [0], color=color, alpha=0.25, linewidth=8, label='¬± SD')\n",
    "    ]\n",
    "    \n",
    "    leg2 = ax2.legend(handles=cumulative_legend, fontsize=10, frameon=True, \n",
    "                     bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    leg2.get_frame().set_alpha(0.95)\n",
    "    leg2.get_frame().set_edgecolor('lightgray')\n",
    "    \n",
    "    ax2.grid(True, linestyle='--', alpha=0.4)\n",
    "    \n",
    "    # =================================================================\n",
    "    # TITLE AND METADATA\n",
    "    # =================================================================\n",
    "    \n",
    "    # Extract replicate info\n",
    "    replicate_info = \"\"\n",
    "    if metadata and 'num_replicates' in metadata:\n",
    "        num_reps = metadata['num_replicates']\n",
    "        if num_reps and str(num_reps) != '1':\n",
    "            replicate_info = f\" (n={num_reps})\"\n",
    "    \n",
    "    # Set main title\n",
    "    main_title = f'{scale_name} Surface Area-Weighted\\nDistribution: {uniqueID}{replicate_info}'\n",
    "    fig.suptitle(main_title, fontsize=14, fontweight='bold', y=0.94)\n",
    "    \n",
    "    # Add subtitle\n",
    "    subtitle = f\"Error bars/bands: ¬± SD | Fits: Lognormal\"\n",
    "    fig.text(0.5, 0.87, subtitle, ha='center', fontsize=11, style='italic')\n",
    "    \n",
    "    return fig, fit_results\n",
    "\n",
    "\n",
    "def generate_surface_area_plots(distribution_df, stats_dict=None, uniqueID=None, \n",
    "                               metadata=None, output_dir=None, config=None):\n",
    "    \"\"\"Generate surface area-weighted distribution plots for both linear and log scales.\"\"\"\n",
    "    \n",
    "    if distribution_df is None or distribution_df.empty:\n",
    "        return False, \"No data available for plotting\"\n",
    "    \n",
    "    plt.style.use('default')\n",
    "    \n",
    "    # Determine output directory\n",
    "    if output_dir is None:\n",
    "        if config is not None and \"directory\" in config:\n",
    "            base_dir = config[\"directory\"]\n",
    "            output_dir = os.path.join(base_dir, \"processed\")\n",
    "        else:\n",
    "            output_dir = os.path.join(os.getcwd(), \"processed\")\n",
    "    \n",
    "    try:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "    except Exception as e:\n",
    "        return False, f\"Failed to create output directory: {str(e)}\"\n",
    "    \n",
    "    created_files = []\n",
    "    \n",
    "    # Generate linear and logarithmic plots\n",
    "    for is_log_scale in [False, True]:\n",
    "        scale_type = 'logarithmic' if is_log_scale else 'linear'\n",
    "        scale_name = 'log' if is_log_scale else 'linear'\n",
    "        \n",
    "        print(f\"Creating {scale_name} surface area-weighted plot...\")\n",
    "        \n",
    "        # Filter data for this scale\n",
    "        plot_df = distribution_df[distribution_df['scale'] == scale_type].copy()\n",
    "        \n",
    "        if plot_df.empty:\n",
    "            print(f\"  Warning: No {scale_type} scale data available\")\n",
    "            continue\n",
    "        \n",
    "        # Get statistics\n",
    "        stats = None\n",
    "        if stats_dict and scale_type in stats_dict and 'surface_area' in stats_dict[scale_type]:\n",
    "            stats = stats_dict[scale_type]['surface_area']\n",
    "        \n",
    "        # Create the plot\n",
    "        fig, fit_results = create_surface_area_plot(plot_df, is_log_scale, stats, uniqueID, metadata)\n",
    "        \n",
    "        if fig is None:\n",
    "            print(f\"  Failed to create plot\")\n",
    "            continue\n",
    "        \n",
    "        # Save fit results to comprehensive fits file\n",
    "        if fit_results:\n",
    "            fit_type, fit_data = fit_results\n",
    "            try:\n",
    "                import json\n",
    "                \n",
    "                # Load existing fits file or create new one\n",
    "                fits_filename = f\"Fits_{uniqueID}_all.json\"\n",
    "                fits_path = os.path.join(output_dir, fits_filename)\n",
    "                \n",
    "                if os.path.exists(fits_path):\n",
    "                    with open(fits_path, 'r') as f:\n",
    "                        all_fits = json.load(f)\n",
    "                else:\n",
    "                    all_fits = {'dataset': uniqueID, 'fits': {}}\n",
    "                \n",
    "                # Add this fit to the collection\n",
    "                fit_key = f\"surface_area_{scale_name}\"\n",
    "                all_fits['fits'][fit_key] = {\n",
    "                    'distribution_type': 'surface_area',\n",
    "                    'scale': scale_name,\n",
    "                    'fit_type': fit_type,\n",
    "                    'parameters': fit_data\n",
    "                }\n",
    "                \n",
    "                # Save updated fits file\n",
    "                with open(fits_path, 'w') as f:\n",
    "                    json.dump(all_fits, f, indent=2, default=str)\n",
    "                \n",
    "                print(f\"  ‚úì Saved fit to: {fits_filename}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  ‚ö† Failed to save fit: {str(e)}\")\n",
    "        \n",
    "        # Save the plot\n",
    "        try:\n",
    "            base_filename = f\"Plot_{uniqueID}_{scale_name}_surface_area\"\n",
    "            \n",
    "            pdf_path = os.path.join(output_dir, f\"{base_filename}.pdf\")\n",
    "            fig.savefig(pdf_path, bbox_inches='tight', dpi=300)\n",
    "            \n",
    "            png_path = os.path.join(output_dir, f\"{base_filename}.png\")\n",
    "            fig.savefig(png_path, bbox_inches='tight', dpi=300)\n",
    "            \n",
    "            created_files.append(pdf_path)\n",
    "            print(f\"  ‚úì Saved: {base_filename}.pdf/.png\")\n",
    "            \n",
    "            plt.close(fig)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚úó Failed to save plot: {str(e)}\")\n",
    "            plt.close(fig)\n",
    "            continue\n",
    "    \n",
    "    return True, created_files\n",
    "\n",
    "\n",
    "# Execute if we have the required data\n",
    "if 'current_distribution_df' in globals() and current_distribution_df is not None:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"GENERATING SURFACE AREA-WEIGHTED DISTRIBUTION PLOTS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    uniqueID = current_metadata.get('persistentID', 'unknown') if 'current_metadata' in globals() else 'unknown'\n",
    "    stats = current_stats if 'current_stats' in globals() else None\n",
    "    metadata = current_metadata if 'current_metadata' in globals() else None\n",
    "    config = CONFIG if 'CONFIG' in globals() else None\n",
    "    \n",
    "    print(f\"Creating surface area-weighted plots for: {uniqueID}\")\n",
    "    print(\"Includes: Linear + Logarithmic (Lognormal fits for both)\")\n",
    "    \n",
    "    success, plot_files = generate_surface_area_plots(\n",
    "        current_distribution_df,\n",
    "        stats_dict=stats,\n",
    "        uniqueID=uniqueID,\n",
    "        metadata=metadata,\n",
    "        config=config\n",
    "    )\n",
    "    \n",
    "    if not success:\n",
    "        print(f\"ERROR: {plot_files}\")\n",
    "    else:\n",
    "        print(f\"\\n‚úì Successfully created {len(plot_files)} surface area-weighted plots!\")\n",
    "        for filepath in plot_files:\n",
    "            print(f\"  - {os.path.basename(filepath)}\")\n",
    "        \n",
    "        current_surface_area_plots = plot_files\n",
    "\n",
    "else:\n",
    "    print(\"No data found. Run the complete workflow first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01b34272-71a3-4b61-9fb1-34caa08898cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "GENERATING RAW PARTICLE COUNT VISUALIZATION\n",
      "================================================================================\n",
      "Creating raw particle count plots for: 20250527_0004_23052025_SUV_5136664_after6weeks_size_488_avg3\n",
      "‚úì Saved raw count plots: Plot_20250527_0004_23052025_SUV_5136664_after6weeks_size_488_avg3_raw_counts.pdf/.png\n",
      "‚úì Updated metadata with total raw particle count\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABW0AAAJdCAYAAABJSlWQAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAEAAElEQVR4nOzdd3hT1f8H8HfSJulINy1ltkCBslpKWS1QhihbQLaCIioCgjhwooKi4hZxoqCACAj+kCEiwteyQRBBZImMsqFlddNBPr8/6r0mTdImbUpLfb+eJw/0znP3OZ977jkaEREQERERERERERERUYWgLe8EEBEREREREREREdG/GLQlIiIiIiIiIiIiqkAYtCUiIiIiIiIiIiKqQBi0JSIiIiIiIiIiIqpAGLQlIiIiIiIiIiIiqkAYtCUiIiIiIiIiIiKqQBi0JSIiIiIiIiIiIqpAGLQlIiIiIiIiIiIiqkAYtCUiIiIiIiIiIiKqQBi0pVKZO3cuNBoNRo4cWeplhYeHQ6PRICkpqdTLInLEhg0boNFo0KlTp/JOChEREf0HZGVlYdKkSahTpw50Op3L8tHkHI1GA41GU97JKJFOnTpBo9Fgw4YN5Z0U+g+6fv06wsLC0LhxY5hMpvJOzn/SyJEjodFoMHfuXIvhrozN3Mrmz58PjUaD2bNnl3dSXOKWDtoqQT7zn4eHB+rUqYPhw4dj165d5Z1EpylBJPOfVquFn58fWrdujenTpyMrK+umpOXatWuYOnUqZsyYcVPWV162bt2K0aNHIzIyEn5+fjAYDKhRowZ69+6N2bNnIzMzs7yTWCJz587F1KlT/1NB8G+//RY9evRA1apVodfrERQUhMaNG2PQoEH4+OOPcf78+fJOokvcuHEDa9euxYQJE9CiRQv4+PjAYDAgLCwM9957L37//fdil/Hjjz+ia9euCAwMhLe3N1q0aIEPP/zQbuZrz549eOmll9CxY0dUqVIFOp0OISEh6NGjB77//nu761EyD0X9fvrpJ7vznz17FqNHj0atWrVgMBhQu3ZtPPzwwzh79qzN6ZOSkopd37PPPms1X3p6OhYsWIDhw4ejQYMG8PT0hJeXF5o0aYKnnnrK7rnjyPa5smC4YMECxMfHw8/PD76+voiPj8c333xT7HzOHm/F9u3b0bdvXwQHB8PT0xONGzfGtGnTcP36dYfTvH79enUfdO3a1eH57Dlz5gxmzJiBPn36oGbNmtDr9fDz80NcXBzef/995OTk2JzvwoULmDZtGnr37o169erB19cXBoMBderUwb333ou9e/cWud7r16/jlVdeQePGjeHp6Yng4GD07dsXO3bssDuPrXyK+a9t27ZW8/yXrm9H5efnY9asWejcuTOCgoLU9DVr1gwjRozAnDlzcPXqVYt5lAJNcQWXqVOnWry8S0tLg5eXFzQaDdatW+dQ+qKioqDRaPD222+XZPPUAIy9X2hoqM35Tpw4gS+++AIPPfQQoqOj4e7uDo1Gg1dffbXI9SUmJuLRRx9FXFwcatSoAYPBAB8fH8TGxmLatGlIT08vNs3r1q3DgAEDUL16dRgMBoSGhqJTp07F7oNdu3bh3nvvRe3atWEwGBAcHIy4uDhMnjwZ+fn5duf766+/MGbMGNStWxceHh4IDAxEixYt8OSTT1ode1uuX7+O+vXrq/v0zJkzxc5TVh566CG8++67SE5ORvPmzdGuXTs0aNAAQMH5OHXq1HJLm6K0+5vIHhFBQkKCei1u2bLF7rTHjx/H2LFjERERAQ8PD3h6eqJRo0Z44okncOHCBbvzJScnY9KkSWjSpAm8vLzg4eGBevXqYfTo0Th69GhZbFaFs3z5cjz88MOIjY1FtWrVoNfr4e/vj/j4eHzwwQfIzc0t8bI//PBDnDp1Ci+88AK02ooVTiptXo9KJykpCVOnTrUKJt9sd999N+rWrYspU6bctNhZmZJbWFhYmACQ+vXrS7t27aRdu3bStGlT8fDwEADi5uYm8+fPL+9kOiUxMVEACAB1m+Lj46VGjRrq8CZNmsjly5fLPC0nTpwQABIWFmZ3mmXLlknDhg3l2WefLfX6lON54sSJUi/LEZmZmTJ48GB1v3p4eEiTJk2kZcuWUq1aNXV4tWrVZN++fTclTa7UsWNHASCJiYnlnZQyl5ubK/369VOPmZ+fn0RFRUlMTIz4+fmpwz/88EOL+X799Vdp2LChjBgxopxSXjKzZ89Wt8nd3V0aN24s0dHR4unpqQ6bNWuW3fmnT5+uzl+3bl2JiooSrVYrAOTOO++UGzduWEx/9OhRdXoAUqdOHYmNjZWAgAB12H333Wc1n4jIV199JQAkJCREvacV/u3YscNmOg8cOCCBgYHqMW3RooV6PIOCguTQoUNW8yj3LYPBYHd9n376qdV8w4cPV7fFx8dHmjdvLvXr1xc3Nzd1fTt37rSa78cff7S7nnbt2klQUJAAkLi4OLvHw1EPP/ywmsbIyEhp1KiR+vcjjzxidz5nj7diwYIF6vbXqFFDYmJiRKfTCQBp1aqVZGZmFpvm7OxsiYiIUNd/2223lXj7FTVr1lSXV7VqVWnZsqXFM7J58+Zy6dIlq/nWrVunTlOlShWJjo6Wxo0bi5eXl5pn+PLLL22uMyMjQ2JjYwWA6PV6iYmJUdfp5uYmixYtsjmf8lxr2bKlzfNj9OjRVvP8V65vR6WmpkpcXJzFsYuJiZHo6Gjx9vZWh69atcpivvvuu09Ne1GmTJkiAKRjx47qsCFDhggAuffee4tN3759+wSAaLVaOXPmTEk2UX1eN23a1OY+vPPOO23ON3HiRItjp/ymTZtW5Pruuece9VyqXbu2tGzZUsLCwkSj0ajnwMmTJ23OazKZZMyYMeq6atasKa1atZLw8HBxd3eXoKAgu+t99dVX1XOxatWq0qpVK4mIiBCDwSAAJD093eZ8X331lTpNQECAtGzZUiIjI9Vr988//yxye0VEJk+ebLGPTp8+Xew8ZeHKlSui1WrFy8tLTp06ZTVeSV95csX+vhVUhH1dUiNGjJCGDRvKr7/+Wt5JcdoXX3xhcS1u3rzZ5nRbtmxR7/EeHh7StGlTadiwoZoPqVKlihw4cMBqvsOHD0tISIgAEJ1OJw0bNrSID3h5ecmGDRvKejPLXbt27dT8cJ06dazySrGxsXL16lWnl5uamioBAQFSt25dyc/Pd33CS6k0eb1biZLH+eqrryyGuzI2UxJKLMs8T1VePvvsMwEgb7zxRnknpdRuzSfVP5TCUOGT9cqVKzJw4EC18H3lypXySWAJmAdtC1u/fr34+/sLAHn44YfLPC2OBG1d6WYGbXNzc9WHWWhoqMybN0+ysrIspjlw4IA8/PDD4u7uLt9//32Zp8nV/ktB29dff10AiKenp3zzzTcWmQiTySS7du2SRx99VObNm1eOqXSdL774Qlq0aCFff/21ZGRkqMNTU1Nl5MiRaqbkjz/+sJp327ZtotFoRKvVysKFC9Xhe/fulapVqwoAefvtty3m+fvvv6VatWry5ptvyrlz59ThN27ckA8//FAt6BcOiov8G9QpLnBSWH5+vjRu3FgAyIABA9QAYUZGhtx1110CQKKioqwCSSW9bw0fPlz69esn69evl7y8PHX40aNHpXXr1gJAatWqZXWfKIrJZJJatWoJAPnkk0+cSk9hixYtEgDi7e0t//vf/9Th69evVws1S5cutZqvJMdbpGA/KoX2t956S0wmk4iIJCUlScOGDYsNFCuUQMmdd97psqBtRESEPProo1Yv09avX68W1AYMGGA135EjR2T27NlWwZq0tDR57LHH1MKNrWCOEjCPjIyUpKQkESk4/99880313mMrAFOS59p/4fp2xujRo9XC15o1ayzG5efny8aNG+X++++XdevWWYwrTdD2hx9+UPOQxb2ceOqppwSAdO3a1antMlfS5/W0adOkd+/e8sorr8iaNWtkwIABDgVtv/vuO1mzZo3NfE9UVJQAkJ49e9qc97nnnlMDzIVfZKWmpsrKlSttzjdr1iw1yPvzzz9bjMvKypKVK1dKbm6u1Xxr1qwRrVYrfn5+8u2331rc8/Py8mT9+vU2X9KYO3jwoOj1evU+VJ5B219//VUASOvWrW2OL+9Aoiv2962ivPf1f1FycrIEBgZKTEyM+gLWVtDWZDKpL3z79+9vUVnp5MmT0rZtW7uBodtuu02AgspP5tf5pUuX1HtAnTp11HxNZfXVV19JYmKi1X11+/bt6r4fN26c08v95JNPBIBMmTLFRSl1rdLk9W4l9oK25a0iBW2vXbsmBoNBwsLC7FZQuVXc0k8qe0FbkYKMo1KQXbx48c1PXAkVFbQVEXnvvfcEgPj7+5f5yVeZg7ZKIKFq1arFrm/z5s2ydevWMk+Tq/2XgrZKEOnVV18t76TcFFeuXLGb2czLy5OmTZsKAHn00Uetxvfs2VMA2Kzh98033whQUKvUPJOXnZ1dZOBCqXUVFRVlNa6kQZ0lS5aoaUlLS7MYl5aWptZgXbZsmcW4kt63ivp64dSpU6LX622uryi//PKLAAU1M0v7dUSTJk0EgLz++utW41577TW7+78kx1tEZNy4cQJA7rjjDqv5tm7dqtZguXDhgt00K4GSHj16qOeBK4K2Re3LxYsXC1BQ69GZwILJZFL38eeff24x7ty5c+Lu7i4AZNu2bVbz3n777Xavt5I81/4L17ej8vLy1LzcggULnJq3NEHbvLw89QWAvVrUIgWBbaXwW5qXgq56XivbXFzQtig7d+5UXwxkZ2dbjPvzzz/Fzc1NgoOD5eLFiw4v8+LFi+Lj4yMeHh5y8OBBh+fLycmR2rVri0ajsXhZ5QyTySQdOnQQT09P9flQnkHbDRs2FFmgLc9Aoiv2962EQdub75577hGNRiPbt29Xn4+2graHDx9Wn+W2aoPu3btXAIhGo7F4fmVmZqq1+W19JXnlyhX1RaQz96LKRsljV69e3el5Y2JiBIAcPny4DFJWtorK691qGLR1TN++fQWA/PTTT+WdlFKpWI2QuJCvr6/aPpStNj3XrVuH8ePHIzo6GoGBgWpbN2PHjsWpU6espp84cSI0Gg1mzpxpNS4yMhIajQYdOnSwGqe0leaq9qkSEhIAFLQ3e+nSJQDAjh078PTTT6Nly5YICQmBwWBArVq1MGLECBw4cMDmcszTlZKSgvHjxyM8PBw6nQ4jR47EyJEjUadOHQDAyZMn7bbNWFxj11euXMGUKVMQExMDX19fGI1GNGrUCGPGjMGePXuc2va1a9fizjvvRNWqVWEwGFCzZk3cf//9OHbsmFPLSU1NVY/jjBkzEB4eXuT07du3R3x8vNXw1atXo3v37qhSpYraVs64ceNw+vRpm8sprl1Lex2xmQ/fsWMHevTogYCAAHh7e6NDhw745ZdfLKZX2kXeuHEjAKBz584Wx868jZn9+/fjnnvuQa1atdS2jurXr4+7777b4XYIJ02aBI1Gg/Hjx9udZv/+/dBoNAgJCbFos27Lli3o378/QkNDodPpEBgYiEaNGuHBBx8ssp3Iwo4fPw4AaN68ucPzAPY7IiuuHUp75/yZM2fw6KOPqm2i+vv7o3Pnzvjuu++cSldxAgIC7J5L7u7u6NKlCwDgyJEjFuPS0tKwfv16AMADDzxgNe+gQYPg6+uLy5cvIzExUR3u4eEBLy8vu+m54447bK6vNJYtWwYAGDx4MHx8fCzG+fj4YNCgQQCApUuXumR9gYGBdsfVqlULkZGRAJzbxq+//hoA0LNnzyKXX5y//vpLvZePGjXKarwybN++fRbpK+nxFhG1HVNb88XHxyMyMhJ5eXlYsWKFzTSLCB5++GFotVp89NFHjm6qQ4ral8q5aDKZnGq3TqPRoGHDhgBg1fbVypUrkZ+fj0aNGiEuLs5qXmUfueo6/y9c345KTk5W25V39v5eGu7u7hg6dCiAgnak7dmwYQPOnDkDLy8v3HXXXTcreWVKudfduHHDqn3ojz76CDdu3MDEiRMREhLi8DK//PJLpKenY/jw4WjUqJHD8y1btgynTp3Cbbfdpp73zpozZw42b96MF154odj8nrOcKUso7a0r+Y2NGzda5SnMr/vCeY7CeUNn8xvmHVft3bsXAwcORNWqVaHVatV8YUn29/fffw+NRmPz/B8zZgw0Gg0MBgOys7MtxhXVEeyVK1cwefJkNG3aFN7e3vDx8UHbtm3xxRdfFNkO+86dOzF06FDUqFEDer0eVatWxaBBg5wub4gIHn30UWg0GkRERODkyZPqOFfkm+2tc/78+UhISIC/vz/0ej1CQ0MRGxuLp59+2qoN5k42OiJztI39wueSiGDx4sW4/fbbERQUBIPBgLp16+LRRx8tst1YZ61fvx7ffPMNHnzwQZvtuZtTzpfAwED4+/tbja9Xr56advNyRW5urnqO1K1b12q+gIAANQ9RVBvaxTl+/DjefPNNdOrUSe1zITg4GN27d8fq1autpv/oo4+g0WjQu3dvu8u8cuUK9Ho9dDodLl++bDFuz5496NOnDwICAmA0GtG2bVv1Wi9JvwnKfd7Ztj6PHDmCPXv2oF69emqeyZxyn1PutQsWLEDLli3h5eWFwMBADBo0SC2zlYei8npFMb9f5efn46233kKzZs3g5eVl9Vw5fPgwRo0ahfDwcBgMBgQFBaFXr15W5XWFeTl/7dq16NSpk9pvxe23347Nmzc7tY2ujM3s378fU6ZMQVxcnNoucrVq1XDXXXdh27ZtVsvu1KkTOnfuDMD6GWfr+evsPfvkyZN4+OGHUbduXbUt/rp166J///5YvHixzXmUa+7bb7+1t8tuDeUZMS6tomraivxb+27mzJlW49zc3ESj0UhISIg0b95cmjZtqtbmCAoKsmoj57vvvhMActddd1kMv3jxovqmVq/XW31q1rlzZwEgv/zyi0PbVFxN2127dqnjlVoO9erVU9PdtGlTiY6OVtt99PT0tFlzQ6lZMm7cOKldu7a4ublJVFSUREVFyahRo+S1116Tli1bqp8QFG5fTVFULZu9e/dK9erV1TeljRs3lubNm4uvr6/NeYqqkWTebltISIjExMSoy/H19XWqJqxS2yg4ONjiM2hnPPvss2p6atasKbGxsWpbOQEBAbJr1y6reYo6riL2t18Z/uGHH4pOp5OgoCCJjY1Vj7G7u7vFMf7999+lXbt26v4p3Ebejz/+KCIFn+gpbST6+flJdHS0NG3aVF1u3759HdoXu3fvVo+LvbaNlM8pzT/DWb58ufo2PCgoSFq0aCGRkZHqdThx4kSH1i8iaq3Ll156yeF5ROy/DRw4cKDd9hmVJkoKn78bNmywuO6aNWumfhoPQJ588kmr9ZvX+nFlDXPlc+L+/ftbpREoaBvM3rmvfFb2yiuvOLy+hQsXqud+Yco9onnz5jJs2DDp3Lmz9O3bV6ZOnSpHjx61u8zw8PAia9d9/fXXAhS02WlO2acBAQEyevRoue2226RHjx7y2GOPyaZNmxzepsIaNGggAOSDDz5waPrs7Gz1Gvy///u/Eq9XRGTu3LkCQCIiIuxOozwHzNtxL+nxTkpKUs9Le210PvDAAwJARo0aZXO80mbdyy+/LCLi0pq2RTl37pyadlvNB9hz/fp19ZwrfJ4oTRI8+OCDNuc9ffq0us7CTSQo9+9Ro0ZJ9+7dpWvXrjJq1ChZvHhxiduCqwzXt6PS0tLUWlHOtkFXmpq2Iv/mt9zd3SUlJcXmvPfff78AkHvuuceptBWm1LQdMmSI9OrVS2677TYZPny4zJkzx6q2a1FcUdN27dq1Nu+tIqLm6fbu3Su7d++WcePGSdeuXeXOO++U1157zW7t2/j4eAEgy5cvlyNHjsiTTz4pd9xxh/Tq1UsmT54sx48ftznf3XffLQBkxowZcvbsWXnxxRelR48e0r17d3niiSeKvcaVT7EbNmwoOTk5IvJvXswVNW2dKUucP39e7XtDybua5y1ee+01tdkuAFZ5j/Pnz6vLKkl+QznHXn75ZTEYDGI0GiU2Nlbq1q2rlqNKsr8vXbokGo1GgoKCrL4QMG93vXDN3Zdfftlmvm3//v1qu5t6vV4aN24s9erVU+8DAwcOtPklwnvvvadOo3x+r+QNdTqdzeewrXx5fn6+es9v2rSpxX53Vb7ZlieffFJNT+3ataVVq1ZSp04d9Sufws202aqdX1Qb+0qzJ4Xzm7m5uTJo0CB1XPXq1SU6Olot01SrVk3++uuvEm+XQmnfPigoSP1apqiathkZGeq+PnLkiNX4n376SQBIw4YNrcYp10LhZlhE/q3B6+/v79S9tTAlD2Q0GqVBgwZW/aEUbkMzOTlZ3N3dRafT2f1aSGlCpnDTNOvWrVObq/L19bVYl/IFblHly6LW1aVLF6fmU/J2w4YNszne/Gs3pawcFhYm0dHR6jZUq1bN7jO1rBWV1yuKUl5MSEiQXr16CQCpV6+exMbGSpMmTdTpvv32W/WaVfrHCA0NFaCgVritmJRyHUyfPl00Go0EBgZKy5Yt1fuXVquVJUuWWM1nr6atK2MzSp7R399fGjVqJC1atJAqVaoIUPA1zjfffGMx/fjx4+0+4wYOHGgxrbP37BMnTqjr9vLykmbNmknz5s3Vvk+io6NtHDmRP/74Qz1et7JKG7Q9cuSI+jmjrYty1qxZcvbsWYthWVlZ6memnTp1shiXnJwsQEGbauaZBeXzAiWDYR6czcnJEU9PT5vBXHscbR7Bz89PbR5h3rx5cuzYMYvp8vLyZPbs2eLu7i5169a1akpBKaS4ublJXFycReZVeYg58pmxvRtDamqq1K5dWwBI9+7drTLHmzZtsgrG2AtaKo1I16lTxyJzkp+fL6+++qoABYFTRx++jzzyiACQfv36OTR9YatWrVILcubbkJqaKv379xcAEh4ebnXMi3uoFhe01el0Mn36dLWgn5ubq3Ym0qZNG6vlFfe5Ze/evQWAPP/882phRrFr1y6rG3FRIiMjBYCsXbvW5vg6deoIANmyZYs6TLmpf/LJJ1Zt0CYmJtptF88WZT/odDp59tln5c8//3SorSpnP+HYtGmT6HQ60el0FveVs2fPSmBgoGg0Gnn99dfl+vXr6ritW7eq94fCHeWURdA2OztbzSC88847FuOUzFaDBg3szv/QQw8JAKc6Z1M6gevdu7fVOOUeYevn5uZms0mLnJwcNaBv63N0kX8/0ddqtRafepvvU1u/gQMHWrQT6gjlMzwA8ttvvzk0j/KZfkBAgMX5UBJKcy62mipQKJ/ov/jii+qwkh5vpRMHg8Fg9zpSnpUdOnSwGqcESiIiItRtv1lB2xkzZqj73ZGXcmlpabJjxw7p0aOHAJBBgwZZTaMEUmw1TSFScM9SMumFAxPK/dvWr2nTpk4HNivD9e0sZf/7+PjI9OnT5e+//3ZovtIGbUX+DTp99NFHVuPMX8yU9rM75Xlt61erVi2bL4JtKWnQ1mQyyfnz52XBggVSrVo1cXd3l+XLl1tMc/78ebXg+f7776udFJr/fH19rdoWvnHjhhp8efvtt9X/m//0er18/fXXVulSKl9MmzbNomM85afVamX69Ol2t0vJG6xfv14dpszriqCts2UJkeLzHcXlFUua31DOMTc3Nxk9erTFJ+VKfrWk+1v53Ni8gzKl3KSkp3BwtkuXLlb3zIyMDPUF5KOPPiqpqanquAMHDqjrKXw9rlmzRjQajVSpUsWqoK+UhXx8fCza7Bax3tc5OTlqu9CtW7e2Cq65Mt9sLjk5WW1H2DyfLFJwn1m0aJFVwNyZJlVu3LihPuN69uxpUSZUAmsxMTGyZ88edXhWVpbaTFLLli1LtF3mlHzM7Nmz1WFFBW1FRF555RUBII0bN5aff/5ZUlNT5dKlS7J06VKpXr266HQ6Wb16tdV88+bNU8uG3333nVy6dEmuXbsmP/30k0RGRopGoyn1p/E//vij7NixwyqPtGnTJqlWrZq4ublZPd+7desmAOx2JNqpUycBLCsrpKWlqc/8+++/X71WTSaTfPTRR2ogtKh7hiI/P19Onz4tH3/8sfj4+Ii3t7fTHdkpLyoL5z8USh7c3d1dfH191YpCIgXPEOXlwTPPPGM17/jx44vs2Nfez/zFij2O5PWKoty33dzcJCQkxKJsosQf/vjjDzEYDOLh4SGff/65xXW2cuVK8fX1FTc3N9m7d6/FspXrwN3dXZ544gm1TJOXlydPP/20+mwtfP9yNmhbktjM0qVLrZoZMZlMsnz5cjEajeLr62vVhJ0jZeuS3LPHjx+vblfhTksPHTpk97rKz89X8x2OnCsVVaUL2qampsq6devUDmzMa4U6qn379gJY1zBSMu/79+9XhykBwA8++EAAy0a5N2/eLACkffv2Dq/b0Y7IHnjgAYeWp/SIXrgmqlJIMRgMVhlORWmCtm+99ZYAkEaNGjkcsLAVtMzJyZHQ0FBxc3OT33//3eZ8SgbLvIZZUZQC6OOPP+7Q9IUpBUhbNUEzMzPVt0Bz5syxGFfcQ7W4oG2fPn2s5klJSVEf2IU73CsuQ6dkzs0zxSWl1JgYOXKk1bjt27er55F55sZgMNisuVUSZ86cUQPDys/Pz0+6dOkiU6dOtdvukjNB25MnT0pwcLAAkM8++8xi3BNPPFHkOaUE+gu/0T59+rTUqFFDatSo4bL29ZRazYGBgVbtgCnXpa0gv0LJINgK0Nii1MoCIBs3brQa//3338uECRNk69atcvHiRbl+/brs2bNHRowYoc5XuIMjpbAHQA4dOmRzvQcPHlSnMW+79PTp0zJo0CD54Ycf5OTJk5KTkyPHjx+XadOmqYE1W51U2ZOfny8dOnSwefyKoryJd0WnkUrBaciQIXanGTx4sACQ8ePHq8NKeryVl5FVq1a1O5/SEUXTpk2tximBEvNA1s0I2p47d059S19cTVKlZpTyCwkJkffff99m7VclP/Hpp5/aXZ7S/ul3331nMfzuu++Wr776Sv766y/Jzs6W5ORkmTdvnlrToW7dunLt2jWHt7EyXN/O+uOPP9TjqvyqVKkiPXr0kDfffNNmB3AirgnaKp1ctm3b1mrct99+K0BBZ6al7UF7woQJMnPmTDl48KBkZmbKlStXZNmyZeoL0cDAQLUDvKI4G7T9/vvvrQJzHTt2tAociRR8xaME7rRarcTFxcnvv/8uOTk5cuDAAfXFka+vr8UxuXLlirpsnU4nkZGRsnHjRrl+/bocP35cvV+4u7tbvRRTguI6nU6qVasmK1eulOzsbDl79qxFzcQVK1ZYpXf9+vUCQIYOHWoxXJmnrNu0tVeWKG3QtqT5DSVPGB0dbbdPjJLu77FjxwpgGUxdunSpAJA333xTDAaDxfbm5uaKl5eXVaWWmTNnCmD9FYHijz/+EI1GY1ULvEWLFnbPA5F/a7EWfi6Y7+vMzEw1qNapUyerQISIa/PN5pR8sr3ttsWZoK1y32/YsKHF8yY5OVkMBoP4+vravB5u3LghrVq1EsC5WomFKe3bx8fHW5QDigvaihS8GFE6JDP/JSQkyPbt2+3O93//93/SvHlzq/mioqKsOrR0tdmzZwsAee211yyGK8FkWy9zzp49K1qtVry8vCwCUkrFpcjISJsvopV7flH3jPfff99qP/Tr18/iJYujlJct9l5QmFecePfdd63Gr1y5Uj0OhRX18rKoX1GVXpzJ6xXFPD5j7+s5pZNke1/kffjhhwJYf6GmXAf2aooq97fCL76cDdqWJDZTlBdeeMHmueBI2bok92zl/uzMV3QKZR/v2LHD6XkrikoRtLX102q1MmTIEKtAlrldu3bJM888I3369JGEhAT1jY1S+DJ/OyTyb2cc5r2AN2vWTAICAiQzM1MMBoPFjVh50/788887vE3mNwUlPfHx8WpHF0BBLZrk5GSL+Q4dOiQvvfSS9O/fXzp27KjOa6/ncqWQ0qtXL7tpKU3QVglsFg5uFcVW0FL53LNVq1Z251Mego4GspWq/i+88ILDaVOkp6erNQDt1ZBSCtWFAyzFPVSLC9rau7EpmcjChZ3iMnRdu3YVAPLFF1/YTZOj/v77bwEKAqWFHwSPPvqoANZvVSMiIkSj0dj8fKkkrl27JlOmTFE/ezH/aTQaefDBB63S5mjQNjMzU214f+zYsVbjlXUWblZFkZeXJ3q9vsjPll3hhx9+UD81sfUpjVJrwVbtSMWLL74ogGPBtZMnT6ovKUrSA63Si6ufn59FAenUqVPqsSv8FYHi2LFjThe+ly9frs7jaAFEKez4+Pg4XCtS+QwOgM3gh7NGjRolQNG1I5Ugmfl9sKTHe/78+QIU1PCzZ86cOQJYf26kBEoKfwZV1kHbnJwcNUjSvHlzq1pQhXXv3l3atWsnkZGR4uHhIUBBsNO8ppGibt26Ali/iDOnPGtt1Ra05cSJE+pLWEebKqgs13dJnD9/Xh5//HGpWrWq1f1dp9PJ888/bxWIckXQ9uTJk+o+L3z99+nTRwDIE088UaptK8qVK1fUF5L2miIx52zQdtOmTdKuXTtp27at1KhRQzQajXh6esro0aOtvhZSKiIABYHZwvnrrKws9WWE+T4xbz7Ezc3N6lNrk8kksbGxAlg3QWZem9dWx1jK100tWrSwGK58il1U7UpXBW2dLUuUNmhb0vyGkid8++237S67pPtb+bJk8ODB6rAJEyYIUNCURkJCghgMBjUPpnwpEx8fb7EcpaahrdqTCuV6UI6f0pxPSEiI3Xk2btxo876nbOu1a9fU50evXr3sfrnnynyzuTNnzghQ0DTByZMnHZrH0aDtokWL1Ptw4WtPaXqpqFqHSqWMkja5onQEWFQNQ3tB25ycHHn22WelSpUqalMZERER4ubmJnq9Xh544AGbX7KaTCZ5++23pVatWuLm5iYRERHSuHFj0ev1otVqpX///qXuHFakIK83Y8YMGTZsmNx2223qta98SWh+PYgU1Pb09PQUrVZrVWFK+ZK2cNlRqZhUOACsUMrJRd0zlixZIu3atZPWrVurz1A/Pz95/vnnnQ5eKjVl7QW+zYO2tl5IK19sGI1Gp9ZbUs7k9Yqi3Lf9/Pxs7rOcnBzx8PAQNzc3uy91Tp48aTPfrFwH9mqKKi8BCr+QdzZoW5LYjJLu6dOny6BBg6Rz587qeV6/fn0BIE8//bTF9MU940p6z37wwQcFKPii0JGvac0pZfjCX6DcSipF0LZ+/fpqcFMpXBkMBrtV900mk1pzqahf4SriSrtuyg318uXLotFo1DaMOnToIB4eHmqm5I477hDA/ifjtpgHbc1/RqNRYmJi5OWXX7Yq/Lz++utqINHer/AnikohZdKkSXbTUpqgrdK+SOEHdFFsBS2Vt1LBwcF2P4tQPpfq3r27Q+spTU3bPXv2qOeXvRvGggULBCj41MhccQ/V4oK25jW8zSm1ADds2GAxvLgM3Q8//KCmKTIyUiZOnChLly51qsd1c8rb+GXLlqnDbty4oX7WU/hc+Pjjj9X1x8bGyjPPPCOrVq0qdeFeROT48eOyePFiefjhh9X1AwWfFplzNGg7dOhQAQre7Jt/ii9SEMhXll/U5ztKLc8LFy6Uevts2bVrlxiNRgEgzz33nM1pXFkT7/Lly2oNxE6dOpXorW1qaqpaU9z8U9zS1LQtTlxcnACQxx57rNhpP/30UwEKaoD98MMPDq9DqS1kq03IkrhVatoqgRKj0WgVECnLoK3JZJJhw4YJUNBWmrNNjWRkZMjLL78sWq1WvL29rYJzpalpWxTlBV/z5s2LnbYyXd+ldfDgQZk3b57ce++9auAb+Lf9ZIUrgrYi/waSzJd/6dIl0el0AsDpwp+zlLYHAwMDiy2olLZN24MHD6p9MRTOU5n3qWB+nzFn/jmzIiUlRZ3P3nmnvCjy8fGxCL4r7cPaqtEvUhB0VpZtXplB+RT7/ffft5pHmb60QduSliVKE7QtTX5DyRMW9Swr6f5W2hI3f2ZERUVJQECA3LhxQ31ZpNTWV2qwF76XKTXqY2Nj7W6bj4+PAP/WmFJqFhduP9H8p7wUiIyMtLmvlVpfQ4cOtcrjmXN1vtmc0q6sTqeTO+64Q15//XXZvHmz3Rf9jgRtd+/erQYJC788EPm3NlutWrXs7julluuYMWNKtF1Ksz22vk4sLmir1Kzr27evxfl2/Phxadu2rQDW7b+K/Nvue3x8vEV+4OLFi2oTF1FRUaX6QmLt2rVWtTgL/7p27Wo1n3Kc33vvPYvhShmq8LNSCZLaCzaZf8ngqB07dkh0dHSJjqvSv4O9vnqU2EGVKlVsjr9x44bT6XWV4vJ6RVHu2/aaCvnzzz8FKGjqx961pLTt7uHhYTGvch3Yq0yivOQq/HWqs0HbksRm5s6dqwa77f0K9/dQ3DOupPfsPXv2qPmusLAwefjhh2XBggV2vxg3p+x7WxUebhWVImhb+GTdsmWLWjvEVq0XpWamt7e3fPLJJ/L3339bvKlTPtUqvNyzZ88KUPApnMi/n5UpN14lk7hp0ybJy8sTo9Eo7u7uVu1uFKW4Nm0LU95EuLm5yfTp0+XAgQOSkZGhZuyVNJk32yDybyGl8HBzpQnaKrXMHPmcT2EraKm0WevIz9F2SUvTpq1S06RmzZp2p1HagyzcaVBxx7W4oK29QIS9jJsjGbrVq1dLfHy8RdDf3d1dBg0aZLcDInuUz2/M39grte7MC3Dm5s+fr2YclJ+Hh4c89NBDTn0yXJTMzEw1oKPRaCw+23QkaKsULmrXrm1Vw13k3xoSjv5c2eGY4uDBg+o9b/To0Xanc1Wbl+np6dKmTRu1cFWaQLtSWDJ/yVaaNm2Lo3xaWlzQavHixaLVakWj0djtDM0eJfNd1P3VGbdKm7ZTp04VwHZNrrIM2ir39MDAwBJ97qdQ2ssq/NVGadq0LYrymWBxNU4q2/XtSpcuXVI/1/T29rYILis11IcPH17kMpTry17zJ0qtcvP9qrx0tBfccqV9+/apz4/igkOu6IgsIyNDrY1lHkw5evSomg5bbfyK/PtJvJeXlzosNzdXvZ/bqyhgHhA2f84qX5kVrrmvMA8I79y5U0QK+rTQ6/USHR1tMyijTF/aoG1JyxKlCdqWJr/hSJ6wJPtbodS6Onz4sFy5ckU0Go3ceeedIvLvM0X5qkAJxhVuC1opOzjyU7ZDqSjhyK9weUYZrjQLYa82ozlX5pvNZWdny5QpU9Q2gJVfcHCwvP3221ZfEhR3PC9evKh+AfLmm2/anEapuebIr7iXX7ZcuXJFAgMDpVq1ajZrHxYVtF2xYoW6/baeQUlJSWoAxzyfuHfvXtFoNKLT6Ww2nZOamqo+TxcuXOj0NomIXL16VW3z+d5775UdO3bI1atX1WOknO+2rnEldmD+Balyb/X397f6Skhp47lwxRxFXl5ekfcMe86ePSsGg0G0Wq1TZXUlWG5eQcecI7GDkqTXlezl9YpS3H17y5YtTt2bzSnXgb2m/JTzw93d3WK4s0FbZ2MzR48eVa+xJ598Uvbs2SNpaWlquUDJcxZeT3H7qjT37O3bt8sdd9xh8azQaDRyxx13yMGDB+1ui9LUlL2XDbcCLSqhdu3a4YsvvgAATJw4EWlpaRbjv/nmGwDAu+++i7FjxyIiIgKenp7q+NOnT9tcbvXq1VGvXj1cuHABR44cwcaNGwEAHTt2tPh348aN2L17NzIyMhAbGwuj0ejaDbSxLU899RSeffZZNG7cGN7e3tBoNEVuS1nz8fEBAFy7dq1Uy1H23T333AMpeMlg97dhwwaHlhkfHw8A2LZtG/Lz80uUnpSUFIiIzWkuXrwI4N99UJi9+TIzM51Kiyv07NkTW7duRUpKCpYvX44JEybA398fS5cuRZ8+fZCXl+fwsoYMGQKtVosffvgB6enpAIBFixYBAIYNG2ZznhEjRmDv3r04f/48Fi9ejAceeADu7u744osvMHz48NJvIAAvLy/MmjULWq0WIoLffvvN4Xl//PFHvPDCC/Dy8sKKFSsQHBxsNY359Z2bm1vseRoeHu6KzVIlJSXh9ttvx6VLlzB06FB8+umndqetX78+AODUqVN2z/3jx49bTFtYTk4O+vbti19//RWNGzfGTz/9ZPdcd4ROpwMAi/To9XrUrl3bIj320hkeHq4uo6TrK+zHH3/EiBEjYDKZ8NFHH+Gee+5xePlHjhzBrl27AMBl57ByLOztC/Nx5setpMdb+X9OTg7OnTvn8Hx79uwBALz11lsIDQ21+E2cOBEAsHnzZnWYK55PkydPxscffwyj0Yg1a9agadOmJV5Wr169AAC///67xfDi9v/Zs2eRm5trMa0jHDkXK+P17UpBQUH48MMPARQ8Qw8ePKiO8/PzA1B8PkQZr0xf2MCBA+Hp6YkjR45g586dAIAFCxYAKHiGlTXz+1tZ7Udz3t7e6NSpEwDLayE8PBwGgwEA1H8LU4bfuHFDHabT6VCnTh2H5is8b8OGDZ2e78CBA8jNzcWxY8dQo0YNq3uRokWLFggNDcU777xjc9nFKWlZojTKOr9Rkv2tSEhIAFBQBtq0aRNERC0XxcfHQ6fTYePGjbhx4wa2bdsGd3d3tGvXzub2/f3338Vum3KOKvO0a9eu2HmSkpJsbteaNWtgNBoxefJkzJgxo8h95Mp8szkPDw9MnToVZ86cwaFDhzBr1iz06dMHly9fxlNPPYX33nvP4WXl5eVh4MCBOH36NO6++248/fTTNqdT9t3kyZOL3Xdz5851eptOnjyJK1eu4Nq1a2jQoIHVtahcI3379rXIJwDAli1bAACtW7e2+QwKCwtTn2Pm+fqtW7dCRNCgQQPUqlXLaj5fX1+0bt3aaj5nrFmzBlevXkVcXBzmzp2LNm3awN/fH1ptQVilqGu/R48e8Pf3x65du3D06FEA/5aVBgwYAL1ebzG9t7c3ACAjI8Pm8pTylrOqV6+O5s2bw2Qy4Y8//nB4vpCQEADAlStXSrTeokyYMAHt27d3+nfhwgWn1mMvr1cayrVUo0aNYq8le3GAlJQUm8OTk5MB2I8rOMrZ2MySJUuQl5eHoUOH4p133kHz5s3h4+NT6hhTae7Zbdu2xdq1a3H16lX89NNPeOaZZ1CzZk38/PPPuP322+1um3K+2irH3yoqZdAWAPr164e2bdviypUrVg865QRQgnfm8vLycOjQIbvLVTIgmzZtwqZNm+Dn54fmzZury9PpdNiwYQM2bdoE4N9MTFkpalsAOHUjLky5KEuiSZMmAIAdO3aUeBkA0LhxYwDA/v37S7Uccz179oTRaERycjK+++47p+aNiIiAVqtFTk6O3QL8gQMHAAANGjSwGK48eG3dlFNTU3Hp0iWn0lIcZ45fYGAg+vbti5kzZ2L//v3w8/PDnj17nMrQVKtWDZ06dUJ2djaWL1+O3NxcLFu2DID9oK0iNDQUQ4YMwezZs/Hrr7+qwd/z5887vP6i+Pj4qDdqJbhSnL/++gt33303TCYTvvrqK/U6L8zPzw/Vq1cH8O+xv1kuXLiArl274uzZs+jduzfmz5+vZhptiYmJgU6nw/Xr121mVvLy8tSAY5s2bazG5+fnY/Dgwfjll19Qt25drFu3DlWqVClx+m/cuIG//voLAFCzZk2Lccr6t27danNeZbitdBZFOUaF16fYtGkTBg4ciLy8PEyfPh3jxo1zavlff/01ACAuLg4RERFOzWuPso1Hjx5VXwqZu3DhAo4dO2YxLVDy4127dm01uFGS/Z+SkoKLFy9a/JSXp7m5ueqwwoV+Z7399tt4/fXX4eHhgVWrVqkFsZJSAmKFA2OOnovVq1e3WUi0p7hzsTJf365Ut25d9f/m93flGVxc/uHPP/8E8G/AqjBfX1/07dsXQEGg7vjx49i+fTu0Wi3uvvvuUqXdEcp54uHhgaCgoDJfH2D7WnBzc0OrVq0AFP8yrUaNGhbD4+LiHJrPYDBYnHOOzmdrnRkZGVb3IfP7p3KfshcMKU5pyhIlVdb5jdLs78LlI/NhXl5eaNmyJbZv344dO3YgPT0dMTExVpVaSpLnV+Y5dOgQTCaTw/OZi4+Pxw8//AAvLy88/vjj+OSTT4qdxxX5ZnsiIyMxevRorFy5Uk2LUhnJERMmTMDmzZsRGxuL2bNn252uLMpYtmRnZ9u8FpXjdeXKFVy8eBGpqanqPI4EI5Xg1/Xr10s9nzOUaz8uLs5mWauosrfBYMBdd90F4N9grfKvreeJ8hzbt2+fzeUpz6+SsJfnKYpSFiqL+9uff/6JrVu3Ov1z9jiWZLuLU79+feh0Opw/f77EAW17+1QZXjiu4CxnYzMljTEVF39wxT3baDSiW7dueOONN3D48GHUq1cPZ8+exZo1a6ymvXz5MpKTk+Hl5WU3n3crqLRBWwB49tlnAQAzZ860yJQpb8JtFX6/+uoru286gH+DsCtXrsTevXvRvn17tRDl7e2NFi1aYPv27Vi3bh2AfzMsZaWobfn5559LFbRVlp2dne30vP369QMAfPjhhw4HyWzp0KEDqlSpgj/++MPhmrTF8ff3x4QJEwAAjz32mN0374qtW7di27ZtAApuEsrNS6ndYy47O1vNIHXr1s1inFKwVArN5orKVJVUSY9f1apV1Vox9mrZ2aNkOBYtWqS+iW7dujXq1avn8DIaN26s1nhydP3KW0h7jh49qk7jSE241NRU9O3bF6mpqXj++ecxePDgIqdXMmDF1dBwpStXruD222/HsWPH0LlzZyxdurTYGqe+vr7o2rUrAGDOnDlW45cuXYq0tDQEBQWptVgUIoKRI0di5cqVqF69OtavX68WHktqzpw5uHbtGtzc3KzWp+zTJUuWWGXC09PTsXTpUgAFteAcdfDgQfz0008AoO4Hc7t370afPn2QnZ2N5557Tn2GOEpE1NpXrqyBFxkZiUaNGgEAvvzyS6vxyrBmzZpZZOpKerw1Gg369+9vd75t27bh8OHD0Ol0uPPOO9Xhy5cvt/u2/KuvvgIA3HbbbS6pdf7555/j6aefhk6nw9KlS63On5JYvnw5AFi9oLnzzjvh7u6OQ4cOYfv27VbzKftowIABDq8rKysLn332GQDb52Jlv74dlZ+fj6tXrxY5jfJ81mq1Fs+a22+/HRqNBklJSXYD7sePH1fH3XHHHXbXoVzPixcvVmucde7cuUyD0QBgMpnU50qnTp3g7u5epusDCp5/iYmJAKyvBeVZuGjRIps1CufNmwcA6NKli835fvjhB5sFWuX+0KFDB4ttHDRoEADg119/VV8A2JovIiJCfWHSr18/h2o4nT59GiKCqVOn2tkTRStNWcKR5drLu5VlfqMk+1th/rXhxo0b4evra3H+JCQkICsrC2+//bbF9OaUbZs5c6bd2miF1a9fH02bNsWVK1cwf/58h+axpWPHjlixYgU8PDwwfvx4m/dQe0qTby5O27ZtnVrup59+ilmzZqFq1apYvny5Re3vwnr16gW9Xo8ff/wRf//9t0vSa6558+ZFXothYWEACr7AKVybV8mr79y502Yg9uTJk2qazfM9ynxHjhyxWRMwLS1NLYeVNAhW1LV/+fLlYs8d87LSH3/8gYMHD6oVXwq7/fbbARR83WHrRXdJakADBQE5JUYQHR3t8Hzt27cHUPJaykXZsGGDQ7VUS/tFgb28Xml4eXmhW7duMJlMmDlzZomWYe9lkTK8qDyKI5yNzRR1nh8+fBirVq0qcj57zzBX3bMVXl5eaNasGQDb90nlC6m2bds69WVmhWPVYMItxF6btgqTySSNGjUSAPLWW2+pw5X279q0aWPRdtaaNWvE19dXbXDZ1nKVtlqU3oTNlyvybycfGo1GtFqt0+1yOtum7dtvvy1AQU+Ex48fV4fv3LlTatSooW5LSdq0NZlMaoP/9toJsdduSlpamnp8evXqZdXO0+bNm63aibTXdqvS4U2VKlVk2bJlVm0s/vnnn/L000871Ut7Tk6O2iFRaGiozJ8/36q32L/++kvGjRsn7u7u8v3336vDlQa0dTqdfPPNNxbbPHDgQAEg4eHhVj2aKudG8+bNLTqHUM47pX0WV7Vpq5znzzzzjM35hgwZIj/88INV+0lLly4VNzc30Wg0cuzYMZvz2nP16lUxGAzi7u6u9rI7Y8YMq+lSU1NlyJAhkpiYaNFOV35+vnzwwQcCFLQT52h70KGhoTJmzBjZsWOHVbtfGzduVHtyjY6Otjh/bLW7c+PGDenZs6cAkD59+lgtz5bTp0+rDbw//vjjcvXqVYvxly9fljlz5li1M3j69GkJCwuTsLAwp9rXy8jIUNuVatOmjVPtZm/ZskW9P5m357V37161HUNb7Z8pPUFXqVKlyHaDzKWmpsrQoUPl119/tRien58vn3/+uXp/stVOZ35+vtoG0YABAyQzM1PddqU33aZNm1odn9GjR8uKFSus2rndsGGD1K5dW4CCNpYLd+5x+PBhtZ2zcePGObR9hSltXuv1epf0TGzum2++Ua8L83ZT//e//6mdx3z77bdW85X0eB8/flxtp/Wtt95Sr5ukpCRp2LChAJCxY8c6nH5Xtmm7ZMkS0Wq1otVqZdGiRQ7P98ILL8iqVausOtVKTk6WSZMmqc9upYMbc0pbsJGRkWp7YCaTSe38y8PDw6rX73feeUc++eQTq/vBsWPH1M6tvLy8rDrD+C9c3466evWq+Pr6ylNPPSX79u2zuH+bTCZZtWqV2g6n0n6mOaUTyfDwcKs2OA8dOiTNmzcXABIXF1dkOvLy8tTO5pTtmjt3bom3y9z8+fPljTfesOqk8sKFC2oHg1qt1m5nPeYcadP27NmzMnHiRJudm27fvl0995o1a2bVJmxWVpbaTubYsWPV/EN+fr48//zz6v3vwIEDFvOZTCZp2bKlAAX9CZi3Ual0+AhA1q1bZ5Um5X4fHx9vsY++//57taO72bNnF7tvFMq6StumbUnLEsW196d0rmuvd/aS5jccadNWpHT7W8mvajQaqw6ifvzxR4vyk62OldLT09UOpYcNGybnzp2zGv/tt99adSS8evVq0Wg04uXlJV988YXV8/3YsWPy6quvyv/93/9ZDLdV3vrxxx9Fr9eLVqu16helLPLNIgX9P0yaNMnquklPT1fbRk5ISLAYZ+t4btq0SXQ6nej1eofLRErZpE6dOlbnhslkkl9//VXGjBlTou0qTlFt2p48eVLNgxTVEVnVqlXV/KFIwT5T8nJFdUTm4eFR4jaIf/vtN7UcaH7POnfunHTs2FG99u1d4zdu3JBq1aoJUNDhI2C/c9y0tDS1Q+UHH3xQLauaTCb55JNP1Guy8Hn822+/yUsvvWTzuK1Zs0bNX9vqyK0oGRkZ4u3tLd7e3lblZpGK0aZtafJ69jjSB8qePXvEYDCo/QwVjgOcO3dOZsyYYdWprXIduLu7y1NPPaWWX/Ly8tQOa318fKw63HK2TVtnYzNKG/UBAQEWHa7+9ddf0rRpU/U8L7wepTNpo9Fosy8YkZLds8eMGSOLFy+2uN5FCsr5SrzKVpu1Sr8Fb7zxhs203CoqddBW5N8OJEJDQ9Wby8mTJ9UMj6enpzRv3lzCw8MFgHTu3Nlu5wEKpeAPwKqwsnr1anVcixYtnN4mZ4O2qampagZHr9dLs2bN1AJ148aN1U53ShK0Ffm3Iw8PDw9p2bKldOzY0eKGZe/GIFJQSFQeNFqtVpo0aSLNmzdXe9ssPE9Rwclnn31W3S+BgYHSqlUradGihXoci8rg2pOenq5mTpVzoWnTptKqVSuLjgBq1qxp1bmNeXpq1aolLVu2VAMnAQEBVoVDkYKbmLI/DAaDxXn37LPPurwjMvNefhs0aCAJCQnSsWNHdT8px8FgMKjbrWQiAMsOjZzRr18/dRlardYq0y1SUBBXpvH29pbo6Ghp2bKlmtHSaDTyxRdfOLxO8x7EfXx8JCoqSlq0aCHBwcEWx/HQoUMW89l6CCsZDqAgyGuvZ8vCHVaYd4Co0+mkWbNm0qZNG6lbt65aSBkyZIjFPObrcqaDMqVzNKAgcGkvjfZ6+Dbv4K9u3boSFRWldqrRq1cvq4L6tm3bLM73onqtNmd+nP39/SUmJkZatWplcbx69OhhM+MnUvBCRunswc/PT2JjY9XzNjAw0KqAIyJqx3bKed2mTRuL6zkiIsJmj7F33HGHeu7Fx8fb3b45c+bYPS5Kj8Ul6eTQEUrgEIA0atRIfSkJFN0DsLPHWzFv3jx1uho1akhMTIzaKUFsbKxkZGQ4nHZXBm2VglxRPc+2a9dOfv/9d4v5lHulXq+Xxo0bS+vWrSUsLEx9YabX6+Xzzz+3uc60tDSJiYlRp4uJiVHPKzc3N5ud1U2cOFG9D0ZEREibNm2kYcOG6v3AaDTaDFz8V65vR1y7dk1dlvJ8jYmJkejoaPXeoOyn8+fPW81/5coVtVM1pSDZpk0btdMk5fnoSMccyvEECoLtpemgzZzSiSdQEFxu3bq1NG3aVNzc3NTnib3n4ZYtWyQoKEj9KQV4Ly8vi+HmHfKYP3cCAwOlRYsWEhMToz6/gIKKAPZ61t61a5facVNAQIC0atVKfda6ubnZDWYfP35cqlevrp77rVq1UgPugP1A8+XLl9VApsFgkNjYWDXfC1j3XF0cZb7SBm1LWpYorvD/yiuvqPsyJiZGzXebn98lyW84GrQtzf4eMWKEOl3hl0NpaWnqOa3Vaq2CzYpDhw5JnTp11OkaNWokbdq0kQYNGqjzt2nTxmq+jz76SB3v4+MjsbGx0rJlS/VlFQCrYIkyvLDly5eLTqcTNzc3i5ehZZVvVjqnAgo63mrZsqVER0eLl5eXmv/ZvXu3xTy2jqcSxAkICCjyPm5+LuXl5cnw4cPV9YeGhkrr1q0lOjpaDYQAsMo/u0JRQVsRkdmzZ6vHVHlu169fXx3m5eUlP//8s9V8P/74oxpQcnNzk/r160vjxo3VvIO7u3upX7oplXSUfGXz5s3F3d1dfHx8ZMaMGUVe4yIijz32mMWzrXA8wdy6devUtPv5+UmrVq3Ue+m7776rXivmzOMJoaGh0rJlS4mKirJ4Prdq1UpSUlKc3vYHHnhAAMh3331nNa4iBG1Lk9ezx5GgrYjIsmXL1OvWw8NDmjdvLq1bt1ZfdgLWlamU62D69Omi0WgkKChIWrVqpd7j7VVQcDZoK+JcbCYvL099OeLm5iaNGjWSpk2bikajkWrVqql5TVvrUTqI9fHxkTZt2kjHjh2tnknO3rOV8p27u7s0atRIPa7KtPY6na1fv764u7vbjEfcSip90DYnJ0e9sX388cfq8L/++kvuuusu8fPzEw8PD4mMjJSXX35ZcnJy7F4ECuXhZjQard4MpKamqiegvbdmRXE2aCtS8Obm3nvvlSpVqoher5c6derIE088IampqXaDs44GbdPT02XixIkSHh6uFtbN01bUjUGkoGfn5557Tpo0aSJeXl7i4+MjjRo1krFjx8revXstpi0uOLl161a5++67pVatWqLX6yUwMFCioqJk1KhRsnr1aqd6kDe3adMmeeCBB6RBgwZiNBpFr9dL9erVpVevXjJnzhyrN2WKVatWye233y4BAQGi1+slLCxMxowZY7O3UsWxY8dk0KBBEhAQIJ6enhITE6OeZ64O2oqILFy4UFq3bq0GlM3P6+XLl8vo0aOladOmEhgYKAaDQerVqyf9+/eXjRs3Frfb7FqyZIm6LnsBmvz8fPn6669lxIgREhkZKX5+fuLp6SkNGjSQ4cOHW50bxTl9+rTMmjVL7rrrLmnSpIn4+/uLu7u7BAUFSYcOHeStt96yWcAuLmhb1M/WOZ+cnCyTJ0+W6OhoMRqN4unpKREREdKjRw/55JNPrGpSlTRoq1y/xf2KylysWrVKunTpIn5+fuLl5SXR0dEyY8YMmwE88/tScT9zubm58tZbb0m/fv0kIiJCfH19RafTSWhoqPTq1Uu+/fZbq5rzhZ06dUoefPBBqVGjhuj1eqlRo4Y89NBDdgvcixcvlnvvvVeaNm0qVapUEXd3d7UA8+6779qttahcR8X97N0zc3Jy1CBS4do8rjRv3jxp27atGI1GMRqN0rZtW5k/f36x8zlzvM1t3bpVevfurd4jGjZsKFOnTnU6EOfKoK2j52Lhe2JiYqJMnDhRYmNjpWrVqmoBKyoqSiZOnGi3515FVlaWTJ06VRo2bCgGg0GCgoKkT58+Fj1Xm9u+fbtMmDBBWrduLdWqVRO9Xi9Go1GaNWsmkyZNsqqZq/gvXd+OOHLkiMyYMUN69eolDRs2FB8fH9HpdFK1alW5/fbb5dNPP7Wq+WYuJydHPvvsM+nUqZMEBQWJu7u7+Pn5SVxcnLz11lsO12RWalcBBbUAXeXAgQMyadIkad++vdSsWVMMBoN4eXlJw4YNZcyYMTZfTikcPXbmz5fs7GyZNWuWDB48WBo0aCB+fn7i7u4uwcHB0qVLF5k5c6bdfI8iKSlJHnzwQalZs6bodDoJDg6Wu+66q8jAg0hBnvDxxx+XunXril6vl4CAAOnevbusXbu2yPkyMzNlypQp0qhRI/Hw8BBfX19JSEhwqqa9QtknpQ3aipSsLFFc4T83N1emTJmi3mfs5RGczW84GrQVKfn+nj17tppeW7XYlNrWzZs3L3I5aWlp8sYbb0ibNm3E19dXDAaDhIeHS5cuXeSdd96xm1/6888/5cEHH5S6deuKh4eH+Pn5SZMmTWTYsGGydOlSqxpaRZW3lNqz5l/blVW++dKlSzJz5kzp06eP1KlTR7y8vMTPz0+ioqLk6aeftvlCqqigrTP3A8Xq1aulX79+EhoaKjqdTkJCQiQ2NlbGjx8vGzZscOirM2cVF7QVKXhJNHz4cAkLCxO9Xi8Gg0Hq168vY8aMkSNHjtid76+//pLRo0dLRESEGAwGtZw2YsQIqwB4SeTk5MiLL76olo9DQ0Nl6NChcvjwYYcCfDt37lSPR7169Ypd3+7du6VXr17i5+cn3t7e0qpVK1m0aJFkZGQIUBDMNXflyhX54IMP5M4775R69eqp5dtq1apJjx495KuvvrKKYTjq119/FQDSv39/q3EVIWhb2ryevWUWd0wVSUlJMnHiRImMjBRPT08xGo3SsGFD6d+/v8ybN8/qhZV5Of+nn36ShIQE8fHxEaPRKF26dLF7bylJ0FbEudhMamqqTJgwQapXry46nU5q1qwpDz74oJw7d67I9Vy4cEFGjhwpNWrUUIPlts4JZ+7Zv/zyi0ycOFGtlKVc0926dZOVK1fazG/u2rVLgILa+rc6jYgIiIiIiIiIiIiowtu9ezdatmyJ6Oho7N2796at94477kBiYiL+/vvvUvVPQEB4eDhOnjyJEydOcF+62H333Yf58+fj119/LXVnxeWtUndERkRERERERERUmSidA7Zr1+6mrvfNN9/EjRs38Prrr9/U9RI56vjx41i4cCEGDRp0ywdsAaDsu6ElIiIiIiIiIiKHJSYm4uLFi+jfvz8MBgMAIC8vDx9++CE+/fRTaLVaPPTQQzc1TTExMfjiiy+QnJwMk8kErZb1AKliOXPmDCZPnoz77ruvvJPiEgzaEhER3SRr1qzBa6+95vD03333HUJDQ8swReVr0KBBOH/+vEPT9uzZE88//3wZp4jKUmU/3ry+b01ffvklvvzyS4en37JlSxmmhm51EyZMwJ49exyaNiYmBh9++GEZp8g1KuN1cuHCBQwcONDh6SdPnowePXqUYYqsnTx5Evfffz90Oh3q1KkDX19fHDlyBGlpaQCA6dOno3nz5jc1TQDwwAMP3PR1EjkqISEBCQkJ5Z0Ml2HQloiI6Ca5ePEitm7d6vD0169fL8PUlL9du3bh5MmTDk0bERFRxqmhslbZjzev71vTqVOnnDpuREX5888/HT6f3N1vnaJ4ZbxOrl+/7tQ2Xbx4sQxTY1uHDh0wfvx4JCYm4ty5czh+/DgCAwPRsWNHjB8/HnfcccdNTxMR3VzsiIyIiIiIiIiIiIioAmEDJEREREREREREREQVCIO2RERERERERERERBUIg7ZEREREREREREREFQiDtkREREREREREREQVCIO2RERERERERERERBUIg7ZEREREREREREREFQiDtkREREREREREREQVCIO2RERERERERERERBUIg7ZEREREREREREREFQiDtkREREREREREREQVCIO2RERERERERERERBUIg7ZEREREREREREREFQiDtkREREREREREREQVCIO2RERERERERERERBUIg7ZEREREREREREREFQiDtkREREREREREREQVCIO2RERERERERERERBUIg7ZEREREREREREREFQiDtkREREREREREREQVCIO2RERERERERERERBUIg7ZEREREREREREREFQiDtkREREREREREREQVCIO2RERERERERERERBUIg7ZEREREREREREREFQiDtkREREREREREREQVCIO2RERERERERERERBUIg7ZEREREREREREREFQiDtkREREREREREREQVCIO2RERERERERERERBUIg7ZEREREREREREREFQiDtkREREREREREREQVCIO2RERERERERERERBUIg7ZEREREREREREREFQiDtkREREREREREREQVCIO2RERERERERERERBUIg7ZEREREREREREREFQiDtkREREREREREREQVCIO2RERERERERERERBUIg7ZEREREREREREREFQiDtkREREREREREREQVCIO2RERERERERERERBUIg7ZEREREREREREREFQiDtkREREREREREREQVCIO2RERERERERERERBUIg7ZEREREREREREREFQiDtkREREREREREREQVCIO2RERERERERERERBUIg7ZEVK46deoEjUZT4vmnTp0KjUaDDRs2uC5RVOGEh4cjPDy8vJPxn6DRaNCpU6fyTgYREVGlwLwukWNKe60QVUYM2hI5KCkpCRqNxuKn0+lQo0YNDB48GL/99lt5J9EuW2nX6/WoVasW7r77buzbt6/M1l1ZMppnz57Fc889hxYtWsDf3x96vR7VqlVDr169MHfuXOTm5pZ3EotUEYOeI0eOhEajwY4dO8o7KeUuPz8fH3/8MeLi4uDn56eeX23atMHjjz+OPXv2lHcSiYiokmNet2SY160YmNctudWrV6NXr14ICQmBTqdDlSpV0LRpU4waNQorVqwo7+QR/ae5l3cCiG419erVw/DhwwEAmZmZ2L17N5YuXYrly5dj/fr1SEhIKOcU2mee9oyMDOzYsQOLFi3CsmXL8MsvvyA+Pv6mp2n+/PnIysq66et1xqJFi/DAAw8gOzsbsbGxGD58OPz8/HDhwgX88ssvuP/++/H111/jf//7X3kntdKqzPv2xo0b6NGjB9avX4/q1atj0KBBCA4Oxrlz53D48GHMnDkT3t7eiImJKe+kEhHRfwDzuq7FvC5VZC+//DKmTp0KLy8v9O7dG+Hh4UhNTcWxY8fw7bff4siRI+jbt295J5PoP4tBWyInRUREYOrUqRbD3njjDTz33HN48cUXsXHjxvJJmANspf2FF17Aa6+9hsmTJyMxMfGmp6l27do3fZ3O+OmnnzB8+HD4+/tjxYoVuP322y3GiwiWL1+O2bNnl1MK/xvq1atX3kkoMwsXLsT69evRrVs3rFq1CjqdzmL8hQsXcO7cuXJKHRER/dcwr+tazOtSRZWUlIRXXnkFtWrVwo4dO1C9enWL8dnZ2fj111/LKXVEBLB5BCKXeOCBBwAAu3fvthr35Zdfom/fvggPD4eHhwcCAwPRrVs3q0zjlStX4Obmhn79+lkM37Vrl/qZ15kzZyzGtWnTBj4+PsjPzy9x2idMmKCuBwDOnTuHKVOmoG3btggJCYHBYEB4eDjGjRuH5ORkq/mVz36OHz+O999/H02aNIHBYMDIkSPRqVMnvPzyywCAzp07q9th/ulSUW0XrVy5Et26dUNQUBA8PDwQHh6OESNGYP/+/Q5t2759+zB06FBUq1YNer0eYWFhmDBhAi5fvuzQ/Ddu3MAjjzwCk8mEJUuWWGVigYL2P/v3749ly5ZZDM/Pz8f777+P6OhoeHp6ws/PD507d8bq1autllHUZ3Vz586FRqPB3Llz1WHKJ4AjR47E8ePHMXDgQAQEBMDb2xtdu3bFH3/8YTXtyZMncfLkSYvPBs0LNf/3f/+Hjh07IiQkBB4eHqhVqxa6d++O5cuXO7SvypqtT97M99uSJUvQokULeHp6olq1anj00UeRnZ1tc1mbNm1Cnz59UKVKFRgMBtSvXx8vvPCCVS2Y3NxcfPjhh+jWrRtq1aoFg8GAkJAQ3HXXXTabKzA/VqtXr0aHDh3g4+NT7Kd627dvBwCMGTPGKmALAKGhoWjRooXV8PT0dLzyyiuIioqCt7c3/Pz8EBMTgxdffBF5eXnqdN9//z2GDRuGiIgIeHl5wc/PDx06dMD//d//FZmuwnJzc/Hee++hRYsW8Pb2ho+PDzp06ICVK1c6tRwiIrr1MK/LvC7zumXLmf0JAFlZWXj66adRq1YteHh4oGnTpvjiiy+wYcMGq223Z+fOnTCZTLjrrrusArYA4OnpabOfAxHBvHnzkJCQAH9/f3h5eaF+/foYM2YMTp06pU63e/dujB8/Hk2bNoWfnx88PT3RrFkzvPHGGxZ5VUesWLECt912GwICAtTtfeedd3Djxg2nlkN0q2FNWyIXcne3vqQeeeQRREdHo2vXrggODsbZs2exfPlydO3aFcuWLVM/NwkMDERUVBQ2btwIk8kErbbgnYp55iYxMREjRowAUBCw+f3333H77bfbXK+jCmciN23ahHfffRe33XYb2rRpA51Ohz179uDTTz/F2rVr8fvvv8PPz89qORMmTMCOHTvQq1cv9O7dG1WrVlUf8hs3bsR9992nZmD9/f2LTdfTTz+Nt99+G4GBgejXrx9CQkJw+vRprF+/HrGxsWjatGmR869cuRKDBw+Gm5sb7rzzTtSqVQsHDx7ERx99hLVr1+LXX39FQEBAkctITEzE8ePHER8fj9tuu63IaQ0Gg/p/EcGQIUOwbNkyNGjQAI888ggyMzOxZMkS9O7dGx988AEeffTRYvdBcZKSktCmTRs0btwYo0aNwrFjx7BixQp07twZhw4dQtWqVeHv748pU6ZgxowZAIDHHntMnV85Pp9++inGjRuHatWqoX///ggKCsL58+exc+dOLF++3KpwVdF8/PHHWLNmDfr27YtOnTrhp59+wocffojLly/jm2++sZj2s88+w7hx4xAQEIA+ffogODgYu3btwmuvvYbExEQkJiZCr9cDKChcPvbYY+jQoQN69uyJgIAAHD9+HCtXrsSaNWuwadMmtGrVyio9S5cuxc8//4zevXtj3LhxSE9PLzL9gYGBAICjR486vM2XLl1Cx44dcfDgQTRv3hxjxoyByWTC4cOH8eabb+LJJ59Ur7PnnnsOer0e7du3R7Vq1ZCSkoKVK1di4MCBmDlzplqYLUpOTg66d++ODRs2ICYmBg888ADy8vKwevVq9O3bFx9++CHGjx/vcPqJiOjWxLwu87oA87qu5uz+vHHjBnr37o3ExERER0fj7rvvxpUrV/Dkk0861ZlsSfKgIoJhw4bh22+/RY0aNTBs2DD4+voiKSkJ3377Lbp3767WLv/iiy+watUqJCQkoGfPnsjKysKGDRvw3HPPYdeuXQ5XIHj++ecxffp01KxZEwMGDICvry82bdqEp556Cr/++iuWLl3qcPqJbjlCRA45ceKEAJBu3bpZjZs2bZoAkF69elmNO378uNWwc+fOSfXq1aV+/foWwx9//HEBILt371aH9ejRQ5o1ayYhISFy//33q8NXr14tAOStt94qVdonT54sAKRTp04iInLx4kVJT0+3mm7evHkCQF599VWL4ffdd58AkJo1a8rJkyet5psyZYoAkMTERJtp69ixoxS+FSnb1qxZM7l06ZLFuLy8PLlw4UKRy7906ZL4+vraTNPChQsFgIwfP95mesxNnTpVAMgLL7xQ7LTm5s+fLwCkY8eOkpOTow4/ffq0hISEiE6nszgvitpHX331lQCQr776Sh2mHE8A8sYbb1hM/8ILLwgAmT59usXwsLAwCQsLs5neFi1aiF6vl+TkZKtxhfe/Kynnzvbt24ud1lb6lf3m5+cnhw8fVodnZWVJgwYNRKPRyNmzZ9XhBw4cEHd3d4mJiZHLly9bLGv69OkCQN555x112PXr1+XMmTNWadm/f78YjUbp2rWrxXDlWGk0Glm3bl2x26T47bffxM3NTQwGgzzyyCPy448/WpzjtgwaNEgAyPPPP2817sKFC5KXl6f+fezYMatp0tPTpVmzZuLn5yeZmZkW45Rz19zzzz8vAGTq1KliMpnU4WlpadKyZUvR6/UW+5qIiG49zOsyr+so5nUd42he19n9OXv2bAEgd955p9y4cUMdfujQIfHw8BAAMmXKlGLTl56eLjVr1hQA0rdvX1m0aJEcPXrUIq9X2McffywA5LbbbpOsrCyLcVlZWRZ57KSkJMnPz7eYxmQyyahRowSAbNmyxWKcrWvl559/FgDSo0cPizyryWSSMWPGCAD57rvvit1WolsVm0cgctLRo0cxdepUTJ06FU899RQ6deqEF198ESEhIXj77betpq9Tp47VsGrVqmHAgAH4+++/cfLkSXW48mb0l19+AVDwmcyWLVvQpUsXdOrUSR0OQP3kzJm3qeZpnzRpEtq3b4/XXnsNHh4eeP311wEAISEhMBqNVvOOGDECvr6+WL9+vc1lP/XUUy5rs+vjjz8GAHzwwQcICgqyGOfu7o6qVasWOf/8+fORlpaG6dOnW6Vp2LBhaNGiBRYvXlxsOi5cuAAAqFmzpjPJVz/veuutt9Ram8pyHn/8ceTl5VnVAC2JOnXq4KmnnrIYpny+qHwC6CidTmfz0/zC+78imjhxIho2bKj+7enpiWHDhkFELD7jnDVrFvLz8zFz5ky1ZoHi6aefRnBwMBYtWqQOMxgMqFGjhtX6mjRpgs6dO2PTpk02P+3q168funbt6nD6Y2Nj8dVXX8FoNOLjjz9Gz549ERoailq1auH++++3+hT14sWL+O6771CvXj2bn75VrVrVokZS3bp1raYxGo0YOXIkUlNTiz1XTCYTPv30U0REROCll16yqLHk4+ODl156Cbm5uVafTRIR0a2JeV3mdYvDvK5rObs/FyxYAACYNm2aWmMdACIjI3Hfffc5vF6j0Yjly5ejcePGWLFihdqclvJF2vfff281z8cffww3Nzd8+umn8PT0tBjn6elpkccOCwuDm5ubxTQajQaPPPIIANi91sx99NFHAAry8V5eXhbLeeONN6DRaCzy70SVDZtHIHLSsWPH1LarFCEhIdi8eTMaNGhgNf3x48cxffp0/PLLLzh79ixycnIsxp87dw5hYWEAgISEBGi1WiQmJmLSpEn47bffkJ6ejs6dO+P8+fNYsmQJTpw4gTp16iAxMRG+vr4227p0JO06nQ5Vq1bF3XffjWeffRbNmjVTp1u2bBlmzZqF33//HVevXrVoK8heh0itW7d2OB3F2blzJwwGAzp27Fii+Xfs2KH+a+tzn+vXr+PSpUu4dOkSqlSpUqq02rJnzx54enra3CdKwWPv3r2lXk90dLRFRg34N9N97do1h5czePBgPPvss2jatCmGDh2KTp06oX379g592gcUZDSTkpIshvXr1w/Nmzd3OA2lYesasLUflPPip59+splJ1Ol0OHz4sMWwvXv34q233sKWLVtw4cIFqyDtpUuXUK1aNYthJbkWRowYgUGDBmHdunXYsmULdu/ejW3btmHu3LmYP38+Pv74Y4wZMwYA8Ntvv0FE0LlzZ5uFj8KSk5PxxhtvYM2aNTh58qRVW7/FdXL2119/4erVq6hevbrVvQ8AUlJSAMBq3xER0a2JeV3mdYvDvK5r87rO7s8//vgD3t7eiIqKspo+Pj4es2bNcnjdsbGx2L9/P7Zv347ExETs3r0bW7ZswQ8//IAffvgB99xzD77++mtoNBpkZmbi4MGDiIiIQP369Ytddm5uLj766CMsXrwYhw8fRkZGBkREHe9IR7s7duyAt7c35syZY3O8p6cn86BUqTFoS+Skbt264aeffgJQEKyYN28ennnmGfTr1w87d+60eHN/9OhRtG7dGmlpaejcuTP69OkDX19faLVabNiwARs3brTI2Pr7+yMmJgabN29Gfn4+EhMTodVqkZCQoHaMkJiYiMDAQOzZswc9e/a0envpaNrteffddzFp0iQEBwfjjjvuQM2aNdW3qDNmzLDKiCuKqxHgjGvXrqFGjRpWmTRHXblyBcC/tRjsyczMLDIjGxoaCgA4e/asU+tPS0tDrVq1ilxmamqqU8u0xVZ7a0oNS2ca5X/66acRFBSEzz77DO+99x7effdduLu7o2fPnpgxY4bNGjTm5s6da9WTdHh4+E0L2jq6H5Tz4rXXXnNoudu2bUOXLl0AAHfccQfq168Po9EIjUaD5cuX448//rB5PZT0WvDw8ECfPn3Qp08fAAUFrnfeeQcvvvgiJk6ciH79+iE0NFQtpNiqBVzYlStX0KpVK5w6dQrt2rVD165d4e/vDzc3N+zduxcrVqywe02bLwMADhw4gAMHDtidLjMz08EtJSKiiox5XeZ1i8O8rmvzus7uz6KmL8l5qtFoEB8fj/j4eAAF7dauWLEC9957L7755hsMGDAA/fv3dyoPCgADBw7EqlWr0KBBAwwZMgQhISHQ6XS4du0aPvjgg2LzoEDBuZ6fn2+z4oCCeVCqzBi0JSqF4OBgTJo0CampqXj11VfxwgsvqI3gA8D777+Pq1evYsGCBbjnnnss5h0zZozVwx8o6Hl29+7d2L17NzZs2IDmzZsjICAAAQEBqF69OhITExEcHAyTyYTOnTu7dHvy8/Mxbdo0VK9eHXv37kVwcLA6TkTw1ltv2Z3XXq+4JeHv748LFy5YdFLhDF9fXwDAn3/+WWwnDkVp164dAOB///sfXnnlFafWf/HiRZvjlOFKGgGo22irZ2RXZHiLo9Fo8OCDD+LBBx/E5cuXsXnzZixatAhLlizB33//jT///LPIApOtnoArImWfp6WlwcfHp9jpX3vtNeTk5GDLli3quaDYsWOHRc/F5lx1LXh4eOCFF17AunXrsGnTJmzduhUDBgxQa4U4UsCaM2cOTp06hVdffRWTJ0+2GPfGG29gxYoVxS5D2W8DBgzAd9995/yGEBHRLYt53X8xr2u5fuZ1XcfZ/enr66t+6WRv+tLQaDTo168fHn/8cbzyyiv45Zdf0L9/fzWI7kgedNeuXVi1ahW6deuG1atXW+zfHTt24IMPPnAoLb6+vtBoNLh06VLJNoboFsc2bYlc4Pnnn0f16tXxySefWHw6c+zYMQDAnXfeaTG9yWTC1q1bbS5L+QRm7dq12Lp1q1rTDyjI5Co93JtP6yqXLl1Camoq2rZta5GJBQo+yS78WbUjlAe0M2/DW7dujZycHJsZfUe0adMGALB9+/YSza/o3Lkz6tati23btqn73B7zN8UxMTHIzs7Gzp07raZTtsn8zbzSs6+tDNCePXtKknQrbm5uDh2DoKAg9OvXD99++y26dOmCQ4cOOdWjbEWmnBfKJ4XFOXbsGAIDA60CtllZWfj9999dnj57vL29Lf5u2bKl+mmprTZ1zdm7BwHA5s2bHVp/o0aN4Ovri99++63Y9RERUeXEvK59zOtaYl7Xec7uz+joaGRmZmLfvn1W02/bts1l6SqcBzUajWjcuDFOnDiBv//+u8h5lXtDr169rALijuZBgYJz/fLly8Wuj6iyYtCWyAU8PT3xzDPPIC8vD9OmTVOHK+13bdmyxWL6N998E/v377e5rA4dOsDNzQ0fffQRMjMzLWoYdO7cGWfPnsWCBQvg7+/v8s/PQ0JC4Onpid9//x1ZWVnq8KtXr2LChAklWqbSGP2ZM2ccnkdpnH7ixInq51+K/Pz8Yt8g33///fDx8cHkyZNtfs6dlZXlUODOzc0NH3/8MbRaLQYPHmzROYa5VatWYeDAgerfSgcAzz33nEWQ6+zZs3jvvffg7u5uURulZcuWAAo6lTCZTOrw7du3u6QTB6DgOFy6dAnXr1+3Grd27Vqrmg95eXnqvi/cycCtaty4cXB3d8eECRNw+vRpq/HXrl2zKDiEhYXh6tWrFufQjRs3MGnSJLu1G0pi8eLF+OWXXyza+FJs27YNGzZsgLu7O9q2bQug4LO3AQMG2GxzEChow1Y5nvbuQQsXLsSPP/7oUPrc3d0xduxYnDx5EpMmTbIZuN2/f7/6WSsREVU+zOvax7wu87ql5ez+VP7/4osvWuzPw4cPY968eQ6vd+fOnZg/f77NfZacnIzZs2cDANq3b68Of+SRR3Djxg2MGzfO6iXH9evX1X1q795w4MABTJ8+3eE0PvroowCAUaNG4fLly1bjL1y4gEOHDjm8PKJbDZtHIHKR0aNH480338T8+fPx/PPPo169ehgzZgy++uor3HXXXRgyZAiCgoKwY8cO/P777+jVqxdWr15ttRxfX1/ExsZi586dcHNzQ4cOHdRxSqY2JSUFffv2LXE7WPZotVqMGzcO7777LqKjo9GnTx+kpaVhzZo1CAsLQ/Xq1Z1eZufOnaHRaDB58mQcPnwYfn5+8PPzw9ixY+3O07NnT0yaNAnvvPMO6tevj/79+yMkJARnz57F//73P0yaNAmPPfaY3fmDg4OxaNEiDBo0CNHR0ejevTsiIyNx/fp1nDx5Ehs3bkR8fHyxbZ4BQPfu3fH111/jwQcfxG233YaWLVsiLi4OPj4+uHjxIjZs2IBjx46ha9eu6jwjRozAsmXLsGLFCkRFRaF3797IzMzEkiVLcPnyZbz77ruoW7euOn3btm0RFxeHX375BXFxcUhISMDJkyexcuVKuz23OqtLly747bff0KdPH3To0AF6vR7t27dH+/btMWTIEHh5eaF9+/YICwtDXl4e1q1bh4MHD2LIkCEu6ynZnmnTplnVdlG88sorLlt/06ZN8cknn2Ds2LFo2LAhevbsiXr16iEtLQ3Hjx/Hxo0bMXLkSHz22WcAgAkTJuDnn39G+/btMXjwYHh4eGDDhg04e/YsOnXq5LJP5ZRPxGrUqIGEhATUrl0bubm5OHjwINatWweTyYQ33njDov2wTz75BPv378drr72GH3/8EV26dIGI4MiRI/j5559x8eJF+Pv7Y8SIEXjzzTcxYcIEJCYmIiwsDPv27cP69etx1113YdmyZQ6l8eWXX8bvv/+OmTNnYvXq1ejYsSOCg4Nx9uxZ/Pnnn/jjjz+wfft2hISEuGSfEBFRxcO8rm3M6zKvW5zi8rrO7s/7778fX3/9NVauXInY2Fh069YNV65cweLFi3H77bdj1apVDl07586dw3333Yfx48cjISEBkZGRcHd3R1JSEn744QdkZmaiV69eGDRokDrP2LFjsXHjRixZsgT169fHnXfeCV9fX5w6dQpr167FnDlz0K9fP7Ru3RqtW7fGkiVLcP78ebRt2xanTp3CypUr0atXL4eb3OrevTtefPFFTJs2DREREejevTvCwsJw+fJlHD16FJs3b8arr76KRo0aObQ8oluOEJFDTpw4IQCkW7dudqf58MMPBYCMGDFCHZaYmCjt2rUTHx8f8ff3l549e8ru3btlypQpAkASExOtlvPMM88IAGnTpo3VuLCwMAEg77//vkvTrsjNzZXXXntN6tevLwaDQWrXri1PPPGEpKenS1hYmISFhVlMf9999wkAOXHihN1lzp07V5o1ayYGg0EAWCyjY8eOYu9W9H//93/SuXNn8fPzE4PBIOHh4TJixAjZv3+/Ok1R+/Hw4cPywAMPSFhYmOj1egkICJBmzZrJo48+Kjt37ix2X5g7c+aMPPPMMxITEyO+vr7i7u4uVatWle7du8uXX34pubm5FtPn5eXJO++8o263j4+PdOzYUVasWGFz+SkpKTJixAgJDAwUT09Padu2raxdu1a++uorASBfffWVOq1yPO+77z6bywIgHTt2tBiWnp4uDz30kFSrVk20Wq0AkClTpoiIyCeffCJ33nmnhIWFiYeHhwQFBUmbNm1k1qxZkpeX59R+coZy7hT127Nnj4iIzXOvqGNva78pdu7cKUOHDpXq1auLTqeTKlWqSIsWLeTZZ5+VQ4cOWUz73XffSYsWLcTLy0uqVKkigwcPlmPHjtk874taZ1FOnTolM2fOlN69e0tERIR4e3uLXq+X2rVry6BBg+R///ufzflSU1PlxRdflMjISDEYDOLn5yfNmzeXl156yeJ83Lt3r9xxxx0SEBCgnofr16+3m15b54+ISH5+vsyaNUvatWsnvr6+6v2he/fu8umnn0pGRoZT201ERBUL87rM6zKv61rO5HWd3Z8ZGRny5JNPSvXq1cVgMEjjxo3l888/l++++87h6yctLU0WLFggI0aMkCZNmoi/v7+4u7tLcHCw3HbbbTJnzhzJz8+3ms9kMsns2bOlbdu24u3tLV5eXlK/fn0ZM2aMnDp1Sp0uOTlZRo0aJdWrVxcPDw9p1qyZfPzxx3L8+HGbx7eoa2XdunXSp08fCQ4OFp1OJ6GhoRIXFyfTpk2zWCdRZaMRsfE9JhERERERERER3TJeeOEF9UusHj16lHdyiKiUGLQlIiIiIiIiIrpFnD9/HtWqVbMYdvDgQbRt2xZubm44e/YsvLy8yil1ROQqbNOWiIiIiIiIiOgWMXbsWCQlJaF169YICAjAsWPHsGrVKuTl5WHOnDkM2BJVEqxpS0RERERERER0i/jmm2/w2Wef4dChQ0hNTYXRaESrVq3w5JNPolu3buWdPCJyEQZtiYiIiIiIiIiIiCoQbXkngIiIiIiIiIiIiIj+xaAtERERERERERERUQXCjsj+YTKZkJycAk9PD2g0mvJODhEREVGlICJIT09H9erVodWyvoArMN9KREREVDYqUt6VQdt/JCenoFq10PJOBhEREVGldPr0adSsWbO8k1EpMN9KREREVLYqQt6VQdt/+Pj4oNvgwfhyxgwYvb3LOznkYiaTCSkpKQgODi73NyXkejy+lR+PceXG41u5Xbt2DWFhYfDx8SnvpFQaSr517gcz4eXlWd7JISIqE8wfEFF5SEtLQ61atSpE3pVB239oNBrodHr4+voyaFsJmUwmXL9+Hb6+vnzgV0I8vpUfj3HlxuNbuZlMJgDgZ/wuZJ5vZdCWiCor5g+IqDxVhLwr73xEREREREREREREFQiDtkREREREREREREQVCIO2RERERERERERERBUIg7ZEREREREREREREFQiDtkREREREREREREQVCIO2RERERERERERERBUIg7ZEREREREREREREFQiDtkREREREREREREQVCIO2RERERERERERERBUIg7ZEREREREREREREFQiDtkREREREREREREQVCIO2RERERERERERERBUIg7ZEREREREREREREFQiDtkREREREREREREQViHt5J4CoIkrPyEBGZqbd8UZvb/gYjTcxRUREREREROQqLPMRUUXHoC2RDbv37cOm7dthMgmOnjgBAIioUwdarQYAkBAXh07x8eWZRCIiIiIiIiohlvmIqKJj0JbIhtioKDSsVw95eXn4bP58AMCoYUOh0+kAFLx1JSIiov+e/YcPY9nq1Th2IglXrl3D849NRFzLlur4PsNH2Jzv/qFDcVfvXgCA5159DfsPH7YY36FtGzw9fnzZJZyIiCywzEdEFR2DtkQ2+BiN8DEakZubCw+DAQAQGhICvV5fzikjIiKi8nQ9Jwd1atdG14QETP9gptX4+R99aPH37j/2Yebs2Yhv3cpieLfOnXDPgAHq38xjEBHdXK4s87GpBSIqCwzaEhERERE5qGV0NFpGR9sdH+Dvb/H3jt93o1mjRggNCbEYbtAbrKYlIqJbE5taIKKywKAtEREREVEZuJqait/2/oHHHh5tNW7Dtm1I3LoV/n5+iI2OwrD+/eHl6WlzOTk5OcjJyVH/zsrOBgCYxASTyVQ2iSciKmcmkwkiUub3OZPJBMi//y/J+mKaNkX9OnWQl5eHz79eAAAYOWSwRVMLvF8T3Roq0rXKoC0RERERURn4ZfNmeHp4IN6szVsA6NQuHlWDgxHg54eTZ85g3pIlSDp1CtOefdbmcqZPn46XX35Z/dvdXYfuQ4YgJSUFnh4eZboNRETlxWQyITU1FSICrVZbZuvJy8tTX4ylpKSogVZnuWk0MGk00Jj97aYp+Cs7KwvZWVmuSC4RlbH09PTyToKKQVsiIiIiojKwbuMmdIqPt2ofsVvnzur/w2rVQvXQUDz+4ks4eiIJEXXCrZbz3HPP4YknnlD/zsrOxkNPTkJwcLDd2rlERLc6k8kEjUaD4ODgMg3a5ubmwvBPm7bBwcGlamPclcsiovLhUYFeiDNoS0RERETkYgcO/4Wz58/jmfGPFDttvfBwuLu54fzFCzaDtgaDQQ0CAAU1bQFAq9GWaSCDiKi8aTQaaLVle6/TarVQqseWdl2uXBYRlY+KdN0yaEtERERE5GI/b9yAiDp1UCcsrNhpT505g/wbN9gxGRFRBZOekYGMzEy7443e3vAxGm9iiojov4RBWyIiIiIiB2Vfv47zFy+qf19MScHxkydh9PZGSJUqAICsrGxs3bkTD9x9t9X85y9exIZt29AyOhq+Pj44ffYs5ixchLphYWjUoMFN2w4iIire7n37sGn7dphMgqMnTgAAIurUgVZbUJ02IS4OneLjyzOJRFSJMWhLREREROSgo8dP4PnXX1f/nvPNQgBAlw7t8fjDDwMANu3YDpGCwnxh7u7u+OPAAaxa+zOyr19HcGAgWjZvjmF39YdbBfocj4iIgNioKDSsVw95eXn4bP58AMCoYUPVzsqM3t7lmTwiquQYtCUiIiIiclCzxo2wasHXRU7TvUsXdO/Sxea44KAgvPHCC2WRNCIicjEfoxE+RiNyc3Ph8U/b4qEhIexgjIhuCr7OJyIiIiIiIiIiIqpAGLQlIiIiIiIiIiIiqkAYtCUiIiIiIiIiIiKqQBi0JSIiIiIiIiIiIqpAGLQlIiIiIiIiIiIiqkDcyzsBRLeS9IwMZGRm2h1v9PaGj9F4E1NERERERERERESVDYO2RE7YvW8fNm3fDpNJcPTECQBARJ060Go1AICEuDh0io8vzyQSEREREREREdEtjkFbIifERkWhYb16yMvLw2fz5wMARg0bCp1OB6Cgpi0REREREREREVFpMGhL5AQfoxE+RiNyc3PhYTAAAEJDQqDX68s5ZURERERERHSrY5N8RKRg0JaIiIiIiIiIqAJgk3xEpGDQloiIiIiIiIioAnBVk3yssUt062PQloiIiIiIiIioAnBVk3yssUt062PQloiIiIiIiIioEmEn2kS3PgZtiYiIiIiIiIgqEXaiTXTr05Z3AoiIiIiIiIiIiIjoX+Ve03bpypXYtus3nD1/Hnq9DpH162PkkKGoWb2aOo2IYNGy77E2MREZmZloUK8exoy8D2E1a6rT5OXl4cuFi7Bx+3bk5uUiunETjB05ElWCAstjs4iIiIiIiIiIiIhKpNxr2u4/dBi9bu+Kt6dOwbRnnsGNGya89OabuH79ujrN//2wGsvXrMHD992L9155GQH+fnjpjTeRlZ2tTvPFggXY/ttveHr8I3jzxRdxPec6Xnn3Xdwwmcpjs4iIiIiIiIiIiIhKpNyDti8/8zS6JiQgrGZN1AkLw2OjH0LK5cs4mpQEoKCW7cqffsLgvn0R36oVwmrVwuMPP4yc3Fxs3LYdAJCZlYV1GzbigXvuRvOmTVEvPBxPjB2Lk6dP44/9+8tx64iIiIiIiIiIiIicU+5B28Iyswpqz/r805PhxZQUXE1NRUyzpuo0Op0OTSMjcfjvvwEAR0+cQP6NG4hp1kydJiggALVr1cShf6YhIiIiIiIiIiIiuhWUe5u25kQEc775Bo0bNEBYrVoAgKvXrgEA/P38LKb19/NF8qXLBdOkpsLd3R3GfwK96jS+frh6LdXmunJycpCTk6P+rTS1YDKZYGKTCpWOyWSCiFgd2/SMDGRkZtqdT6/TA/LvMpT5TSaTzeFUPuwdX6o8eIwrNx7fyo3HlYiIqPiyp9HbGz5G401MERFVdBUqaPvZvHlIOn0ab774otU4DTQWf4ug0BBbBBqN7ammT5+Ol19+Wf3b3V2H7kOGICUlBVlF3Ejp1mQymZCamgoRgVb7bwXzX3/fg5179sAkgpNnzgAAwmrWhPaf8yY2qpka3E9JSYFOpwNQ0PGdreFUPuwdX6o8eIwrNx7fyi011fYLdCIiov+S3fv2YdP27TCZBEdPnAAARNSpA622oOyZEBeHTvHx5ZlEIqpgKkzQdta8+dj5+x5Mf2EyqgQFqsMD/P0BAFdTryEwwF8dnpqWpta+DfDzQ35+PjIyMy1q215LS0Nk/fo21/fcc8/hiSeeUP/Oys7GQ09OQnBwsFWNXbr1mUwmaDQaBAcHWwQEOrVvh5YxzZGXl4fPv14AABg9YrgahNXr9Nj/1xEAQHBwMPR6PQAgNzcXBoPBajiVD3vHlyoPHuPKjce3cuMzkoiICIiNikLDevWQl5eHz+bPBwCMGjZULXsyDkFEhZV70FZEMGv+fGz/bTemT34eoSEhFuOrBgcjwM8Pe/fvR73wcABAXn4+9h8+jPuGDAFQ8HbK3c0Ne/7cjw5t2wAArly9hlOnz+D+oUNtrtdgMKhBN6Cgpi0AaLVaFhgrKY1GY3V8/Xx94efri9zcXHh4FJwP1UNDLYKzSpVu83m1Wq3N4VR+bB1fqlx4jCs3Ht/Ki8eUiIgI8DEa4WM0FpQ9/4lFhIaE8OUmEdlV7kHbT+fOw6bt2zH58cfg6eGhtmHr5eUFg14PjUaDO7t3x9KVq1C9aiiqh1bFkpWrYNDr0TE+DgDg7eWF2zt1xJcLF8LXaITR6I0vFy5CWK1aiG7atIi1ExEREREREREREVUs5R60XfO//wEAnn/tdYvhE0c/hK4JCQCAAb17ITc3F5/OnYuMrCw0qFcXrzzzNLw8PdXpH7znHrhp3fDmRx8hJzcX0U0a47GHn4Aba3cQERERERERERHRLaTcg7arFnxd7DQajQZ3D7gLdw+4y+40er0eD993Lx6+715XJo+IiIiIiIiIiIjopmI1VCIiIiIiIiIiIqIKhEFbIiIiIiIiIiIiogqEQVsiIiIiIiIiIiKiCoRBWyIiIiIiIiIiIqIKhEFbIiIiIiIiIiIiogqEQVsiIiIiIiIiIiKiCsS9vBNAVJmlZ2QgIzPT7nijtzd8jMabmCIiIiIiIqLKjeUwIqoMGLQlKkO79+3Dpu3bYTIJjp44AQCIqFMHWq0GAJAQF4dO8fHlmUQiIiIiIqJKheUwIqoMGLQlKkOxUVFoWK8e8vLy8Nn8+QCAUcOGQqfTASh4w0tERERERESuw3IYEVUGDNoSlSEfoxE+RiNyc3PhYTAAAEJDQqDX68s5ZURERERERJUTy2FEVBmwIzIiIiIiIiIiIiKiCoRBWyIiIiIiIiIiIqIKhM0jEBERERE5aP/hw1i2ejWOnUjClWvX8PxjExHXsqU6/v1Zs/DL5i0W8zSsVw/vvDxV/TsvLw9fLlyEjdu3IzcvF9GNm2DsyJGoEhR4szaDiIiIiCo4Bm2JiIiIiBx0PScHdWrXRteEBEz/YKbNaVpEReGx0Q+pf7u7W2a5v1iwADt/34Onxz8CH6MRXy5ciFfefRfvvzoNblp+CEdEREREDNoSERERETmsZXQ0WkZHFzmNTueOAH9/m+Mys7KwbsNGPDF2DJo3bQoAeGLsWIx6dCL+2L8fLaKiXJ1kIiIiIroFMWhLRERERORC+w8dxvBx4+Dt5Y2mkZEYMWgg/P38AABHT5xA/o0biGnWTJ0+KCAAtWvVxKG//2bQloiIiIgAMGhLREREROQyLaOj0b51G4RUCcLFlBQs+O7/MHn6dMyYNg06nQ5XU1Ph7u4Oo7e3xXz+vn64ei3V5jJzcnKQk5Oj/p2VnQ0AMIkJJpOp7DaGiKgcmUwmiEip7nMmkwmQf/9va1mumqY8luUIVy2H6L+iIl0jDNoSEREREblIh7Zt1f+H1aqFiDp18cBjj2HX3r2Ib9WqiDkFGo3G5pjp06fj5ZdfVv92d9eh+5AhSElJgaeHh6uSTkRUoZhMJqSmpkJEoC1he995eXnqS6+UlBTodLoym8bVyzp89CiOJSUhNzcXM7+YjfhWLRERHl7MFpcsTUT0r/T09PJOgopBW/pPSM/IQPKlS7hh54Gv54OLiIiIykBggD+Cq1TBuQsXAQABfn7Iz89HRmamRW3ba2lpiKxf3+YynnvuOTzxxBPq31nZ2XjoyUkIDg6Gl6dn2W4AEVE5MZlM0Gg0CA4OLnHQNjc3FwaDAQAQHBwMvV5fZtO4clmH/v4bK9b+jMzsbLhptTh2MgkXU1Iw6p670cjOs8IeR9NORAU8KtALcQZt6T/h931/Yv3GjdDpdDialAQAiKhTB1ptQY2WdkXWfCEiIiIqmbT0dFy6cgWB/3RMFlGnDtzd3LDnz/3o0LYNAODK1Ws4dfoM7h861OYyDAaDWuAGCmraAoBWoy1xIIOI6Fag0Wig1Zb8XqfVagHNv/+3tRxXTePKZW3ctg1Z2dkw6PXQaDQIDamKiykp2LhtO5o0bFjMVpcs7URUoCJdIwza0n9Ci6hmCPL3g6+vLz5fsAAAMGrYUPXTEL1Oh627dpVnEomIiOgWkH39Os5fvKj+fTElBcdPnoTR2xs+RiMWLluGdq1aIcDfH8kplzB/6RL4Go1o2zIWAODt5YXbO3XElwsXwtdohNHojS8XLkJYrVqIbtq0vDaLiIgqkHMXLsLTwwOZWVkACoLXnh4GnLtwoZxTRkQ3E4O2VKmkZ2QgIzPTarjSkLS3lzc8/qmpEhoSon4akpube/MSSURERLeso8dP4PnXX1f/nvPNQgBAlw7tMe7++3Hy9BkkbtmCzMwsBPj7o1njRnh6/HiLZgwevOceuGnd8OZHHyEnNxfRTRrjsYefgFsFqtlBRETlp3poVfxx4CBECto7FxFkX89Bg7r1yjtpRHQTMWhLlcruffuwaft2mEyCoydOAPinGQSNBjk5OejUvl05p5CIiIhuZc0aN8KqBV/bHf/KM08Xuwy9Xo+H77sXD993ryuTRkRElUTndu1w9PgJXL56FW5aLS4kJ8Po7Y3OLM8S/acwaEuVSmxUFBrWq4e8vDx8Nn8+gIJmENzc3HD58mVUC62G7b/9Vs6pJCIiIiIiIrKtUYMGGDlsKGZ8/jlycnLQrFEkuiYkqB1W2vvCVKE02eMIVy6LiFyLQVuqVHyMRvgYjcjNzbVoBsHd3R1uGg18jN7FLIGIiIiIiIiofEVGRCC8Vi0AwOgRI9Sm/YAivjD9p6PthLg4dIqPd2g9rlwWEbkWg7ZERERERERERLcIe1+YKh1tG70dr6zkymURkWsxaEtEREREREREdIuw94WpeW3c8lgWEbkWu6glIiIiIiIiIiIiqkAYtCUiIiIiIiIiIiKqQBi0JQJw+OhRJJ0+jb/+PopZ87/GoSNHyjtJREREREREVMmw7ElEjmLQlv7zDh05grmLFiMtPQP5N25g/+HDmP3NQj48iYiIiIiIyGVY9iQiZzBoS/95iVu3IjM7Gwa9HjqdDqEhIcjMykLi1m3lnTQiIiIiIiKqJFj2JCJnMGhL/3nnLlyEp4cHNBoNAECj0cDTw4BzFy6Uc8qIiIiIiIiopCpaUwQsexKRMxi0pf+86qFVkX39OkQEACAiyL6egxqhoeWcMiIiIiIiIiqJ8miKoLggMcueROQM9/JOAFF569yuHY4eP4HLV6/CTavFheRkGL290bl9O4eXkZ6RgYzMTLvj9TqdK5JKREREREREDjBvikCj0SA0JAQXU1KQuHUbGjVo4PL1mQeJ3bRa7D98GEmnT+PBe+5W1+eKsicR/XcwaEv/eY0aNMDIYUMx4/PPkZOTg2aNItE1IQGR9esjNzfXoWXs3rcPm7Zvh8kkOHriBAAgok4daLUFn720a9WqzNJPRERERERElpSmCDKzsgCUfVMEjgSJiyp7EhEVxqAtEYDIiAiE16oFABg9YgT0er1T88dGRaFhvXrIy8vDZ/PnAwBGDRsK3T81bPU6Hbbu2uXaRBMREREREZFN1UOr4o8DByEi0Gg0alMEDerWK5P1ORokLm3Zk4j+O0octM3JzcXVa9eQm5cHX6MR/n5+rkwX0S3Fx2iEj9GI3NxceBgMAIDQkBD1AexojV0iIiJyPeZbiYj+e252UwQ3O0hMRJWfU0Hby1euYG3iBuzauxcnTp2CmEzqOB8fI5pGRqJTu3ZoHRMDrZZ9nNGtT2lIPud6DmbN/xpdEzqUSftHRERE5FrMtxIR/bfd7KYI2F4tEbmaQ0HbK1evYf6SJdi4bRsMBgMa1a+Pgb17w8/PF3qdDukZmbiQnIy/jh3F6zM+QHCVINw3eDAS4uLKOv1EZaaohuTrhYeXd/KIiIjIBuZbiYhIcTObImB7tUTkag4Fbcc89RTq162Lp8Y/gtYxMXB3tz/bheRkrN+4CZ/Nm4/LV6+if8+eLkss0c1UVEPyDNoSERFVTMy3EhGRM1z5dSXbqyUiV3IoaDv58ccQ3aSJQwsMDQnB8EED0b9XT1xMSSlV4ojK083ubZSIiIhKj/lWIiJyVFFfV7JZPCIqbw4FbR3N+Jrz9vJC3bAwp+cjqijYkDwREdGth/lWIqLKKz0jAxmZmXbHG7294WM0Ory8or6uZNCWiMqbUx2R2ZNy+TJOnTmL+nXrwNfHxxWLJCp3bEieiIio8mG+lYjo1rV73z5s2r4dJpPg6IkTAICIOnWg1WoAAAlxcegUH+/w8vh1pWNcHSwnIsc4HbT9eulSXM/JwUPDhwMA9u7fj2nvvof8/Hx4e3tj+guTEVazpssTSnSzFdWQfG5ubnknj4iIiIrBfCsRUeUSGxWFhvXqIS8vD5/Nnw8AGDVsKHQ6HYCC4KEz+HWlY1wdLCcixzgdtN226zf069Fd/XvB0u8QXrsWBt/ZFwuXLcOS5Svw1PhHXJpIovLChuSJiIhuXcy3EhFVLj5GI3yMRuTm5sLDYABQ0D55Sctp/LrSMa4OlhORY5wO2l6+cgXVqlYFAKSlp+Pv48cx5alJaBEVhdy8PHy5cKHLE0lERERE5CzmW4mIqChFfV1J/3J1sJyIHFOiNm3FJACAQ0f+hlarRZPISABAoL8/0tLTXZc6IiIiIqJSYL6ViIiKwq8riaii0jo7Q2hICHbu3QMA2LRjB+rXqwfDPze1K9eusVo8VWh/HT2KpNOn8dffRzFr/tc4dORIiZZz2EXLISIiorLDfCsRERER3aqcrmnbvUtnfDZvPhK3bEVmZiYefeghddyhI0dQq0YNlyaQyFWOJiXh/1b/iLT0DLhptdh/+DCSTp/Gg/fcjXrh4Q4v59CRI5i7aLFTy1GCvDnXczBr/tfomtABjRo0sJqOvXISERG5DvOtRETkCo6W54iIXMnpoG3Prl1h9PbGob//RoO69Swa6M7Ny8VtHTq4NIFErrJj925kZWfBoNdDo9EgNCQEF1NSkLh1m1NB28StW5GZne3wcooK8hZ+0LNXTiIiItdhvpWIqGK4lSunOFOeIyJypRK1aZsQF4eEuDir4eMfeKDUCSIqK8mXLsHT4IHMrGwAgEajgaeHAecuXHBqOecuXISnhwcys7IcWk5RQd7CD3n2yklERORazLcSEZW/W7lyijPlOSIiVypR0BYATp87h/2HDiMtIx13dOyIAH9/XL56FUZvb7WtMKKKJKRKFfx94gREBBqNBiKC7Os5aFC3nlPLqR5aFX8cOOjwcpwJ8rJXTiIiItdjvpWIqHzdypVTnK20Q0TkKk4HbW+YTPh4zhz8b9NmCAANgNioaAT4++PjL79E3bBwDB84wPUpJSqltrGxOHvhIi5fvQY3rRYXkpNh9Pa2+FTSEZ3btcPR4ydw+epVh5bjbJCXiIiIXIP5ViKiiuFWrpzC8hwRlRetszMsWbECG7dtx/3DhuGjN6ZDzMbFRkXj9337XJg8IteJCA/HyCGD4etjhLu7G5o1isRDw+9BZP36Ti2nUYMGGDlsqMPL6dyuHbw9PZGTm4u8vDxcSE6Gt5eX08FiIiIicg7zrUREVFoszxFReXG6pu3/Nm3GkH790K9nD9wwmSzGVQ0JxsWUFJcljsjVGkZEILxWLQDA6BEjSvxmN9KJ5ShB3hmff46cnBw0axSJrgkJTgeLiYiIyDnMtxIR3ToKd1ZmMplw+fJl3BCBVqstt87KKnJ57vDRo0g6fRo513Mwa/7X6JrQge3sElUiTgdtL1+9isj6ETbH6XU6ZF+/XupEEVU2zgR5iYiIyDWYbyUiunXY6qysVvXq8PTwADTl21nZzSzPKcHrvLw8XM/JAQBcSE62aP/Xx2jEoSNHMHfRYqSlZ8BNq8X+w4eRdPo0HrznbgZuiSoJp4O2/r6+uJCcjKjGja3GnT1/HlUCAlySMCIiIiKi0mC+lYjo1mHVWZkAA3v1RNWqVdWatv8FSvAagNr+79xvv1XHK8HrxK1bkZmdDYNeD41Gg9CQEFxMSUHi1m0M2hJVEk4HbWOjo7FkxUrERkXB398fAKDRAJlZWVi19me0ahHj6jQSERERETmN+VYioluHVWdlAgQHBaHaP0Hb/woleG2PErw+d+EiPD08kJmVBQDQaDTw9DDg3IULNyWdRFT2nA7a3jNwAHbv24dxzzyLZo0bQQNg/pKlOHXmDNzc3DC0Xz/Xp5KIiIiIyEnMtxIR0a1GCV4Xp3poVfxx4CBEBBqNBiKC7Os5aFDXfsCXiG4tTr+uCvDzw3uvvIyEuDgcO5EErVaLpFOnEBsdhbenvFQuDYMTERERERXGfCsREVVWndu1g7enJ3Jyc5GXl4cLycnw9vJC5/btyjtpROQiTte0BQoywI+Mut/VaSEiIiIicinmW4mIqDJq1KABRg4bihmff46cnBw0axSJrgkJiKxfv7yTRkQuUqKgLRERERHRf9H+w4exbPVqHDuRhCvXruH5xyYirmVLAEB+fj4WfPcdftv7By6kJMPb0wvRTZvgviFDEGTW6dlzr76G/YcPWyy3Q9s2eHr8+Ju6LURElVV6RgYyMjORl5eH6zk5AIALycnQ6XQACtqFrQxfW0RGRCC8Vi0AwOgRI6DX68s5RUTkSg4FbRd9/73DC9RAg6H9+5U0PUT/CUomwh79P5kJIiIick5Z51uv5+SgTu3a6JqQgOkfzLQYl5Obi2NJSRjSrx/q1K6NjKxMzP56AV597328P+0Vi2m7de6EewYMUP9mQZuIyHV279uHTdu3A0BBp2YA5n77rTo+IS4OneLjyyVtRESOcixou8yZzC8YtCUqhpKJMJkER0+cAABE1KkDrVYDAGjXqlV5Jo+IiOiWVdb51pbR0WgZHW1znLeXF6Y9+6zFsNH33osnp0xB8qVLCKlSRR1u0BsQ4O/v1LqJiMgxsVFRaFjPfodcRm/vm5gaIqKScShou/Lr+WWdDqL/FCUTkZeXh8/mF1xfo4YNVT/X0et02LprV3kmkYiI6JZU0fKtWdlZ0Gg0MHpZBgg2bNuGxK1b4e/nh9joKAzr3x9enp42l5GTk4Ocfz7vLVhmNgDAJCaYTKaySzwRkYuZTCZA/v2/rXuYMo2IQESKnMbecry9vODt5VV8WpxMU1HTuHpZjriV005UUVWk85pt2hK50OGjR5F0+jRyrudg1vyv0TWhAxo1aGA1nY/RCB+jEbm5uernOqEhIeqnkbm5uTc13UREROR6ubm5mPftEnSMi4OX178B2U7t4lE1OBgBfn44eeYM5i1ZgqRTp6xq6SqmT5+Ol19+Wf3b3V2H7kOGICUlBZ4eHmW+HURErpKXl6e+hEpJSVErrdiaRgBcS02Fu7s7tFqt08spizQVtz5XLquyp52ookpPTy/vJKicDtqePX8eV65dQ7NGjazG/XnoEIICAlA9NNQliSO6lRw6cgRzFy1GWnoG3LRa7D98GEmnT+PBe+5GvfDw8k4eERHRf0555lvz8/Px1scfw2QyYezIkRbjunXurP4/rFYtVA8NxeMvvoSjJ5IQUSfcalnPPfccnnjiCfXvrOxsPPTkJAQHB9utnUtEVBHl5ubC8E+lleDgYJvteSvTiAj8/fwQEhJiFbR1ZDllkabi1ufKZVX2tBNVVB4V6IW400Hb2d8sRI3QUJuZ35179uDc+Qt48cknbMxJVLklbt2KzOxsGPR6aDQahIaE4GJKChK3bmPQloiIqByUV741Pz8fb374ES6mpOC1556zqGVrS73wcLi7ueH8xQs2g7YGg0EtJAMFNW0BQKvRWgUyiIgqMq1WW9Cg+D//t3UPU6bRQAONRmNzOkeWUxZpKm59rlxWZU87UUVVkc5rp4O2R48fxx2dOtoc1ywyEhu3bnNqefsPH8ay1atx7EQSrly7hucfm4i4li3V8e/PmoVfNm+xmKdhvXp45+Wp6t95eXn4cuEibNy+Hbl5uYhu3ARjR45ElaBAp9JC/z3pGRnIyMxEXl4erv/zmceF5GSLtmUdde7CRXh6eCAzKwsAoNFo4OlhwLkLF1yfcCIiIiqWq/OtjlACtucuXsDrzz8PXx+fYuc5deYM8m/cYMdkRESVWHFlT6O3N3yMRpev19Em/Iio4nE6aJuZnW237Sy9Xo+MfwJWjrqek4M6tWuja0ICpn8w0+Y0LaKi8Njoh9S/3d0tk/3FggXY+fsePD3+EfgYjfhy4UK88u67eP/VaXCrQBFyqnh279uHTdu3A4Datuzcb79Vx7dr1crhZVUPrYo/DhyEiECj0UBEkH09Bw3q2u+1lIiIiMqOq/OtAJB9/TrOX7yo/n0xJQXHT56E0dsbQQEBeGPmhziWlISXnnwCJpMJV69dAwAYjUbo3N1x/uJFbNi2DS2jo+Hr44PTZ89izsJFqBsWxkI0EVElVlzZMyEuDp3i4126zqKa8OMzh6jiczpoG/T/7N15fBNl/gfwz6RN0jbpfQOFQktpkbZcFaFQTg9WFG9uZV3wWC9EV0R3V9gD1HU99ueJFxblUEFR8VYOKajcWKCUQguF0qb33SRt5vdHyUho0yRt0hz9vF+vviAzk5lvMm3myXee5/sEByP35CkMHTKkzbrck6cQHBRo0/5GpqZiZGpqh9vI5d5mex7UNzTgu23bsfjee6SYFt97L+588CEcys7G8JQUm+KhnmVESgoGxZlPqirkcmTt2WPVviampyPvVD7KKyvhJZOhWKOBWqXCxLHp9gqXiIiIbGDvdisA5J3KxxMrVkiP3/5gLQBg0rixmH3TTfhl/34AwINP/tXkeSueeALJg5Pg7e2NQ0eO4PNvvkVjUxPCQ0IwcuhQzLrpRnY2ICLyYJa+e6pVKrsfs6MSfpcmbburR66xx7E5jupxTOSObE7aXjFiBD7+/HMkDoxHyuDB0vLfjh7Dxi++wJXj2x+C1hXZx3Iw989/hspPhSGJiZh36y0ICmxtZOfl56O5pQXDkpOl7UODg9E3pg+OnTjBpC11yF+t7vCCoNPprN5XUkIC5s+aiRdXrYJWq0VyUiKmZGQgceBAm/ZDRERE9uGIdmvy4CR8/v4as+s7WgcA4aGhePqvf+1wGyIi8jyWvns6grUl/LqzR66xx7HBICIvPx8AEN+/P2Sy1mK5juhxTOSubE7azrzxBuz/7TD+tvJp9IqORlhIMMoqKlF0/jxievfG7JtusmuAI1NTMfbyUYgIC0VJaSne/3gjnly5Ei/+85+Qy+WorK6Gt7d3m7tSQQGBqKyqNrtfrVYL7YU6MkDrLLwAYDAYYDAY7PoaqPsZDAZA/P3/BoMBoijCYBDbLLd1Px0tTxgwALF9YgAAC+bMgUKh+H29Dfsxt5za9/v55fvkqXiOPRvPr2dz5nnt7nYrERGRK7G2hJ8tPXK7ytjjWK/X4/XMTADAnbNmmtT2JaJWNidtVX5+eG7ZMmz+6mvsP3wYmrJyBPr7Y/bNN+H6a64xWzess8ZdcYX0/34xMYjvPwB/WrQIew4exJgO6422fiiZs3LlSixfvlx67O0txzUzZqC0tBQNHXTVJ/eg1+ulpHxpaSm8vLxQXV0NnU5nslxuYaKxS/dj3N5Zy6l9BoMB1dXVEEXRpWZ6JPvhOfZsPL+erbra/E10R+vudisREZErsbaEX3dOqm3scazT6aTavlEREVAoFHY/FpG7szlpCwC+Pj6YeeMNmHnjDXYOx7KQ4CCEh4WhqLh1AojgwEA0Nzejrr7e5I5MVU0NEgcONLufpUuXYvHixdLjhsZGLHzkUYSHh/POjgfQ6XRQXrgAhIeHw9vbG4IgIDAwyGS5pQvDpfsxbt/dy7U6Hev+dMBgMEAQBISHhzPh46F4jj0bz69nc/aXMGe2W4mIiJypoxJ+F+Ok2kSuqVNJW2eqqa1FWUUFQi5MTBbfvz+8vbxw4LdsjLtiFACgorIKZwrP4o8zZ5rdj1KplJJiQGtPWwCQyWT8wugBZDIZIPz+f5lMBkEQWuvkXLLc1v04Y/mB7GzW/bGg9fzy79eT8Rx7Np5fz8VzSkRE9tZdk2Z5gsT4eMTGtJbwu2vevHZvpnJSbSLXZFXS9qVVb2LGDdMRFRGBl1a92eG2ggA8uHCh1QE0NjXhfEmJ9LiktBSnTp+Weg6u3bQJ6WlpCA4Kgqa0DJkffYgAtRpXjBwBoHXY25UTxuOdtWsRoFZDrVbhnbXr0C8mBqntzBRMnsHSjJMKDysnwLo/RERE1nFku5WIiJyvOyfN6ims7ZFLRN3LqqTt4WNHcf3VV7X+/+hRqQdge4SOVrYj71Q+nlixQnr89gdrAQCTxo3Fn//4R5wuPIutO3eivr4BwUFBSB6chMfuvx9+vr7ScxbMmQMvmReeefllaHU6pF42GIvuXgwv9uzwWJZmnEzvsN6x+2HdHyIiIus4st1KRETO152TZvUk1vTIJaLuZVXS9u0XXvj9/y++0MGWtksenITP319jdv0/ljxmcR8KhQJ333E77r7jdnuGRi7AXI/aPtHRmH7NNVB4y7Fm48cATHueKuRyZO3Z062xEhERkfM5st1KRETO152TZhEROZPNNW01ZWUICQqCt3fbp7a0tKC8shIRYWF2CY7Imh617fU8PXz0qEmNo0njxiI0MNA5L4KIiIicgu1WIiLPw0mziKinsLl+wMKHF+PU6dPtrss/cwYLH17c5aCIjEakpGDh3Lm4c9ZM9O3TG3379Mads2Zi4dy5WDh3Loa2U7f44hpHzS0tyM7JwTsfrEVeQUH3vwAiIiJyGrZbiYg8z8T0dKh8faHV6aDX61Gs0UDl58dJs4jI49jc01bsYJ3BYGid0YHITizVctXpdG2e016No/PFxdiWtQt9evVCk1YLACjWaEwm8vJXq7vpVREREVF3YLuViMjzcNIsIuopbE7atmrbwNXr9dh36BAC/P27GBJR17RX48ggijhy/DgyP/xISv6u3rBBek7G6NGYMGaMU+IlIiIiR2K7lYjI03DSLCLqCaxK2q7btAnrP/kUQGuz9y/Llpnd9soJE7oeFVEXtFfjSCYIuGzQIPxx9izIZG2rgqhVKidESkRERPbGdisREZFzmJtI3IgjXIlsY1XSdmBcHP4wZQpEiPjy+x8wJi0NwZdM6uQt90ZsnxiMHzPaIYESWWtiejryTuWjvLISXjIZijUa+KvVmJg+BtGRke0mbYmIiMgzsN1KRETkHJYmEucIVyLbWJW0HZmaipGpqQCApiYtZt54A6IiIhwaGFFntVfjaNLYcQgJDHB2aERERORgbLcSERE5x4iUFAyKi4Ner8frmZkAgDtnzTSZS4aIrGdTTVutTocD2dkYkzaSjV9yaZfWOPL29oZGo3FyVERERNRd2G4lIiLqXpYmEici29g0TlypUECn00F54Y+PiIiIiMgVsd1KRERkXzl5eSgoLMTxE3l4I3MNjuXmOjskIo9mc3HP1MsG41D2EUfEQtTj8KJHRETkOGy3EhER2cex3FysXrceNbV1aG5pQXZODt76YC2/wxI5kE3lEQDg1uuvx8qX/ge5Qo4xI0ciOCgIgiCYbMPZAIksu/ii5yWTITsnBwWFhVgwZzbiYmOdHR4REZHbY7uViIg8UW1dHerq66HX69Gk1QIAijUak9qx9r6+bc3KQn1jI5QKBQRBQFREBEpKS7E1axeSEhLseiwiamVz0vbhv/0dALBu0ydYv+mTdrfZvCaza1ER9QAdXfSYtCUiIuo6tluJiMgT7Tt8GDt27wYAqXbs6g0bpPUZo0djwpgxdj1mUXEJfH18UN/QAAAQBAG+PkoUFRfb9ThE9Dubk7Yzb7gBl3RQIKJO4EWPiIjIsdhuJSIiTzQiJQWD4uLMrlerVHY/Zq+oSBw6chSiKEIQBIiiiMYmLRIGmI+DiLrG5qTt7JtvckQcRD0OL3pERESOxXYrERF5In+1utvL+0xMT0feqXyUV1bCSyZDsUYDtUqFiWPTuzUOop7E5onIiMg+JqanQ+XrC61OB71ej2KNBio/P170iIiIiIiIyKUkJSRg/qyZCPBXw9vbC8lJiVg4dw4SBw50dmhEHsvmnrYA0GIwYN+hQzh7rghavc5knQABM2+8wR6xEXk040XvxVWroNVqkZyUiCkZGUgcOBA6nc7yDoiIiMgitluJiIjsIzE+HrExMQCAu+bNg0KhcHJERJ7N5qRtTW0tHv/nv3D2/HkIAMQLyy8uF8bGL5F1eNEjIiJyHLZbiYjcT05eHgoKC6Ft0mLdp5tx7VVX4rJBg5wdFhFRt7M5abvmo48hl8vx9osv4E+LHsZzy5bBX63CVz/8iD0HD+Jfjy9xRJxEbqm2rg519fXQ6/Vo0moBAMUaDeRyOQBAceFfIiIisj+2W4mIOs/4XcYctUpl97qqx3JzsXrdetTU1sFLJkPuyZMo+aAUC+bOQVJCgl2PRUTk6mxO2h46cgSzbrwRIcHBAACZTEB0ZCTunD0Ler0e76xdh7/cf5/dAyVyR/sOH8aO3bsBAD5KJQBg9YYN0vr0tDSnxEVERNQTsN1KRNR5xu8yBoOIvPx8AEB8//6QyVrHK2SMHo0JY8bY9Zhbs7JQ39gIpUIBQRAQFhqKqupqbM3axaQtEfU4NidtyysqEBEeBi+ZDIIgSL0HASBt+DA898qrdg2QyJ2NSEnBoLg4s+sVcjmy9uzpxoiIiIh6DrZbiYg6z/hdRq/X4/XMTADAnbNmSqMG1SqV3Y9ZVFwCXx8f1Dc0AAAEQYCvjw+KiovtfiwiIldnc9I2wN8fDQ2NAICQ4GCcLjyLIYmJAIC6ujoYDAb7RkjkxvzV6g6HDHHCMSIiIsdhu5WIqPOM32V0Op00ajAqIsKh83D0iorEoSNHIYoiBEGAKIpobGpCQgcdYYiIPJXNSdu4/rE4c+4s0oYNxcjUVKz/9FP4+frC29sbmR9+1GGvQiIiIiKi7sJ2KxGRe5mYno68U/kor6yEl0yG0vJyBAUEYOLYdGeHRkTU7WxO2k678kqcL9EAAObecjOO5+XhhTfeANB6123hvHn2jZDIDViacMwRRfqJiIioY2y3EhG5l6SEBMyfNRMvrloFbZMWiXFxuPaqq5A4cKCzQyMi6nY2J22HDhmCoUNa/x8YEICX/v0vnD57FgKAPr16wcvLy84hEpmXk5eHgsJCaJu0eCNzDaZkjHNKgXpLE445okg/ERERdYztViIi95MYH4/YmBhABGbeMB29e/d2dkhERE5hc9L2UoIgtH6gEnWzY7m5WL1uPWpq6+AlkyE7JwcFhYVYMGc24mJjuzUWSxOOOaJIPxEREdmG7VYiIudylU43RETuoFNJ24aGRmz5/jscPnoMtXV18FerkTI4CVMnT2ZyirrN1qws1Dc2QqlQQBAEREVEoKS0FFuzdnV70tbShGNERETkHGy3EhG5ho463TBxS0TUls1J22KNBk+uWInS8nKEh4UiODAIRSXFOHTkCL764UesePIJREVEOCJWIhNFxSXw9fFBfUMDgNbeM74+ShQVFzs5MiIiInIFbLcSEbmOjjrduHvSlnOcEJEj2Jy0fXPN+9Dp9Xj2738zKQZ+LDcXK176H95c8z7+9shiuwZJ1J5eUZE4dOQoRFGEIAgQRRGNTVokDOBM0ERERMR2KxGRK/HkTjec44SIHMHmpO3ho0excN7cNrM3JiUkYO4tt+Ct99+3W3BEHZmYno68U/kor6yEl0yGYo0GapUKE8emOzs0IiIicgFstxIRuQ5P7nTDOU6IyBFsTtrK5XKEhYa2uy48LFTq/k/kaEkJCZg/ayZeXLUKWq0WyUmJmJKRgcSBA6HT6ZwdHhERETkZ261ERK7DkzvdcI4TInIEm5O2o4YPR9Yvv2J4cnKbdVm//Iq0oUPtEReRVRLj46VZoO+aNw8KhcLJEXUvY+0kc1g7iYiIejK2W4mIXEdHnW6IiKgtm5O248eMxv/efAtP/+9/GD96DIKDAlFZVY1tu3YhLz8fDy5YgLz8Amn7+P6xdgyXiC5mrJ1kMIjIy88HAMT37w+ZTADA2klERNSzOaLdmp2Tg01btuBkfgEqqqrwxKKHMHrkSGm9KIpYt+kTfLN1K+rq65EQF4d75t+Bfn36SNvo9Xq8s3Ydtu/eDZ1eh9TBl+He+fMRFhpiz5dPRORyenqnGyIiW9ictP37M88CAMoqKrB7z15puSitf0Z6LADYvCazqzESIScvDwWFhdA2afFG5hpMyRjn9jOMdsTa12usnaTX6/F6Zuvf2p2zZprMUkpERNRTOaLd2qTVon/fvpiSkYGVL/2vzfqNX2zBp199hUV334XeUVHYsHkz/v70M3jtP8/Cz9cXAPDm++/j1/0H8Nj998FfrcY7a9fiH//9L1741z/hJZN16TUTERERkWewOWn70MKFjoiDyKxjublYvW49amrr4CWTITsnBwWFhVgwZzbiYmOdHZ7d2fJ6jbWTdDqdNEtpVEQE71gTERHBMe3WkampGJma2u46URTx2ddf47bp0zEmLQ0A8PDdd2Peffdj+67dmDp5EuobGvDdtu1YfO89GDpkCABg8b334s4HH8Kh7GwMT0mxe8xERERE5H5sTtpOzhjniDiIzNqalYX6xkYoFQoIgoCoiAiUlJZia9Yuj0za9rTXS0RE5Cjd3W4tKS1FZXU1hiUPkZbJ5XIMSUxEzokTmDp5EvLy89Hc0oJhF9XZDQ0ORt+YPjh24gSTtkREREQEoBNJW6LuVlRcAl8fH9Q3NAAABEGAr48SRcXFTo7MMXra6yUiIvIUlVVVAICgwECT5UGBAdCUlbduU10Nb2/vNiWMggJa6+22R6vVQqvVSo8bGhsBAAbRAIPBYK/wiYjaZTAYpLoyBkPXPnes2ZdxG1EUIYqiyTbGiZj1ej2amlo/F4uKi03Kw9k6EbMtMXW0jauyNnZ7vg+e/p6SZ3Ol30Wrkravvrsas268AcFBQVbveNeePdDpdJiQnt7Z2IgAAL2iInHoyFGIoghBECCKIhqbtEgYEOfs0Byip71eIiIie3KFdqsAweSxKOKSJe1pve63Z+XKlVi+fLn02NtbjmtmzEBpaSl8fXy6FiwRkQV6vV66cVRaWiolSB21L+M2IoCqCze6ZBfqff+y/wB+PXAAwO+fq2+ueV967uXDhmHU8GEOi6mjbVyVtbHb833w9PeUPFttba2zQ5BYlbQ9d/48Fix+BGNGjsTEsekYPGiQVD/zYkXFJfhl/z58v2MHyisq8eif77V7wNTzTExPR96pfJRXVsJLJkOxRgO1SoWJYz3zhkBHr7enTchGRERkK2e2W42J4srqKoQEB0nLq2tqpN63wYGBaG5uRl19vUlv26qaGiQOHNjufpcuXYrFixdLjxsaG7HwkUcRHh4uTW5GROQoOp0Oygufo+Hh4V2aP8OafRm3EUURQYGBiIiIkJK2E8amY+SwoWb335metrbE1NE2rsra2O35Pnj6e0qezceFbohblbT99xNL8fO+ffj4s8+x7D/PwUsmQ3RUFIICAiCXy1FXX4diTSnq6uqgVCoxOWMcZkyf3mZoGFFnJCUkYP6smXhx1SpotVokJyViSkYGEgcOhE6nc8gxTYbdXLj7V6zRSHf/FA68C2ju9Yqi2KMmZCMiIuoMZ7ZbI8PDERwYiIPZ2dK1Wd/cjOycHNwxYwYAIL5/f3h7eeHAb9kYd8UoAEBFZRXOFJ7FH2fObHe/SqVS+mILtPa0BQCZIJMSGUREjiKTyaRurTJZ1z53rNmXcRsBAgRBMNkuMCAAgQEBnT5+V2PqaBtXZW3s9nwfPP09Jc/mSr+LVte0vWLECFwxYgROFhRgz4GDOJ6Xh4qqKmh1OgT4+2PU8OFITkrCqBHDecef7C4xPh6xMTEAgLvmzXP4Xbh9hw9jx+7dACD1zlm9YYO0Pv3CjNCO0t7rffXddzlBGRERkRUc2W5tbGrC+ZIS6XFJaSlOnT4NtUqFiLAwXH/NNfjos8/RKzIKvaIi8eFnn0OpUGD8mNEAAJWfH66cMB7vrF2LALUaarUK76xdh34xMUgdMsTcYYmIiNwGR4gS2YfNE5HFxcYyQUQeb0RKCgbFma8hq5DLkbVnT6f335mevJygjIiIyDaOaLfmncrHEytWSI/f/mAtAGDSuLF4+O67cfO0a6HT6fDa6tWoa2hAQtwA/GPJYybJ4QVz5sBL5oVnXn4ZWp0OqZcNxqK7F8PLhXp2EBERdcax3FyzI0SZuCWyjc1JW6KewF+tbrcWkjHZWt/Q0G6y1doaSp3pycsJyoiIiJwveXASPn9/jdn1giBg9s03YfbNN5ndRqFQ4O47bsfdd9zuiBCJiIicZmtWltkRokzaEtmGSVsiG1hKtmaMHo0JY8ZY3E9nevL2tAnZiIiIiIiIyL1YO0KUJRSILGPSlsgGlpKtF88C3RFzPXmN2ptgzRkTshEREREREXkiSyXrrB1F6e7s/T5YM0KUJRSIrMOkLZENLCVbHa27J2QjIiIiIiLyRPYaRenu7P0+WDNClCUUiKzDpC0RERERERER9Sj2GkXp7uz9PnQ0QtSIk2wTWYdJW3IJxiEZ5iguDM0gIiIiIiIi6ipnj6J0FY54HyyNEOUk20TWsTlpW15ZicbGJvTpFQ0AaDEY8OmXX+JkQQGGJSfjyvHj7R4keT7jkAyDQURefj4AIL5/f8hkAgAgPS3NmeERERGRG2K7lYiIyPVwkm0i69ictH3lnXcQHhqKe+fPBwBs+ORTrP/0U6j8/JD1y6/w9vLmHxrZzDgkQ6/X4/XMTADAnbNmSsXPFXI5svbscWaIRERE5GbYbiUiInI91pRQICJAZusTThacRsrgwdLjb7Ztw/RrrsG6N17H1ZMmYsv339s1QOoZ/NVqREdGIioiAj5KJXyUSkRFRCA6MhLRkZEctkJEREQ2Y7uViIjINRlLKAyKj8dd8+YxYUvUDpuTtrV1dQgKDAQAFJ47h8qqKkzOGAcAGJOWhnPnz9s3QiIiIiKiTmC7lYiIiIjclc1JW5WvL6pragAA2TnHoVappALTAgQ0NzfbN0IiIiIiok5gu5WIiIiI3JXNNW0Hxg3Axi++gLeXFz7/5hsMS06W1hWXahASHGzXAImIiIiIOoPtViIi6oraujrU1ddDr9ejSasFABRrNNLcK2qViqX8iMhhbE7azrn5Zvz9mWfxrxdehFqlwm3Tr5fW/bx3HxIGDLBrgETtsXTxVFz4l4iIiHoutluJiKgr9h0+jB27dwMAfJRKAMDqDRuk9RmjR2PCmDFOiY2IPJ/NSdu42Fi8/eILOFt0Hr0iI+Hn5yut+8OUKegVFWnXAInaY+nimZ6W5pS4iIiIyHWw3UpERF0xIiUFg+LizK5Xq1TdGA0R9TQ2J21//GknRg5NRXz/2DbrEgfGY8+Bg+gdHW2P2IjMsnTxVMjlyNqzpxsjIiIiIlfDdisRkWMZR0Ca4+7lA/zVareOn4jcm81J25dWrcJ/lj2FAH//NutKSkvx0qpVmDRurF2CIzLH0sVTp9N1YzRERETkithuJSJyLOMISINBRF5+PgAgvn9/yGQCAJYPICLqCpuTtmIH63R6PWQyWRfCIaLOyMnLQ0FhIbRNWryRuQZTMsYhKSHB2WERERE5FdutRESOZRwBqdfr8XpmJgDgzlkzTSbqIrKVp/fgJrKWVUlbTVkZNGVl0uOTBaeh0+tNttHpdPh661aEh4baN0Ii6tCx3FysXrceNbV18JLJkJ2Tg4LCQiyYM5uJWyIi6nHYbiUi6j7GEZA6nU6aayQqIgIKhcLJkZE9WZoI3N5JVPbgJmplVdL2hx07sO6TTyEAEAC8vnp1m22MPRkWzptrt+CIyLKtWVmob2yEUqGAIAiIiohASWkptmbtapO05R1LIiLydGy3EhF1P47882yWJgK3dxKVPbiJWlmVtB07ahT69okBIOKZ/3sZ8269Fb2ioky2kcu90a9PH0SGhzsiTiIyo6i4BL4+PqhvaAAACIIAXx8lioqL22zLO5ZEROTp2G4lIupe9hr516Y3pwiUlpdD5u0NmUzGDiZOZGkicHsnUdmDm6iVVUnbmN69EdO7NwDgoYULkTZsaLsTOhBR9+sVFYlDR45CFEUIggBRFNHYpEXCgLYXVd6xJCIiT8d2KxFR97Jl5F9H2vTmFIGNW76EUqkEBHYwcSZLE4ETkWPYPBHZ5IxxjoiDiDppYno68k7lo7yyEl4yGYo1GqhVKkwcm95mW96xJCKinoTtViIix7Nl5F9HLu3NaTAYUF5ejtDQUKmnLRFRT2Jz0hYAjhw/ju27dqO0rAw6ve6StQL+/cRSO4RGZDtzBdK9vLxQXl4OXz8/BAYEODlK+0pKSMD8WTPx4qpV0Gq1SE5KxJSMDCQOHOjs0IiIiJyO7VYiIseyZeRfRy7tzWkwGOAlCIiIiIBMJrN32ERELs/mpO3323fgf2+9BbVKhd7RUZB7yy/ZQmz3eUTdwWyBdBHQarWYMn58uz1Q3V1ifDxiY2IAAHfNmwetTofzJSVmt1fIL/27JSIi8jxstxIRtWXvyYltGflHRETWszlpu3HLFowddTkevvtuqQ4mkaswVyDdOLSmX9++Toiq+1macCw9Lc2Z4REREXULtluJiNqy9+TEHPlHROQYNidtNWVluPv2eWz4kksyVyDdOLSmpxRPtzThmEIuR9aePc4MkYiIyOHYbiUiasuekxMbe+0GBQQgKiICAHD91VdDLpfjfEmJzb12iYjodzYnbWN69UJVdbUjYiEiO7E04ZhOd2lNPyIiIs/DdisRUVv2nJzYbHm6C2zttUtERL+zOWl7+2234p2165CclITQkBBHxERERERE1GVstxIROZa58nRGtvTaJSIiUzYnbbd89z3qGxtx91/+ggF9+7UZ6iAIAv66+GGr95edk4NNW7bgZH4BKqqq8MSihzB65EhpvSiKWLfpE3yzdSvq6uuREBeHe+bfgX59+kjb6PV6vLN2Hbbv3g2dXofUwZfh3vnzERbKxrm7Mw630ev1aNJqAQDFGo3J0B0OtyEiIqL22LvdSkREpsyVpyMioq6zOWlbUFgImUxAYEAAyqsqUV5VabJegGDT/pq0WvTv2xdTMjKw8qX/tVm/8Yst+PSrr7Do7rvQOyoKGzZvxt+ffgav/edZ+Pn6AgDefP99/Lr/AB67/z74q9V4Z+1a/OO//8UL//onvGQyW18iuRAOtyEiIqLOsne7lYioJ8rJy0NBYSG0TVq8kbkGUzLGISkhwdlhERF5PJuTtm+/+IJdAxiZmoqRqantrhNFEZ99/TVumz4dYy7Mdv/w3Xdj3n33Y/uu3Zg6eRLqGxrw3bbtWHzvPRg6ZAgAYPG99+LOBx/CoexsDE9JsWu81L043IaIiIg6y97tViKinuZYbi5Wr1uPmto6eMlkyM7JQUFhIRbMmc3ELRGRg9mctO1OJaWlqKyuxrDkIdIyuVyOIYmJyDlxAlMnT0Jefj6aW1owLDlZ2iY0OBh9Y/rg2IkTTNq6OQ63ISIiIiIico6tWVmob2yEUqGAIAiIiohASWkptmbtYtKWiMjBbE7aasrKLG4TERbWqWAuVVlVBQAICgw0WR4UGABNWXnrNtXV8Pb2btPjMiggEJVV5mcL1mq10F6okQoADY2NAACDwQCDwWCP8KkTDAYDIP7+f3udC4PBAFEU3ebcmnsfHL3cXbnb+SXb8Rx7Np5fz+bM89qd7VYiIk9UVFwCXx8f1Dc0AGitBe7ro0RRcbGTIyMi8nw2J20XPLzYYvWvzWsyOxlO+y6tNyaKsKICmQhBML/VypUrsXz5cumxt7cc18yYgdLSUjTU13c+WOoSvV4vJdNLS0ulCce6ymAwoLq6GqIoQuYGdY7NvQ+OXu6u3O38ku14jj0bz69nq642fxPd0ZzRbiUi8iS9oiJx6MhRiGLr92tRFNHYpEXCAPMl7IiIyD5sTto+uHBBmyRqTV0tftm/H+UVlZgxfbrdggsOCgIAVFZXISQ4SFpeXVMj9b4NDgxEc3Mz6urrTXrbVtXUIHHgQLP7Xrp0KRYvXiw9bmhsxMJHHkV4eDjrpDqRTqeD8sKEY+Hh4VAoFHbZr8FggCAICA8Pd4uEgLn3wdHL3ZW7nV+yHc+xZ+P59WzOvMZ0Z7uViMgTTUxPR96pfJRXVsJLJkOxRgO1SoWJY9OdHRoRkcezOWk7JSOj3eU3/uEPePp//0NpRXmXgzKKDA9HcGAgDmZnIy42FgCgb25Gdk4O7pgxAwAQ378/vL28cOC3bIy7YhQAoKKyCmcKz+KPM2ea3bdSqZSSVkBrT1sAkMlk/MLoRDKZTOpGbe9zIQiC25xfc++Do5e7M3c6v9Q5PMeejefXcznznHZnu5WIyBMlJSRg/qyZeHHVKmi1WiQnJWJKRkaHHaSIiMg+7NqKnjwuA99u227TcxqbmnDq9GmcOn0aQOvkY6dOn4amrAyCIOD6a67BR599jt179uJ0YSFefGMVlAoFxo8ZDQBQ+fnhygnj8c7atTiUfQQnCwrw39deQ7+YGKQOGdLRoYmIiIioh+pMu5WIqCdKjI9HbEwMBsXH465585iwJSLqJjb3tO1Ii6EF9TbWg807lY8nVqyQHr/9wVoAwKRxY/Hw3Xfj5mnXQqfT4bXVq1HX0ICEuAH4x5LH4OfrKz1nwZw58JJ54ZmXX4ZWp0PqZYOx6O7F8GJvHSIiIiJqR2farURERO6itq4OdfX10Ov1aLown0mxRiPNZ6JWqeCvVrv9MYk8mV2Sts3NzSgoLMTajZvQv29fm56bPDgJn7+/xux6QRAw++abMPvmm8xuo1AocPcdt+PuO2636dhERERE1LN0pd1KRETkLvYdPowdu3cDAHwulIZcvWGDtD5j9GhMGDPG7Y9J5MlsTtpeP+92s7PwqlQq/GPJY10MiXqynLw8FBQWQtukxRuZazAlYxySEhKcHRYRERG5IbZbiYiopxqRkoJBcXFm1ztiAnZnHJPIk9mctJ15ww0QLmn9yuUKRIaHYURqqknZAiJbHMvNxep161FTWwcvmQzZOTkoKCzEgjmzmbglIiIimzmj3fqnRQ9DU1bWZvkfpkzGvfPn44U33sCPP+00WTcoLg7PLV9m91iIiFwFh813P3+1utvfU2cck8iT2Zy07ahMAVFXbM3KQn1jI5QKBQRBQFREBEpKS7E1axeTtkRERGQzZ7Rbn//HchgMBunx6bNn8benn8HYy0dJy4anpGDRXQulx97edp1mgojIKt05ypHD5omIbNelFuK58+dRW1eHAH9/9IqKsldM1EMVFZfA18cH9Q0NAFrrGfv6KFFUXOzkyIiIiMjddVe7NTAgwOTxx59/geiICAxJSpSWyeXeCA4KclgMRESWdPcoRw6bJyKyXaeStjt/+QXvrF2H8ooKaVloSAj+NGc20i+/3G7BUc/SKyoSh44chSiKEAQBoiiisUmLhAHmL+49haXhRIoL/xIREZEpZ7Zb9c3N2JqVhRumToVwUZ2G7GM5mPvnP0Plp8KQxETMu/UWBAUGmt2PVquF9sL1HwAaGhsBAAbRYNKrl4jIWj/u3In6xobfRzmGh7eOctyZhUHx8SbbGgwGQPz9/5353FH5+UHl59fhNpfu12AwQBRFfs55KGt+r+zxu0dkK1f6PbM5abv34EH85+VX0LdPH0y78kqEBAehvLIS27J24dmXX8HfHlFiZGqqI2IlDzcxPR15p/JRXlkJL5kMxRoN1CoVJo5Nd3ZoTmdpOFF6WppT4iIiInJlzm63/rx3H+obGjA5Y5y0bGRqKsZePgoRYaEoKS3F+x9vxJMrV+LFf/5Tuhl7qZUrV2L58uXSY29vOa6ZMQOlpaXw9fFxWPxE5LlOFxbC28sboihCFEXo9Hp4e3ujoPAMNBqNybZ6vV66cVRaWmr2s8reDAYDqqurIYoiZDJZtxyTuo81v1fO+t2jnq22ttbZIUhsTtp+uPkzDE1OxlOPPmLywXnTtddi2bP/wYefbmbSljolKSEB82fNxIurVkGr1SI5KRFTMjKQOHCgs0NzOkvDiRRyObL27OnGiIiIiFyfs9ut323fjhGpKQgNDpaWjbviCun//WJiEN9/AP60aBH2HDyIMWZuwi5duhSLFy+WHjc0NmLhI48iPDyckwATUaf0i4nB4aNHIQgCBEGAQi5Hc3MzYvv2RUREhMm2Op0OygsdR8LDw6FQKLolRoPBAEEQEB4ezqStB7Lm98pZv3vUs/m40A1xm5O2p86cxmP33dfmQ1MQBPxhyhQ89+qrdguOep7E+HjExsQAAO6aN48fyhdYmoVTp9N1YzRERETuwZntVk1ZGQ5lZ2Ppooc63C4kOAjhYWEoKi4xu41SqZS+tAKtPW0BQCbImMggok6ZNHYsTuYXoLyyqnWUY2kp1CoVJo0d2+ZzRSaTAcLv/+/Ozx1BELr9mNQ9rPm9cubvHvVcrvR7ZnPSVibIoG9uaXddc0uLSb0uInIs1rolIiIyz5nt1u+370BgQADShg7tcLua2lqUVVQghBOTEVE34ihHIiLXZ3PSduCAAdi05QuMHJoK5UW9IPV6PT758ksMiuekUUTdhbVuiYiIzHNWu9VgMOD7HTswadw4eHl5Scsbm5qwdtMmpKelITgoCJrSMmR+9CEC1GpcMXKEQ2IhIjKHoxyJiFybzUnb2TffhL+ufBoLH16M9FGXIzgwCJVVVdi1dy9q6+rw76WPOyJOImoHa90SERGZ56x268EjR1BaXo4rx2eYLJfJZDhdeBZbd+5EfX0DgoOCkDw4CY/dfz9r0xIREV0iJy8PBYWF0DZp8UbmGkzJGIekhARnh0XUbWxO2l42aBD+seQxvLfhQ3z53fcQ0VpnZlBcHP5y35/5B0TUjexV69ZYZsEctUrV4XGIiIhckbParcOTk/H5+2vaLFcqFPjHksccckwiIiJPciw3F6vXrUdNbR28ZDJk5+SgoLAQC+bMlq7f/B5Lns7mpC0AJCcl4bllT6FJq0V9fT1UKpU0NJuIXIe1dyaNZRYMBhF5+fkAgPj+/SGTtdb6yxg9GhPGjOnW2ImIiOyB7VYiIiL3szUrC/WNjVAqFBAEAVERESgpLcXWrF3Sd1p+jyVPZ3XStqGxEd5eXiZ1bnyUSqnRq9Pp0NzSwqFdRC6iozuT+uZmk2TumLSRWDh3LvR6PV7PzAQA3DlrpjShmVqlcuZLISIisgnbrURERO6tqLgEvj4+qG9oANA6UsbXR4mi4mJpG2O5QH6PJU8ls2ajkwUFmHXX3cjOyTG7Tfbx45h19z04npdnt+CIqPMuvjMpl8sRFRGB+oYGbNrypZTMbW5pQXZODtZu+gRV1dWIioiQvtRGRUQgOjIS0ZGRHFJCRERug+1WIiIi99crKhKNTU0QRREAIIoiGpu06B0VJW3jr1YjOjKS32PJY1mVtN3y3XcYnpqK4SkpZrcZnpyMtKFD8eX3P9gtOCLqPOOdSUFoHRpivDN54tSpdpO5W7N2OTliIiKirmO7lYiIyP1NTE+HytcXWp0Oer0exRoNVH5+mDg23dmhEXUbq5K2h44exbgrRlncLv3yy5F93HyvBiLqPubuTAJoN5l78TATIiIid8V2KxERkftLSkjA/FkzEeCvhre3F5KTErFw7hwkDhzo7NCIuo1VNW0rq6oRGRZucbuIsDBUVlV3OSgi6rqJ6enIO5WP8spKeMlkKNZooFapENOrF86cOwdRFCEIgpTMTRgQ5+yQiYiIuoztViIiItdVW1eHuvp66PV6NGlbOxUVazQmdWiNZQ0S4+MRGxMDALhr3jyTWvVEPYFVSVu5tzcatU0Wt2vSaiH3tnpuMyJyIOOdyRdXrYJWq0VyUiKmZGRAFEW8ueb9NslcDjMhIiJPwHYrERGR69p3+DB27N4NANIEoas3bJDWZ4wejQljxjglNiJXY1VLtVdUFLKP5WBkamqH2/127Ch6XVQUmoicy9ydyfaSuYkDB0Kn0zkzXCIioi5ju5WIejJjL0ZzLu7FSOQMI1JSMCjO/ChPtUrVjdEQuTarkrZXjByBTV9sweRxYxHTu3e725w+exZf/fAjbp42za4BEvUktgwV6QoOMyEiIk/FdisR9WTGXowGg4i8/HwAQHz//pDJWuezYC9GcjZ/tZo3DoisZFXS9rqrrsL323fgL8v/gduuvx6jRgxHZHhrrbCS0lL8vG8fPvrscwQGBGDaVVc6NGAiT8ahIkRERF3DdisR9WTGXox6vR6vZ2YCAO6cNdOkEwgREbkHq5K2fr6++MeSx7DixZewesMGvHdREgkARAD9YvrgyUWL4Ofr64g4iXoEDhUhIiLqGrZbiagnM/Zi1Ol0UieQqIgIjqwjInJDVs++EB0ZiZf+/S/s3rsPB7OzUVpeBgAIDw3D8ORkjBoxHDKZzGGBEvUEHCpCRETUdWy3EhEREZG7s2nKXJlMhvTL05B+eZqj4iEiIiIi6jK2W4mIiIjIndmUtCUiIiIiIiIi99Vdkx8TEVHXMGlLRERERERE5OKMyVZzrE22cvJjIiL3wKQtERERERERkYszJlsNBhF5+fkAgPj+/SGTCQCsT7Zy8mMiIvfApC0RERERERGRizMmW/V6PV7PzAQA3DlrpklZA2tw8mMiIvfApC0RERERERGRizMmW3U6nVTWICoiAgqFwsmRERGRI8hsfcKeAwfR1NTkiFiIiIiIiOyG7VYiIiIiclc297T95/PPw8vLCwkDBmDokMuQOmQIBsXHw0tmc/6XiIiIiMhh2G4lIiIiIndlc9L2hX/+Awezs3HoyFFs3LIF6z75FL4+PrgscRCGXjYEqUMuQ78+fRwRKxERERGR1dhuJSIiIiJ3ZXPSNi42FnGxsbh52jTom5txLDcXB7OP4GB2Nt7+4ANAELA58z1HxEpEREREZDW2W4mIiIjIXXVpbFh5RQXOl5SgqPg8ijUaiACiIyLsFBoRERERkX2w3UpERERE7sTmnrZZv+5pHWaWnY3i0lIEBQQg5bLBuHPWLKQOuQzhoaGOiJOIiIiIyCZstxIRERGRu7I5afvM//0flAoFpl11JSakp7MOGBERERG5JLZbiYiIiMhd2Zy0HTk0FUdyjmPTF1uw79BhDEsegqFDhuCyQYOgUCgcESN5kNq6OtTV15tdr5DLuzEaIiIi8mRstxIREXm+nLw8FBQWQtukxRuZazAlYxySEhKcHRZRl9mctP37I4+gxWDA8bw8HMrOxsHsI/js628gk8mQlJCAoUOG4JbrpjkiVvIA+w4fxo7du2EwiMjLzwcAxPfvD5lMAACkp6U5MzwiIiLyIGy3EhERebZjublYvW49amrr4CWTITsnBwWFhVgwZzYTt+T2bE7aAoCXTIbBCQkYnJCAWTfdhNyTp/DBxx/jQHY2fjt6lI1fMmtESgoGxcVBr9fj9cxMAMCds2ZCp9ejvqEBCrkcTVotAKBYo4H8Qs9btUoFf7XaaXETERGRe2K7lYiIyHNtzcpCfWMjlAoFBEFAVEQESkpLsTVrF5O25PY6lbStrK6WeiscOnoUFRUVAIB+MX0w9LIhdg2QPIu/Wg1/tRo6nQ4+SiUAICoiArv27sWO3bsBQFq+esMG6XkZo0djwpgx3R8wERERuTW2W4mIiDxXUXEJfH18UN/QAAAQBAG+PkoUFRc7OTKirrM5aXvf44/j7LkiiAAiwsIwPDkZqZcNRupllyEwIMABIVJPYOyBa45aperGaHou1gIiIiJPwnYrERGRZ+sVFYlDR45CFEUIggBRFNHYpEXCAPP5BSJ3YXPSNqZXb1x31VUYOmQIoiIiHBET9UDGHrjkPKwFREREnobtViIiIs82MT0deafyUV5ZCS+ZDMUaDdQqFSaOTXd2aERdZnPS9vEHH3BEHETkZKwFREREnobtViIiIs+WlJCA+bNm4sVVq6DVapGclIgpGRlIHDjQpv3U1tWhrr7e7HrOs0PO0KmatkTk2owXHL1e3+7EbooL/16MtYCIiIiIiIjI3STGxyM2JgYAcNe8eVAoFNI6a5Ox+w4fxo7du2EwiMjLzwcAxPfvD5lMAMB5dsg5OpW03bozC5998w0Ki85Br9O3Wb95TWaXAyOizjNecID2J3ZLT0tr8xzWAiIiIk/EdisREVHPZW0y1jjPjl6vx+uZrW2DO2fNlDo+cZ4dcgabk7a/7NuPl958E5PHjcPJggJMyciAXq/HL/v3IyQ4GONHX+GIOInIBpYmdlPI5cjas8dkGWsBERGRp2G7lYh6Kk4wTO7M0shRW0oVWJuMNc6zo9PppI5PURERJr12ibqbzUnbj7/4HDdMvQbzbrsN323fjj9MmYL4/rGorKrCkn/+C2GhoY6Ik4hsYGliN51O12aZvWoBERERuQq2W4moJ+IEw+TuLI0ctaVUAZOx5M5sTtqeO1+M2TfdBOHCY4OhBQAQHBSEGdOnY9OXW3Dl+PH2jJGIuklHtYCIiIjcDdutRNQT2WuCYXv2diSyhaWRoyxVQD2FzUlbg8EAb29vyGQy+CiVqKyqltaFh4WiWFNq1wCJiIiIiDqD7VYi6onsNcGwPXs7EtnC0sjRzmDJEHJHNidtI8PDUVFZBQCI7dsXO3bvxqgRwwEAWb/+ipCgIHvGR27K0gyNigt3Z4mIiIgcxRnt1rUbN2HdJ5+YLAsKDMSaV14GAIiiiHWbPsE3W7eirr4eCXFxuGf+HejXp4/dYyGinsleEwyztyN5CpYMIXdlc9I25bLBOHQkG+PHjMb1V1+FZ19+BSdOnYK3tzfOnT+PO2bMcESc5GYszdCYnpbmzPB6LEtDnJhMJyIiT+KsdmvfPr3xr8cflx7LZDLp/xu/2IJPv/oKi+6+C72jorBh82b8/eln8Np/noWfr69D4iGinsVeEww7orcjkTPYq2QIUXezOWl7+623Qq9vBgCMHTUKMpkM23btggABN027FlMyMuweJLkfSzM0KuRyZO3Z48wQeyRLQ5yYTCciIk/irHarl8wLwe304hVFEZ99/TVumz4dYy5ccx+++27Mu+9+bN+1G1MnT3JIPETkWSwN8+YEw0Sm7FUyhKi72Zy0lcvlUuINAMakpUmNTgDSEAzq2SzN0KjT6ZwZXo9laYiTtcl0S+UvOCkBERG5Ame1W4tKinHH/Q/AW+6NQXFxuP2226RePZXV1RiWPMQkxiGJicg5cYJJWyKyyNph3pxgmOh39ioZQtTdbE7admTbrl1Y/8mneP0/z9pzt+TGWOzbtVga4mRtMt1S+QtOSkBERK7OUe3WhPg4PHz3PegdHYWq6mps+HQz/rL8H3jl6ZWorKoC0Frj9mJBgQHQlJWb3adWq4X2QlkjAGhobAQAGEQDDAaDXeMnItf2486dqG9s+H2Yd3h46zDvnVkYFB8vbWcwGADx9/+742eFwWCAKIpuGTt1L0u/7+PHjMGJi0uGlJRArVJh/Jgxbbb1hL8d6hpXOudWJ23rGxrw8959qKqpRq+oKIwaPlyqz7Vrzx58sHEjCs8VITws1GHBknvp6C5wXGyss8OjLrBU/oKTEhARkTM5s906MjX19wcxMUiMj8fCRx7Fjz/txKD41h49Akx794oi0FF/35UrV2L58uXSY29vOa6ZMQOlpaXw9fGxZ/hE5OJOFxbC28sboihCFEXo9Hp4e3ujoPAMNBqNtJ1er5du9pSWlpqMOnAXBoMB1dXVEEXRpDY40aUs/b6HBgZi+tVX4d31G6DT6RAfG4sxaSMREhhg8ndjzb7I89XW1jo7BIlVSdui4hI8/s9/orqmBiJaG5VDkhLx5KKH8Z9XXsH+w4ehUqkwf+YMXHfVVY6NmNxGR8W+9c3N7IHrBiyVQVD5+bVb/oKIiMhZXK3d6uPjg9iYPigqKcYVI0cAACqrqxASHCRtU11T06b37cWWLl2KxYsXS48bGhux8JFHER4ezsnLiHqYfjExOHz0KARBgCAIUMjlaG5uRmzfvoiIiJC20+l0UF5op4eHh7tlO91gMEAQBISHhzNpSx2y5vc9KCgIO37+BQDwwMIFZv8mPOFvh7rGx4VuiFuVtP3g44/R0NSIWTfdhPgB/VGi0eDDzZ/hL/9YjsJzRbhqwnjMnzmTvevIhLli3ydOnULeqVMW6zCR81kqg8CJy4iIyNW4WrtVr9ej8FwRBg8ahMjwcAQHBuJgdrY06kjf3IzsnBzcMWOG2X0olUrpCyTQ2tMWAGSCjIkMoh5m0tixOJlfgPLKqtZh3qWlUKtUmDR2rMnngUwmk7rwy2Tu+1khCIJbx0/dw5rfd2v/Jjzlb4c6z5XOuVVJ2+ycHMyYPh23Xn+9tCw6MhLL/vMcrpk0CX/+43xHxUduzFyxby+ZDM0tLe32wGXS1rVYKoNg7cRlRERE3cXZ7da3167F5cOGITw0FNU1NdiweTMaGhsxedw4CIKA66+5Bh999jl6RUahV1QkPvzscygVCowfM9qhcRGRZ0hKSMD8WTPx4qpV0Gq1SE5KxJSMDCQOHOjs0IiIyM6sStpW19a2SaYNvvB43BWj7B8VeYSJ6enIu7jYt0YDtUoFg8EAuSi26YFbVFzs5IjpUsaJy3Q6XbtlEKyduIyIiKi7OLvdWl5RgedeeRU1tbUICAjAoPg4PLd8GSLCwgAAN0+7FjqdDq+tXo26hgYkxA3AP5Y8xjIHRGS1xPh4xMbEAADumjePw7eJOmAs+afX69F0oVZtsUZjMh9LR5N1EzmTVUlbg8EAxSXFl+UXLgy+PmxgUvvM3QX+cefOdnvgJgyIc3bIRERE5Oac3W597P77O1wvCAJm33wTZt98k8NjISIi6umMJf8ASB2RVm/YIK3PGD0aE8aMcUpsRJZYlbQFgLPnz0Mm85IeGwyGC8uL2mwb3z+265GRR2jvLrAoiu32wJ04Nt3J0RIREZEnYLuViIiIgN9L/pnDuZnIlVmdtH3pjVXtLn/htdel/xtn6N28JrOrcZGb62gIQlBgIG6bPh2r3l/DOkxERERkd2y3EhEREfB7yT8id2RV0vahhQsdHQd5GEtDENLT0liHiYiIiOyO7VYiIiIi8gRWJW0nZ4xzdBzkYSwNQVDI5cjas6cbIyIiIqKegO1WIiIiIvIEVpdHILKFpSEIOp2uG6MhIiIiIiIiIupYTl4eCgoLoW3S4o3MNZiSMQ5JCQnODot6KJmzAyAiIiIiIiIiInKmY7m5WL1uPWpq69Dc0oLsnBy89cFaHMvNdXZo1EOxpy0REREREREREbmVjiZABwC1SmXTJGRbs7JQ39gIpUIBQRAQFRGBktJSbM3axd625BRM2hKRRRwiQkRERERERK7E0gToGaNHY8KYMVbvr6i4BL4+PqhvaAAACIIAXx8lioqL7Rg1kfVcPmm7duMmrPvkE5NlQYGBWPPKywAAURSxbtMn+GbrVtTV1yMhLg73zL8D/fr0cUa4RB7n4iEiXjIZsnNyUFBYiAVzZiMuNtbZ4REREREREVEPZGkCdLVKZdP+ekVF4tCRoxBFEYIgQBRFNDZpkTDA/DGIHMnlk7YA0LdPb/zr8celxzLZ76V4N36xBZ9+9RUW3X0XekdFYcPmzfj708/gtf88Cz9fX2eES+RROhoiwqQtEREREREROYOlCdBtNTE9HXmn8lFeWQkvmQzFGg3UKhUmjk232zGIbOEWE5F5ybwQHBQk/QQGBABo7WX72ddf47bp0zEmLQ39YmLw8N13Q6vTYfuu3U6OmsgzGIeICIIAgENEiIiIiIiIyPMkJSRg/qyZCPBXw9vbC8lJiVg4dw4SBw50dmjUQ7lFT9uikmLccf8D8JZ7Y1BcHG6/7Tapt19ldTWGJQ+RtpXL5RiSmIicEycwdfIkJ0ZN5Bk4RISIiIiIiIh6gsT4eMTGxAAA7po3DwqFwskRUU/m8knbhPg4PHz3PegdHYWq6mps+HQz/rL8H3jl6ZWorKoC0Frj9mJBgQHQlJV3uF+tVgvthdkFAaChsREAYDAYYDAY7PsiqA2DwQCIv//f0e+5wWCAKIo8txa0d17GjxmDExcPESkpgVqlwvgxY7r9PHYUN8+vZ+M59mw8v56N55WIqGO1dXWoq683u16tUtl1CDgREbkHl0/ajkxN/f1BTAwS4+Ox8JFH8eNPOzEovrWnnwDB5DmiiEuWtLVy5UosX75ceuztLcc1M2agtLQUDR1cMMk+9Hq9lDQvLS2FXC536PEMBgOqq6shiqJJTWRqVd/QgPqGBjQ3N6OmthYAcDQnB97erR8RV4/PwLpPN0On0yE+NhZj0kYiJDAApaWl3XoezeH59Xw8x56N59ezVVdXOzsEIiKXtu/wYezYvRsGg4i8/HwAQHz//pDJWr/VZowejQljxjgzRCIicgKXT9peysfHB7ExfVBUUowrRo4AAFRWVyEkOEjaprqmpk3v20stXboUixcvlh43NDZi4SOPIjw83OYZBnuyzt4V1ul0UCqVAIDw8HCHDzkwGAwQBAHh4eFMCLRj+67d+OnnnwEAAf7+AIAvvv9BWj86baQ06dgDCxdI56u7z6M5PL+ej+fYs/H8ejYOKyQi6tiIlBQMiouDXq/H65mZAIA7Z82UOkTw+ykRUc/kdklbvV6PwnNFGDxoECLDwxEcGIiD2dlSQknf3IzsnBzcMWNGh/tRKpVSsglo7WkLADKZjF8YbXAgO7tTd4VlMpnUHbq73nNBEHh+zRg5NBWJA+PNrlfI5di9dy8A0/OVe+oUCs4WQtukxZvvf4ApGeOQlJDQLTFfiufX8/EcezaeX8/Fc0pE1DF/tRr+ajV0Oh18LnxHjYqI4E0vIqIezuWTtm+vXYvLhw1DeGgoqmtqsGHzZjQ0NmLyuHEQBAHXX3MNPvrsc/SKjEKvqEh8+NnnUCoUGD9mtLND7xF4V9gzGBuK5uh0ujbLjuXmYvW69aiprYOXTIbsnBwUFBZiwZzZTkvcEhERERH1ZMaRkHq9Hk0XypgVazQm389YH5eIyD24fNK2vKICz73yKmpqaxEQEIBB8XF4bvkyRISFAQBunnYtdDodXlu9GnUNDUiIG4B/LHkMfr6+To68Z+Bd4Z5ra1YW6hsboVQoIAgCoiIiUFJaiq1Zu9okbTm5AhERERGR4xnr4wKQvp+t3rBBWs/6uERE7sPlk7aP3X9/h+sFQcDsm2/C7Jtv6qaIqCt459dzFBWXwNfHB/UNDQBa/xZ9fZQoKi5usy0nVyAiIiIicjzjSEhzOBKSiMh9uHzSljwL7/x6jl5RkTh05ChEUYQgCBBFEY1NWiQMaNtIZBkNIiIiIiLHs1T2jIg6jyNIqbsxaUvdind+PcfE9HTkncpHeWUlvGQyFGs0UKtUmDg2vc22LKNBRERERERE7owjSKm7MWlL3Yp3fj1HUkIC5s+aiRdXrYJWq0VyUiKmZGQgceBAZ4dGREREROR2cvLyUFBYCG2TFm9krsGUjHEmc0Ww1ByRc3EEKXU3Jm2JqNMS4+MRGxMDALhr3jz2nCUiIiIi6oRjublYvW49amrr4CWTITsnBwWFhVgwZ7aUuGWpOSLn4ghS6m5M2hKRWZbu5isu/EtERERERJ23NSsL9Y2NUCoUEAQBURERKCktxdasXVLSlqXmiIh6FiZtyS4sDeUh92Tpbn56WppT4iIiIiIi8iRFxSXw9fFBfUMDAEAQBPj6KFFUXCxtw1JzREQ9C5O21GXWDOUh92Tpbr5CLkfWnj3SY0uzabJnLhERERFRW72iInHoyFGIoghBECCKIhqbtEgYYL4tTkREno1JW+oya4bykHuydDdfp9OZPLY0myZ75hIRERFRT2KpU4Nx8rCJ6enIO5WP8spKeMlkKNZooFapMHFsejdGS0REroRJW+oya4byUM9g7Jl7PC8P+WdOQ6vTwc/HB2PS0hDXP7ZNz1wiIiIiIndkbTLW2KmhprYO+adPo8VgQHBQEMJCQuCvVkmThyUlJGD+rJl4cdUqaLVaJCclYkpGBhIHDuzGV0VERK6ESVvqMg7lISN/tRpni4rw2dffoL6hEV4yGU6ePo2SsjIsmDMbcbGxzg6RiIiIiKjLrE3GjkhJgbfMC+s3fwoIgFwuh5cgoEnbhOlXX43UIZdJ+0yMj0dsTAwA4K558zgjPRFRDydzdgDk/iamp0Pl6wutTge9Xo9ijQYqPz8O5emhLi6XIZfLERURgfqGBmzN2uXs0IiIiIiI7GJESgompY9Fk7apTTJ2UvpYjEhJAdDaqeHwsaPQ65vho/SBUqFA7169oNc34/CxY5xYjIiIzGLSlrrMOJQnwF8Nb28vJCclYuHcORzK00MZy2UIQmsdW5bLICIiIiJPY0sylu1jItdXW1eH8yUlKNZo0KTVokmrRbFGg/MlJThfUoLaujpp25y8PBQUFuL4iTy8kbkGx3JznRg5eTKWRyC74FAeMmK5DCIiIiLqCayd24PtYyLXZyx5AgA+SiUAYPWGDdJ6Y8mTY7m5WL1uPWpq6+AlkyE7JwcFhYVYMGc2J2Inu2PSlojsprauDilJg3E0NxdNWi1kMhnOFhXBz88XyYOTsP/wbygoLIS2SYs3MtdgSsY4XtiIiIiIyC1Zk4y11D6uratjiQQiF2CcVNsctUoFwLQcoCAIiIqIQElpKbZm7UJSQoLVkxQSWYNJWyKyG+PdSV8fXwQG+EOr1UKECF8fH2z++htUVlXxjiQRERERuT1rk7EdtY9/3LkTzS0tGJGSgrr6euj1ejRptQCAYo0GcrkcAJM8RN3BX6226u/MUg9749+8wSAiLz8fABDfvz9kstbyKMYeu0TWYNKWiOymo7uT73+8EVXV1WbvSBIRERERuQtrkrETxoyxqveetcOyicj5LPWwN/7N6/V6vJ6ZCQC4c9ZMk5swRNZi0paI7Kaju5PVNTVQ+fmhsakJACdgICIiIiL3Ze1Qamt671m7LyJyvonp6cg7lY/yykp4yWQo1migVqkwcWw6gN//5nU6nXQTJioigvP+UKcwaUtWsVSXRXHhrhGROZyAgYiIiIiczV71Jq0dSm0Ne+6LiBwrKSEB82fNxIurVkGr1SI5KRFTMjKQOHCgs0MjD8SkLVnFUl2W9LQ0Z4ZHbsDSHUkiIiIiIkdjvUki6qrE+HjExsQAAO6aN4+9aMlhmLQlq5iry6LT61Hf0ACFXM6i+dQh3pEkIiIiImezpt4kZ38nIiJXwKQtWcVcXZZde/eyaD5ZjXckiYjI03302WfYtWcvzp0/D4VCjsSBAzF/xkz06RUtbfPCG2/gx592mjxvUFwcnlu+rJujJep5rKk3uW3XLvbGJSIip2PSlrqERfOJiIiIfpd9LAfXXjkFAwcMgKGlBZkffYy/P/MMXn3mafj4+EjbDU9JwaK7FkqPvb3ZLCdyFZz9nYi6KicvDwWFhdA2afFG5hpMyRiHpIQEZ4dFboatQ+oSFs0nIiIi+t3yJY+ZPF5010LM/fN9yCsowJDERGm5XO6N4KCgbo6OiKzB2d+JqCuO5eZi9br1qKmtg5dMhuycHBQUFmLBnNlM3JJNmLQlIpsZ63zp9XrWMiYiIupAfUMjAMD/kp552cdyMPfPf4bKT4UhiYmYd+stCAoMdEaIREREZEdbs7JQ39gIpUIBQRAQFRGBktJSbM3axaQt2YRJW7IJu/gT8Pusu0DXahlzkgciIvJkoiji7Q8+wOCEBPS7UNMdAEampmLs5aMQERaKktJSvP/xRjy5ciVe/Oc/pRugF9NqtdBeuEkKAA2NrYlgg2iAwWBw/Ash8kAGgwEQf/9/e39L1mxDjmMwGCCKIt93cjmWPhuKiovhq1SivqEBACAA8FUqUVR8nr/PbsCVzhGTtmQ1dvEnI3vVMjYmfy+d5MFgaEFzSwsuHzYMo4YPb3f/TOYSEZGre/2991BQWIhn/vY3k+XjrrhC+n+/mBjE9x+APy1ahD0HD2JMWlqb/axcuRLLly+XHnt7y3HNjBkoLS2F70V1conIenq9XroZUlpa2u4NE2u2IccxGAyorq6GKIqQyWTODodIYumzISggAMUlGrS0tEAQBDRptaitr0efXtHQaDTOCJlsUFtb6+wQJEzaktXYxZ+M7FXL2NwkD/t/+w17DhzA3oOH8PHnXwDgjL1ERORe3ngvE7/uP4CVf30SYaEhHW4bEhyE8LAwFBWXtLt+6dKlWLx4sfS4obERCx95FOHh4fDz9bVr3EQ9hU6ng/LCiLHw8PB269Vasw05jsFggCAICA8PZ9KWXIJxpGiLKBo72qJFFCETWx8ZOxdNnTIF54pLUFVbCy+ZDJVVVQj098fUyVMQERHhvBdAVvFxoRviTNqS1YqKS+Dr4/N7F39BgK+PEkXFxU6OjNyVuUkeJo8bh1HDh3PGXiIicjuiKOKNzEzs3rsPK598AlFWfDmrqa1FWUUFQsxMTKZUKqXEEdDa0xYAZIKMiQyiTso9dQoFZ1vLvr35/gftln2TyWSt45ov/J9/b91PEAS+9+QyDmRn/14m0Kf1upz50UfSemPnossGDcIfZ83Ei6tWQavVInlwEqZkZCBx4ECnxE22caXPGyZtyWq9oiJx6MhRiKIIQRAgiiIam7RIGGB+mDxRZ3DGXiIiclevrX4PO3bvxpMPL4Kvjw8qq6oAAH5+flAqFGhsasLaTZuQnpaG4KAgaErLkPnRhwhQq3HFyBHODZ6oh2DZNyLqDFvKBCbGxyP2Qj37u+bN43dZ6hQmbclqE9PTkXcqH+WVlfCSyVCs0UCtUmHi2HRnh0ZERETkEr764QcAwBP/XmGy/KG7FmJKRgZkMhlOF57F1p07UV/fgOCgICQPTsJj99/PUgdE3YRl34ioM+xVJpDIWkzaktWSEhIw/+Iu/kmJ7OJPREREdJHP31/T4XqlQoF/LHmsm6Ihovaw7BsREbkDJm3JJuziT0RERERErsY4QZA5xgmCAJZ9IyL3YMvnGnkmJm2JiIiIiIjIre07fBg7du+GwSAiLz8fABDfvz9kstaZxIwTBAEs+0ZEzmdNQtaWzzXyTEzaEpHDGS9Ier0eTVotAKBYo4Fc3jr7teLCv0REREREnWGcIEiv1+P1zEwAwJ2zZkrtzYsnCLK27FtOXh4KCguhbdLijcw1mJIxjjVviahDlr77GnvHWpOQteVzjTwTk7ZE5HDGCxIA+CiVAIDVGzZI6/vH9G23QcyGMhERERFZwzhBkE6nk9qbURERZsu5WSr7diw3F6vXrUdNbR28ZDJk5+SgoLAQC+bMZnuUiMyy9N3X2DvWmoSsrZ9r5HmYtCWrWHu3iKg9xgtSe07mF+Cjzz9v0yCekjEO323bzoYyEREREVnNXjf9t2Zlob6xEUqFAoIgICoiAiWlpdiatYttUSIyq6PvvsDvvWOZkCVrMGlLJszVVfll/378euAAvL28OrxbRNQe4wWpPZ98+SWatNo2DeLPv/0OLS0tbCgTERERkVXs2Tu2qLgEvj4+qG9oAAAIggBfHyWKiosdEToReYiOvvsS2YpJWzJhrq6KwWCAAGB4SgpGDR/e5nmspUKdZa5BXKwpRWR4OBvKRERERGQVe/aO7RUViUNHjkIURQiCAFEU0dikRcIA8z3oiIiI7IlJWzJhbV0VInsx1yAOCQpCY1MTG8pEREREZBV79o6dmJ6OvFP5KK+shJdMhmKNBmqVChPHpts7bCIionbJnB0AuRZ/tRrRkZGIioiAj1IJH6USURERiI6MRHRkJBO2ZHcT09Oh8vWFVqeDXq9HsUYDlZ8frrv6qnaXs6FMRERERO3pFRUp3fQHIN307x0VZfO+khISMH/WTAT4q+Ht7YXkpEQsnDsHiQMH2jtsIqIOGWt1Hz+Rhzcy1+BYbq6zQ6JuwqQtETmVuQbxNZMmsaFMRERERFYz1xmgszf9E+PjERsTg0Hx8bhr3jy2Q4mo211cq7u5pQXZOTl464O1TNz2EEzaEpHTmWsQs6FMRERERNZi71gicjeWetFeXKtbLpcjKiIC9Q0N2Jq1y0kRU3diTVtql/GDQ9ukxRuZazAlY5zNxfuJ7K22rg519fU4mV+A3fv2QVNWhoiwMIwaPgxB/v7w9fNDYECAs8MkIiIiIjsytgHNuXjeDeNNfwC4a948KBSKdvel1+vRpNUCAIo1Gs7hQUTd7uJetF4yGbJzclBQWIgFc2ZL+Rd71uom98OkLbVhzQcHkTPsO3wYW777HmfOnkVjUxMEQUD+6dPYd+gQosLDMX3qVNa8JSIiIvIw+w4fxo7du2EwiMjLzwcAxPfvD5lMAABkjB6NESkpViVjjfsCAB+lEgCwesMG6VgZo0djwpgx3fbaiKjnurgXrSAIiIqIQElpKbZm7ZJyL+Ym7uYE3T0Dk7bUhjUfHETOMCIlBXsOHERlZSVkMhkEQUDf3r1RVlGBsJAQDE9JdnaIRERERGRnI1JSMCguDnq9Hq9nZgIA7pw10yQha20y1rgvc9QqlaNeBhGRCWt60U5MT0feqXyUV1bCSyZDsUYDtUpl0lnJltEI5F6YtKU22P2eXJW/Wo3qmhqo1Wo06XQAAB8fH/irVKipq+WFiIiIiMgD+avV8FerodPppIRsVESESekDa5Oxxn0RETmbNb1ojbW6X1y1ClqtFslJiZiSkWFSq9ua0QgcQeCemLSlNtj9npyto1pjgQEBKCktNfn9rG9oQN/evXG+pAQyWdv5FXlnkYiIiMj12LN3GJOxRORurOlFC1iu1W3NaARyT0zaUhvWfnAQdZW55Oz+337DngMH4OXlbTK8rbm5GdW1tWhubkaTVguZTIazRUUwiAac12jw2ur3UFBYCIB3FomIiIhcna29wzhZMhF5Emt60VrDmtEI5J6YtKU27PXBQWSJudpjzc3NEAEMT0nGqOHDpe1/2b+/NZkr84IgE6DVaiFCREhgEARBQOqQy2AQDQB4Z5GIiIjI1dnSO4yTJRORJ7LUi9YWvLHleZi07aEsDUXqHRVltw8OInOsqT128TC3yePGmSRxjQwGA8rLyxEdFY1jubkAeGeRiIiIyNUZe4cdPnoUxRoNtE1abP76m3YTDZwsmYjIPN7Y8kxM2vZQloYipaelOTM86iFsrT1mbnuDwQAvQYC/mj1qiYiIiNyJtYkGTpZMRGQeb2x5JiZteyhjD8fjeXnIP3MaWp0Ofj4+GJOWhrj+sVDI5cjas8fZYRLZ5Pglw0HGpI1Er6gos9tf2pPXnpNhEBEREZFl1iYaunOy5I4mxQXYJiSi7mHLZ5G1N7aO5eZia1YWiopL0CsqEhPT05nUdWFM2vZQ/mo1zhYV4bOvv0F9QyO8ZDKcPH0aRSUluGXaNPTt05sNFHIreQUF2LjlS5NeGoePHkVQYCBUfn5WTW5h62QYRERERNQ11iYaunOyZHPzLhixTUhE3cGWzyJrbmwdy83FWx+sRX1DA3x9lDh89BhOFpxmCQUXxqRtD9beXe38M2fwzvp1iI2JYQOF3MrP+/ahobHB5Pf5fEkJIsPDcNv111uc3AKwbTIMIiIiIrLMUq8ua3vQdudkydbMu0BE5Gi2fBZZurF1vqQEH3/+OSoqKxES3DqRt8rPFxWVlfj48y+wYO4cqFUqjjx1MUza9mDt3dUOCgiAj1KJhXPntvscNlDIVWnKyuCr9EF9QyMASBehquoaREVESDchoiIioNXpUFdfL/1cSuXnZ7I9JzQjIiIisp01vbps6UFrz1nWO2LrvAtERLaytvSBtZ9Flm5sffrVV9h3+DeIooiqmhrpeQaDAZXV1Qj7KgRJCQkceepimLTtwdq7q63T65E0cCCiIyOdHR6RTSLCwnAiP7/dXho5l9S6DQkOxsmCfE7ER0REROQg1vTqio6M7NYetERErsIRZVg6urF1w9SpKCsvx6nTZ6TPZFEUUVFZhQGx/XDD1KlQq1QceepimLTtwbqzLhSRo10xYgTOFZegvLLK5Pe5f7++bWYk9vXxwS3TpqG5pbndifhOF541SfJOyRjHGj9ERETk8ew5Kas1vbrunT8fgOUetJwYjIg8jT3LsFjzGRkdGYlbrrvOZPRDY5MWIcHBuPW666SOe/5qNXQ6XYcjTzmBd/dh0raHqq2rQ1BgIK6/5mq8u349tDod4mL7YUxaGgIDAlBbV8c/MnIr8bGxmD/jNrz01lsmvTR+3Lmz3RmJt+/ejfKKCpOJ+ErKyjAlYxy+27bdJMlbUFjI4uxERETk8ew5Kas1vbqsTcZyYjAi8jT2LMNi7WdkUkICFsyZja1Zu1BUXIyBA+IwaWy6zSMbOIF392HStoe6+I86LjYWANCk1eLHnTvx486d/CMjtzSonV4a73+8sd0ZiU+cOgWlUtkmmfv5t9+hpaWlzfKtWbvaJG3tdYeRdyqJiIjI0axpb1g7Kas1+7KmV9e2XbusSjRwYjAiIvNs+YxMSkiw2Bnp0vKCl4485QTe3YdJWw9jaXZWIzZ8qKcwNyMxgHaTucWaUkSGh7dZXlRc3Gbf9rrDyDuVRERE5GjG9kZNbR3yT59Gi8GA4KAghIWEwF+tktob1gyNtbbtYqlXl7XfSTgxGBGRefb8jDyWm9umvOClI0+Nx7N0raCuY9LWQ9TW1eFQ9hFs+GwzGhub4KNUolijwZHjx3HNxIlISkiAys+vzfPYg488nbnazTG9euHMuXNtkrkhQUFobGpqd0KzSxm/aBzPy2u3Nu6lNz/M9UrpEx2N6ddcA4W3HGs2fgyAdyqJiIjIvkakpMBb5oX1mz8FBEAul8NLENCkbcL0q69G6pDLbNqXtb2sOurVxWQsEZFr2ZqV1W55wfZGnpLjMWnrIfYdPozMDz9ETU0tmltaIAgC/Hx9UV5ZidXrN6B3dDQqqqoAsAcfeZ7aujpoysrQpNO1qYcWFBiI26ZPx6r310i1bkenpaGxoRHrN3+KJq0WMpkMZ4uK4Ofni/TLL8fuPXutmqDPX63G2aIifPb1N21q4y6YM1sq5m5kqVdKeloa71QSERGRQ/ir1Th87Cj0+mb4KH0gCAJ69+qFktJSHD52DGOvGCVta2loLHtZERF5pqLiknZHpLY38pQcj0lbDzEiJQVbvvsevkofVFZXAwD69u6NqpoaKBUK3D1vHnvwkcfaf/g3fL99O5RKZbv10NLT0kxq3e7auxc7du+Gr48vAgP8odVqIUKEr48Pck+exMjUVNTW15kkeQMDArDz51+we98+aMrKEBEWhtEjRmD3vn2ora+36k6kpV4pCrkcWXv2dMdbRkRERD2QNV/GrRkaa2QpuUtERO7FXHnB9kae8hrgeEzaepCIsDAcP3HC5I9Lr9cjLjYW1bW1KNZooG3SYvPX3/CPiTzK8JRkhAYFIjQ0FDKZTFpe39CA+oYGKLzlJj1wjeUIVH5+7ZYNUcjlyC88A+D3JO/zn3yKM2fPorGpCYIgIP/0aew7dAiiKEKtUkEQWnvLdnQn0tgr5fDRo+3+PR4+epQXPSIiInIYa76MWzs01pbkLhERuQdz5QUvHXlq7TXA2nmXqH1M2roZczUxf9m/H2fPF6G2vh4tBgO8ZDLkFRTA28sLBYVnkHPiBBtU5LH81WpEhIUhIiLCJGlr7YzERsa/r/qGhjZJ3gC1Cio/P8hkMgiCgL69e6OsogIymQyiKFp1JxIwf3GbkjEO323bzr9TIiIiJzDXxjYyzgNh7XauqLauDilJg3E0N7dNeajkwUmorauDv1pt9dBY1j0kIvI8SQkJmD9rJl5ctUoaeTolI0OaQNLImmvAsdxcvPXBWtQ3NMDXR4nDR4/hZMFpfse1AZO2TmZs+J3MLzAZdj10yGWICAtr0xPwl/37sefAATQ0NuHM2bPSrK/BgYFQ+/lh0rixrfU9S8sQER6GK0aMwO69+5B78iQbVNTjWDsjsZGx5izQNsl7/EQefHx84OXl1brexwf+ahUAAQKAyupqizVwAfMXt8+//Q4tLS38OyUiInICS3XnjTd6jdvV1NYh//RpqS0eFhICf7XKpeeLMMbeXnmoH3fuRHNLCyaMGWP10FjWPSQi8kyJ8fEm5QXbq1du6RpwvqQEH3/+OSoqKxESHARBEKDy80VFZSU+/vwLLJg7p80cMNQWk7ZOtu/wYWz57vs2w65/2bcP3t7eCPD3R3lFhUlytsXQgpqaWpNZX/XNetw06Q9IHXJZm7v7X/+4FSo/PzQ2NQFgg4p6DltnJDaX5K1vaMC6TZuQf6YQLRcm+mtqakJ1TS369+uLkSmpWL/5U2h1OsTF9sPw5GRodTrk5ee3Kb+Qf6YQCrm8TTmFYk0pIsPDbf7iw+EmREREXWdsAxzPy0P+mdPQ6nTw8/HBmLQ0xPWPlW70jkhJgbfMC+s3f2rSFm/SNmH61Vcjdchldo3LXAeP0SNGmMRlTe9fa29mWzs01pa6h0RE5FksXQM+/eor7Dv8G0RRRFVNjfQ8g8GAyupqhH0Vgnvnz7fqWO48yqWrmLS1M1t/mUakpGDPgYOorKw0GXatKStDWEhI674uSc4GBQSioaHR4qyvRmxQEVnHXJJ3265dqKmrR0NjI5pbWqTyIxBFHMk5jrLyCsTFxgIAmrRafP7tdyivrISfr6/JTZewkBCUV1agWd/c5u8xJCgIjU1NNv2dcrgJERGRffir1ThbVITPvv4G9Q2N8JLJcPL0aZSUlWHBnNlSbyB/tRqHjx2FXt9sti1uTaLV2i+X5jp47Dt0CDG9e+PaK6cAgFW9f629mW3t0Fhrk7tEROQejNcvvV5vUi7w4onsjdcRS9eAG6ZORVl5OU6dPiP1tBVFERWVVRgQ2w83TJ1qcsyOrpnWjoax5TWac+loXGdj0tYCS79ARcXF+GX/fqmXW2BAIE4W5Fs9ZMpfrUZ1TQ3UajWadDoArcOuAwP8oSkrg1KpbNMgLDx3DkGBgVb3nGWDiqhrjD1TTuYX4Of9+6TyI0Mvuwzh7ZQxqW9owLHcXHz1w49teuFcNX4Cdu/di3PFxSZ/j1eOz8C3W7dZ/Xdq7+Em7LFLRERdZc9kpTOOaW2NVktDQq1JtF78faCja7C5Dh5lFRWIDA/DiJQUALB7719rhsZam9wlIiL30FG5QMA0OWrpGhAdGYlbrrvOpJNRY5MWIcHBuPW666TvqtZcM60dDWPLa+woZzd8yBC7vJ/2wKStBR39AoWGhECn08HLy0vq5eajVOKKESOw89dfTBpNDY0NyLhiFPpER+N8SYnJMQIDAlBSWtqmhx2AdhuETU1NVvfIq62rQ1BgIK6/5mq8u369NHx7TFoaAgMCpAkHiMg8Y8+U6MjIdnuzt+fbbdvQYjC0uelSUVWFP82Z3e7FrVdUlNVffOw13KS2rg6Hso9gw2eb0djYBB+lEsUaDY6dOIEZ109vt+QKERF5FnslPm1NVtqDLce0dIPS2hqtlkaxWZtotfYa3F4HD3+1ClXVNdJ5sdT711GsSe4SEZF7sHVOGEvXgKSEBCyYMxtbs3ahqLgYAwfEYdLYdJPvuNZcM60dDWPta7R0o7Pmou/XzuZRSdst332PTV9uQWVVNfr27o2Fc+fissRBXdpnR+ULGpuaIAAIDw250KjzgaasHD/u3AlRFE0aTflnzuCTr77C9t0/m2TzgwMDYTC0oLm5uc0sruGhvVGs0Zg0COsbGhHTuzcqq6s6nPXV6OI7JRcP3/5x5078uHOnS0+WQOTO2vvip5DLUXDmDKZdOQVREREAgOuvvhpyuRznS0rQOyrK7EXv0i+aKUmDOxxukpyYhFfffbfNF9NL9xMYEIgfftqBmppaNF+o1+vn64uq6mq8+cH7GJOWhpraOof1lro0nlHDh6NXVFSbxMGo4cMQ5O+PwvPF2HPwQJd7BFt7XFtfr70SH87orUZE3c8ebVdrRkpY2sZeiU9bkpXWfsZZit3aY1pTUsjakmKWRrFZm2jdd/gwMj/8sO01uKYG76xfh9tvu83qicHsNSmYtUNjbRlCS0RE7sHWOWGskZSQ0OH3NWuvmdaOhrHUbrCmzJExh+YKPCZp+9PPP+Ot99/HPfPnY3DCQHz941Ys+89/8MozTyMiLMzq/Rw/eRK/7NtncoLb+wUCgPKKCijkcpRXVgIA/Hx8oG9pgU6nk5IoQGujKSggAAaDAU3aJpNsflV1Nfz91QgOCoIgE0xmcW3SaiGKIrQ6ndQgbH2stTjrq5Gtd0qIqOtq6+oQGBCA4pISk4nLKqoq4eXlhbc+WNvucJP0tLR292fui+aUjHGoqqk1GW4SGBCAgQMGtOm1c+T4cQy9bAgOHsmGVquTliuVCii85QgPDUV1bS0AoG/v3jivKUFDQwN+2PETmrTaNl/ghw4ZguqaaotJYXPLRw0fjsbGpjZx7j14CAq5Nyqqqk2Ou/fgQfirVGgxGODl5dWmN5Kvr49JqRp7HdfW1xsYEIiD2dlmEx/dvR93WT5+zBiEBgbi2IkT2L5rl9Pj4XL7LjcmzciUPdquu/fuxadff2W2lyYAq3pyWpP4tGevUGuHQlpzPGuOaW1JIWtKilkzig1Au+2A2rp69I7uJXWyGBQXhwC1GnJvb1TXtF6DoyLCUVffAF8fHwyKi0NtXR1SkgbjaG5uhx027DWHhbVDY20ZQktERGSOue/Ol14zLd2ctGUEqaV9Db3MvpOKdoUgiqLo7CDs4ZGnnkJcbCz+/Mc/SsvufWwJrhgxHHfMmGHx+Q0NjZhx112Ii41Fk1YrJUBUfn4ICQrCmXPnUFdfD0EQEBcbi1OnC9DQ2ARAhEKukBpHOr0eQGtZA2OjKS42FiWlpfCSydDc0mKyn/MlJUiIG4A5N99sEk99QwPqGxpQdL5YmpV+eHIyhicnt1tD04h3tdtnMBig0WgQEREBmUzm7HDIzlzt/G7btQtbvvsehefOSROXtRgM8JLJEBUZgYnp6Rg1fLi0vfHvXeEtx5qNHwMA7rn9dsjlcmjKyrBpyxacOXuubY/afv0wMT0dh48dQ1FxMXpFRSEoMADf72jbc7ahsRGGlhbIvLykzyY/X98LN35av3AZDAaT0QQQBEAUpS/Uxi/w0ZGRqK2ra/NZOSVjHL7f8ZNJEtlYMiZrz6/SxbNJq229iaXTQqvVSRfnjuI5W1TUWsdbENpsr1Aq4KNQQiaTSfv39fVBetrl+HnfPpM4W1papBIzzVYctzOv11+txvniYtQ3NrrEftp7/829P+b2Y6/9m93exwcpg5Nw5PhxNGl13RY/l3fPcm+ZDK88+wyqq6sREBDglM9lV9SVtqux3RoaEoy6unrTzzOdDgH+atx+220A0G5Pzou3MSbW/rryadTV16OsogJA6wQf1TXVUPmp8K+lj2Pbrl0W9zUiJQWvv5eJ4ydOtPnsShw4EHffPk/qpWl+u3jcffvtZnuhXhp7x/tqPeb7H3+Mb7ZugyiKJu0E4+f91RMnSCWFDh89KpUqGj1yZJtSRdt27eqwB07G6NEA0G47wNvLC3379MYfprT2XJbe09o6qeeQscNGoL8a8y6cwx27d6O2rh7llRXQarVQKpUICwmBWvX7fBnHcnPx5pr3caaoCF4yGYICA6FWqbBw7hybasxaO6lyT57Jm7qPq7XxiajVxaMtXs/MBPD791fAtmuAue/Ol14zX333XRw6ctQkn1ZSWorUwYNx7x/nW9VOMbZ5LO2rpqYGgYGBLtF29Yikrb65Gbfc+Sc8/sADGJ02Ulq+KnMNTp05jaf/+tc2z9FqtdBeGMoDtCZN/vjQQwgKCkZY0O+JkfKqKgQFBKCpqQnlF3of+KtU8FEqkTx4MH47ehRanQ5KpbK1EaVQIHnwYBw+cgSa8nJpez9fXylRYqw/OaBvX1TX1kLl54snH37YJL6dv/yKXXv2mH3NY9LSMHbU5V1963oMg8GAsrIyhIWF8YLvgVzt/NbW1aG+oQH5Z85g78GDKC0vR3hoKNKGDkNs3xio/PxMLmId/b0Xnjsn9eZv74vmpHHj8KfZs6TlxRoNXlz15oW7k629faIiI1FTU4PqmhoE+PtLkxhGRUaivKICjY2NkMlkMIiiSYLZIIrw8/WF/sLNqJhe0aisrkFtbS3kSgVCg4Klz8qyigrpMy4sJERaXqzRQHeh51PLhZj9LowkaDEYoFar0Xxh/1GRkairq0VVTS2CAwOlO5/G45ZXVCDoouXG+Gvr6qS4L94/ACgUCkRFRJjEqdPp4Ofra/I+NDQ0oKq6GqHBwai98CV0QN++KKusRFV1NRQKOYIDgyy+3vKqKuh1OqhVqjbx2/K+VVRXoVmnR4C/f5fi6ej9N/f+tLcfe+2/o+1FUYRSqbR6e3vEz+Xds/x8cTE2Zb6HqqoqBAYGdvwB2kPY2nY1126Ni+0PnbYJVRd6acb0ikZtfWvC/C/33QcA+M8rr6CxsdHsNsYE3Op165F76pR0o65PdBTKK6swcMAAzJ85A/UNDe1eXxoaGuCjVOKhuxYiL78A327bhnPnz7f54tUnOhpXTpggtV///cILqKtvQEVVFYC27WJz17OLjxcVEYGdv/xq8Zjx/WOR+eFHOH220OSzs6K6Cv1j+mLerbdA5eeH+oYG6PV6rF7f2mN0/swZ0hdQ47XbeI03x9ipwpp2QG1dHbJzcvDpl1+hsalJ+j7h6+ODG//wB6lUhrXHO3HqFNZu+gQ6nQ4pgwdj1PDh7bY7iNyFq7XxiaiVPfNV1n53Pn7yJN7b8CHOnT8PL5kMgf7+UKlUuP2225AQNwC1dXVWt3myc3LwyZYvUVxaapKzM157a2trcdngwa7RdhU9QFlFhThtzlzx6PHjJss3fLpZvPuRR9t9zlNPPSUCkH68vLxNHvOHP/zhD3/4wx/+8Md+PydPnuyOZqFbsLXtynYrf/jDH/7whz/84U/3/hQWFnZX09Asj6lpC0CqIWskQmwd3tuOpUuXYvHixdLjqqoqDBgQh/z8U87PpJPd1dTUICYmBoWFhU7v3k72x/Pr+XiOPRvPr2errq5G3759ERIS4uxQXI61bddL260GgwGjRo3Cr7/+2mYf1FZaWhr2dNAjyJO482t1h9hdJUZnxtGdx2b7gDyRq3yOkHmiKGLEiBHo1auXs0PxjInIAvz9IZPJUFlVbbK8uroGQYHtf7grlUooLxTNN2ppaUZgYCAvCB4sICCA59eD8fx6Pp5jz8bz69k4tPV3trZd22u3enl5saOBlby8vHrMZ4s7v1Z3iN1VYnRmHM44NtsH5Elc5XOEOqZQKFyi7er8COxA7u2N+P6xOJCdbbL8YHY2kmwovE9ERERE5Gj2aLved6FmLVnWk94rd36t7hC7q8TozDhc5T0gclf8G3IPrnKePGIiMgD46eef8fxrr+PPd/4RifHx+HrrVny7dRteeeZpRISFWXy+K80OR/bH8+vZeH49H8+xZ+P59Ww8v+3ratuViMjT8fpBRD2dR5RHAIBxV1yBmto6rP/kU1RUVaFfnz546i+PWt3oVSqVeOqpp9oMPSPPwPPr2Xh+PR/PsWfj+fVsPL/t62rblYjI0/H6QUQ9ncf0tCUiIiIiIiIiIiLyBB5R05aIiIiIiIiIiIjIUzBpS0RERERERERERORCmLQlIiIiIurhbrzxRgQHB+OWW25xdiguj++Ve+B5IiJPVlhYiAkTJmDw4MFISUnBRx995OyQqB1dPU+saUtERERE1MNt3boVdXV1eO+99/Dxxx87OxyXxvfKPfA8EZEnO3/+PEpKSjB06FBoNBoMHz4cx48fh0qlcnZodJGunif2tAXw6quvon///vDx8cGIESPw008/OTskssKOHTtw3XXXoVevXhAEAZ9++qnJelEUsWzZMvTq1Qu+vr6YMGECjhw5YrKNVqvFAw88gLCwMKhUKlx//fU4e/ZsN74KMmflypVIS0uDv78/IiIicMMNN+D48eMm2/Acu6/XXnsNKSkpCAgIQEBAAEaPHo2vvvpKWs9z61lWrlwJQRCwaNEiaRnPsXtbtmwZBEEw+YmKipLW8/y6n4kTJ8Lf39/ZYbgFvlfugefJc7GHIREQHR2NoUOHAgAiIiIQEhKCiooK5wZFbXT1PPX4pO2GDRuwaNEiPPnkkzhw4ADGjRuHqVOn4syZM84OjSyor69HamoqXn755XbXP/vss3j++efx8ssvY8+ePYiKisKVV16J2tpaaZtFixbhk08+wfr167Fz507U1dVh2rRpaGlp6a6XQWZs374d9913H37++Wd89913aG5uxlVXXYX6+nppG55j99WnTx88/fTT2Lt3L/bu3YtJkyZh+vTpUlKH59Zz7NmzB6tWrUJKSorJcp5j93fZZZfh/Pnz0s9vv/0mreP5bWXpBpU9WLqJbeROnRTau9FjD574XjnDuXPnMHfuXISGhsLPzw9Dhw7Fvn377LZ/nieyxNvbGy+++CKOHj2K77//Hg8//LDJdwSirrCm81BX2ftzbu/evTAYDIiJibFrnK7MHdtYnTpPYg93+eWXi/fcc4/JssTERPHxxx93UkTUGQDETz75RHpsMBjEqKgo8emnn5aWNTU1iYGBgeLrr78uiqIoVlVViXK5XFy/fr20zblz50SZTCZ+/fXX3RY7WUej0YgAxO3bt4uiyHPsiYKDg8W33nqL59aD1NbWigMHDhS/++47cfz48eJDDz0kiiL/fj3BU089Jaampra7juf3d5999pm4ZcsW8fjx4+Lx48fFJ554QpTL5WJ2dna7cQhYyAAAjg9JREFU2+/cuVPU6XRtlh87dkw8f/58u8/58ssvxSeffFLcuHFjm/aQ0fr160W5XC6++eab4tGjR8WHHnpIVKlU4unTp02227p1q3jzzTfb/kLt6NdffxVjY2PFlJQU6TOjPXyvnKOiokLs16+fOH/+fPGXX34R8/Pzxe+//17My8trd3ueJ+oOycnJ4pkzZ5wdBnmIq6++Wnz33XfF7Oxs8eDBg+K1114r9u3bV6yrq2t3e2d/zpWVlYlJSUliVlaW7S/WjblbG6uz56lHJ221Wq3o5eUlbtq0yWT5gw8+KGZkZDgpKuqMS/+ATp48KQIQ9+/fb7Ld9ddfL95+++2iKIriDz/8IAIQKyoqTLZJSUkR//73vzs8ZrLNiRMnRADib7/9Jooiz7EnaW5uFtetWycqFArxyJEjPLce5PbbbxcXLVokiqJokrTlOXZ/Tz31lOjn5ydGR0eLsbGx4owZM8STJ0+Kosjza4nxBtWlWlpaxNTUVPGWW24Rm5ubpeXHjx8Xo6KixGeeecbivs19obC2k4KzE1zmbvRciu+V8yxZskQcO3asVdvyPJE527dvF6dNmyZGR0ebPcevvPKKGBsbKyqVSnH48OHijh072t3Xnj17xMsuu8zBEVNPdmnnoYs5+3OuqalJHDdunJiZmWnDK/JcrtrG6sp56tHlEcrKytDS0oLIyEiT5ZGRkSguLnZSVGQPxvPX0bktLi6GQqFAcHCw2W3INYiiiMWLF2Ps2LEYMmQIAJ5jT/Dbb79BrVZDqVTinnvuwSeffILBgwfz3HqI9evXY//+/Vi5cmWbdTzH7m/UqFHIzMzEN998gzfffBPFxcUYM2YMysvLeX7NaGlpwfr161FfX4/Ro0e3WS+TyfDll1/iwIEDuP3222EwGHDy5ElMmjQJ119/PR577LFOHVen02Hfvn246qqrTJZfddVV2LVrV6f26Sj33Xcfrr32WkyZMqXD7fheOc9nn32GkSNH4tZbb0VERASGDRuGN998s91teZ7IHEtl7qwtYVheXo7bb78dq1at6o6wqYeqrq4GAISEhLRZ58zPOVEUMX/+fEyaNAnz5s3r1HE8hSu3sbp6nrw7FZmHEQTB5LEoim2WkXvqzLnl+Xc9999/Pw4fPoydO3e2Wcdz7L4GDRqEgwcPoqqqChs3bsQdd9yB7du3S+t5bt1XYWEhHnroIXz77bfw8fExux3PsfuaOnWq9P/k5GSMHj0acXFxeO+993DFFVcA4Pk1+u233zB69Gg0NTVBrVZLN6ja06tXL/z444/IyMjA7NmzsXv3bkyePBmvv/56p49vbSeFq6++Gvv370d9fT369OmDTz75BGlpaZ0+rq2MN3r27Nlj1fY9+b1yplOnTuG1117D4sWL8cQTT+DXX3/Fgw8+CKVSidtvv73N9jxP1J6pU6eaXEcu9fzzz+NPf/oTFixYAAB48cUX8c033+C1116TbgZrtVrceOONWLp0KcaMGdMtcVPP017noUs563MuKysLGzZsQEpKilRrdc2aNUhOTu70cd2NO7SxunqeenTSNiwsDF5eXm16dGg0mjZvOrkX4wzWxcXFiI6OlpZffG6joqKg0+lQWVlp0tNHo9Hwwu9CHnjgAXz22WfYsWMH+vTpIy3nOXZ/CoUC8fHxAICRI0diz549eOmll7BkyRIAPLfubN++fdBoNBgxYoS0rKWlBTt27MDLL78sTebAc+w5VCoVkpOTceLECdxwww0AeH6NzN2gMvelom/fvsjMzMT48eMxYMAAvP3223ZJZFtKon/zzTddPkZnWXuj51I98b1yNoPBgJEjR2LFihUAgGHDhuHIkSN47bXX2k3aAjxPZBtjz7XHH3/cZDl7GJIzdNR56GLO+JwbO3YsDAZDl4/hztyhjdXV89SjyyMoFAqMGDEC3333ncny7777zuO+MPQ0/fv3R1RUlMm51el02L59u3RuR4wYAblcbrLN+fPnkZ2dzfPvAkRRxP33349Nmzbhxx9/RP/+/U3W8xx7HlEUodVqeW49wOTJk/Hbb7/h4MGD0s/IkSMxZ84cHDx4EAMGDOA59jBarRbHjh1DdHQ0/4YvYbxBNXLkSKxcuRKpqal46aWXzG5fUlKCu+66C9dddx0aGhrw8MMPd+n47tBJ4eIbPd7e3vD29sb27dvxv//9D97e3mhpaWn3eT3xvXK26OjoNl+Gk5KS2gxbvxjPE9nClp5rn376KYYOHYqhQ4fit99+c0a45MGMnYe2bt1q0nmoPfycc46e0Mbq0T1tAWDx4sWYN28eRo4cidGjR2PVqlU4c+YM7rnnHmeHRhbU1dUhLy9Pepyfn4+DBw8iJCQEffv2xaJFi7BixQoMHDgQAwcOxIoVK+Dn54fZs2cDAAIDA/GnP/0JjzzyCEJDQxESEoJHH30UycnJFmupkePdd999WLt2LTZv3gx/f3/pgzAwMBC+vr4QBIHn2I098cQTmDp1KmJiYlBbW4v169dj27Zt+Prrr3luPYC/v3+bIWQqlQqhoaHScp5j9/boo4/iuuuuQ9++faHRaPCvf/0LNTU1uOOOO/g3bIHxBlV7ysrKMHnyZCQlJeGjjz7CiRMnMGHCBCiVSjz33HOdOt7FnRRuvPFGafl3332H6dOnd2qf9ma80XOxP/7xj0hMTMSSJUvg5eXV5jk99b1ytvT0dGm0hFFubi769evX7vY8T9RZ7GFIziKKIh544AF88skn2LZtW5vOQ5fi55zr8Mg2ls1Tl3mgV155RezXr5+oUCjE4cOHtzsrILmerVu3igDa/Nxxxx2iKIqiwWAQn3rqKTEqKkpUKpViRkaG+Ntvv5nso7GxUbz//vvFkJAQ0dfXV5w2bZp45swZJ7waulR75xaA+O6770rb8By7rzvvvFP63A0PDxcnT54sfvvtt9J6nlvPc+lM8DzH7m3GjBlidHS0KJfLxV69eok33XSTeOTIEWk9z2+rpUuXijt27BDz8/PFw4cPi0888YQok8lMPu+MWlpaxBEjRoh/+MMfRK1WKy0/fPiwGBoaKj7//PPtHqO2tlY8cOCAeODAARGA+Pzzz4sHDhwQT58+LW2zfv16US6Xi2+//bZ49OhRcdGiRaJKpRILCgrs/6Lt5NLPjIvxvXKeX3/9VfT29hb//e9/iydOnBA/+OAD0c/PT3z//ffbbMvzRNbAJTOya7Va0cvLS9y0aZPJdg8++KCYkZHRzdFRT3TvvfeKgYGB4rZt28Tz589LPw0NDW225eec8/SUNhaTtkREREREDmDpBtWlvv32W7GxsbHN8gMHDphNaFu6iW3kbp0UOkraiiLfK2f6/PPPxSFDhohKpVJMTEwUV61aZXZbniey5NKkrSiK4uWXXy7ee++9JsuSkpLExx9/vBsjo57Kms5DF+PnnHP0lDaWIIqiaJ8+u0RERERERERE5l1c5m7YsGF4/vnnMXHiRKnM3YYNGzBv3jy8/vrrUgnDN998E0eOHDFbioOIyBMxaUtERERERERE3WLbtm2YOHFim+V33HEHVq9eDQB49dVX8eyzz+L8+fMYMmQIXnjhBWRkZHRzpEREzsWkLREREREREREREZELkTk7ACIiIiIiIiIiIiL6HZO2RERERERERERERC6ESVsiIiIiIiIiIiIiF8KkLREREREREREREZELYdKWiIiIiIiIiIiIyIUwaUtEZMEvv/yCG2+8EX379oVSqURkZCRGjx6NRx55xGS7CRMmYMKECU6JcfLkybjnnnuccuy//e1vGD58OAwGg1OOT0RERERERORpBFEURWcHQUTkqrZs2YLrr78eEyZMwMKFCxEdHY3z589j7969WL9+Pc6ePStte/ToUQDA4MGDuzXGzZs3Y8aMGTh58iR69+7drccGgOrqasTGxuL555/HH//4x24/PhEREREREZGnYdKWiKgD48ePx7lz55CTkwNvb2+TdQaDATKZ8wcsjBo1CgMGDMC6deucFsMDDzyAb7/9Fjk5ORAEwWlxEBEREREREXkC52cbiIhcWHl5OcLCwtokbAG0SdheWh5h/vz5EASh3Z9ly5ZJ29XU1ODRRx9F//79oVAo0Lt3byxatAj19fUW4ztw4AB+/fVXzJs3z2T56tWrIQgCtm7dinvvvRdhYWEIDQ3FTTfdhKKiIpNtY2NjMW3aNHzxxRcYNmwYfH19kZSUhC+++ELaV1JSElQqFS6//HLs3bu3TRzz5s1Dbm4utm7dajFmIiIiIiIiIuoYk7ZERB0YPXo0fvnlFzz44IP45ZdfoNfrrX7u3/72N+zevdvkZ+7cuQB+L6HQ0NCA8ePH47333sODDz6Ir776CkuWLMHq1atx/fXXw9JgiC+++AJeXl7IyMhod/2CBQsgl8uxdu1aPPvss9i2bZsUw8UOHTqEpUuXYsmSJdi0aRMCAwNx00034amnnsJbb72FFStW4IMPPkB1dTWmTZuGxsZGk+ePGDECarUaW7Zssfr9ISIiIiL74lwMHeNcDETkTtp2HSMiIsnTTz+NnJwc/N///R/+7//+D3K5HGlpabjuuutw//33Q61Wm31uXFwc4uLipMcfffQRPvjgAzzxxBO47bbbAAD/+9//cPjwYfzyyy8YOXIkgNaGbO/evXHLLbfg66+/xtSpU80eY/fu3Rg4cKDZOK655hr873//kx5XVFTgscceQ3FxMaKioqTl5eXl+Pnnn6WauL169cLQoUPx5ptvIi8vD35+fgAAQRBwww034Pvvv8d1110nPd/LywupqanIysoyGysREREROc7FczE8++yzbeZi+O9//ytt++qrrzolxs2bNyMrKwuZmZlOOf6jjz6Kl19+Ge+99x7nYiAil8eetkREHQgNDcVPP/2EPXv24Omnn8b06dORm5uLpUuXIjk5GWVlZVbtZ/v27Zg3bx7mzp2Lf//739LyL774AkOGDMHQoUPR3Nws/Vx99dUQBAHbtm3rcL9FRUWIiIgwu/766683eZySkgIAOH36tMnyoUOHmkxilpSUBKC1F4YxYXvx8kufDwARERE4d+5ch/ESERERkWM8++yz6N+/P7755hvMnDkT48ePx8yZM/Hcc8/hzJkzJtsOHjy42yfPBYAVK1bgxhtvdMrkuQAQGBiIuXPn4umnn7Y4oo2IyNmYtCUissLIkSOxZMkSfPTRRygqKsLDDz+MgoICPPvssxafe+TIEdxwww0YN24c3n77bZN1JSUlOHz4MORyucmPv78/RFG0mBRubGyEj4+P2fWhoaEmj5VKpfS8i4WEhJg8VigUHS5vampqcywfH582+yUiIiKi7sG5GDgXAxF5FpZHICKykVwux1NPPYUXXngB2dnZHW579uxZXHPNNejbty82btwIuVxusj4sLAy+vr5455132n1+WFhYh/sPCwtDRUWFbS/AQSoqKizGS0RERESOMXr0aLz11lt48MEHMWfOHAwfPrxN29Ocv/3tb23qzL7yyit4//3328zFcPbsWTzxxBNISUnBkSNH8Pe//x2//fYbvv/+ewiCYPYY1szFcO2112Lt2rUoLCzEX/7yF8ydOxc//vijyXbGuRiefPJJBAYGYvny5bjpppuwdOlS/PDDD1ixYgUEQcCSJUswbdo05Ofnw9fXV3r+xXMxTJo0yar3h4jIGZi0JSLqwPnz5xEdHd1m+bFjxwC01n41p7q6GlOnToUgCPjyyy8REBDQZptp06ZhxYoVCA0NRf/+/W2OLzExEZ9++qnNz3OEU6dOYciQIc4Og4iIiKhH4lwMnIuBiDwLk7ZERB24+uqr0adPH1x33XVITEyEwWDAwYMH8d///hdqtRoPPfSQ2efOnj0bR48exapVq1BYWIjCwkJpXZ8+fdCnTx8sWrQIGzduREZGBh5++GGkpKTAYDDgzJkz+Pbbb/HII49g1KhRZo8xYcIEvPPOO8jNzUVCQoJdX7stysvLceLECTzwwANOi4GIiIioJzPOxbB371788MMP2Lt3L7Zt24alS5fijTfewJ49e6waFWXtXAxGF8/F0FHStitzMVyctLXXXAx79uwxGwsRkStg0paIqAN//etfsXnzZrzwwgs4f/48tFotoqOjMWXKFCxdulRqDLbnyJEjMBgMWLBgQZt1Tz31FJYtWwaVSoWffvoJTz/9NFatWiUN3+rbty+mTJmC2NjYDuObPn061Go1Nm/ejL/85S9dfbmdtnnzZsjlcqknBhERERE5x8iRI6WesHq9HkuWLMELL7yAZ5991uJ8DJbmYsjLyzNbcsGauRgiIyPNrudcDEREppi0JSLqwG233WZ1InLbtm0mjwsKCqx6nkqlwj//+U/885//tDE6ICAgAPPnz8e7776LRx99VKojNn/+fMyfP7/N9hMmTGgzU665ONubUTc2Nrbd5e+++y5uvfXWDhviRERERNS9OBdD+zgXAxG5A5nlTYiIyJX99a9/xblz57Bx40anHH/Hjh3Ys2dPp5LORERERGQf58+fb3e5PediOHnyJEJDQ6XevBf/WBohlpiYiFOnTln/ghzo1KlT0gRrRESuij1tiYjcXGRkJD744ANUVlY65fjl5eXIzMzEgAEDnHJ8IiIiIuJcDNbiXAxE5C6YtCUi8gDTpk1z2rFvvPFGpx2biIiIiFpxLgbrcC4GInIXgthecUIiIiIiIiIiIjt64IEH8MMPP+DIkSPSXAzdbdy4cejbty8++OADpxyfiMhaTNoSERERERERkcOVlJQgISEBb7/9Nm655ZZuP/6OHTtw1VVX4ejRoyztRUQujxOREREREREREZHDGediaGxsdMrxORcDEbkT9rQlIiIiIiIiIiIiciHsaUtERERERERERETkQpi0JSIiIiIiIiIiInIhTNoSERERERERERERuRAmbYmIiIiIiIiIiIhcCJO2RERERERERERERC6ESVsiIiIiIiIiIiIiF8KkLREREREREREREZELYdKWiIiIiIiIiIiIyIUwaUtERERERERERETkQpi0JSIiIiIiIiIiInIhTNoSERERERERERERuRAmbYmIiIiIiIiIiIhcCJO2RERERERERERERC6ESVsiIiIiIiIiIiIiF8KkLREREREREREREZELYdKWiIiIiIiIiIiIyIUwaUtERERERERERETkQpi0JSIiIiIiIiIiInIhTNoSERERERERERERuRAmbYmIiIiIiIiIiIhcCJO2RERERERERERERC6ESVsiIiJyGEEQLP6sXr26w31s27YNK1as6FIMzz33XKef70kKCgqwbNkyFBUVtVkuCAI+/vhjq/e1bds2CIKAvXv32jtMq5SVlVn1+3OpgwcPYtmyZWhoaHBIXI7Y/8cffwxBEFBQUGDT886fP4/HHnsMQ4cOhb+/P3r16oVbb70VeXl5JtstW7bM7N/nPffcY7LtF198geHDh0OpVCImJgZPPfUUWlpaTLb57rvvMHv2bMTFxUEQBNx///2det2dVVpaioceegijRo2CUqmEWq1ud7s33ngDV199NaKjoxEQEIC0tDR8+OGHbbbbsGEDbr75ZvTu3ZufJ0RERNRtmLQlIiIih9m9e7fJDwA88MADJsuuvfbaDvfR1aQt/a6goADLly9vk7SNjo7G7t27MWnSJCdF1n0OHjyI5cuXOzRp68j922Lfvn3YuHEjbr31VmzevBn/93//h1OnTuHyyy/H2bNnpe0WLFjQ5m/1mWeeAQBMnTpV2u7nn3/G9OnTkZSUhM8++wyLFy/Gf/7zHyxZssTkuF999RUOHjyI8ePHIygoqFte68XOnTuH9evXIyIiAiNHjjS73b/+9S/06dMHr776KjZt2oTRo0djxowZeOWVV0y2+/jjj3Hq1Clcd911jg6diIiISOLt7ACIiIjIc11xxRVtlvXt27fd5a5Ep9PB29sbMpln3N8WRRE6nc7seqVS6fLnhGw3duxYHD9+HN7evzf5MzIy0KdPH7z99tt46qmnAAB9+vRBnz59TJ77+uuvIzg42CRpu2zZMgwdOhQffPABAODqq69GS0sLnnjiCfzlL39BZGQkAOC5557D888/DwD48ccfu/QaYmNjsWzZMsyfP9/q56SkpKCkpESK+dChQ+1ut3//foSHh0uPp0yZgrNnz+K5557DfffdJy3fsGGD9FnwxhtvdOJVEBEREdnOM76JEBERkVsyGAxYsWIF+vfvD6VSiYEDB+LFF1+U1i9btgzLly9HfX29NFx7woQJAICcnBzMnDkTMTEx8PPzw+DBg/Hf//4XBoPB5jhiY2Nx//334z//+Q/69esHX19flJeXW3WMyZMn4/bbb5ceHzx4EIIg4KabbpKW5eXlQRAE7Ny502wMEyZMwLRp05CZmYm4uDj4+vpiwoQJOH78uMl2//3vf5GWlobAwEBERERg2rRpyM3NNdlm/vz5GDJkCL788kukpqZCqVTis88+w8SJEwEAaWlp0vsJmC+PkJmZiWHDhsHHxwdhYWH4wx/+gNOnT5t9DaL4/+2ddVhVWfv3v4c4waFLSkFQEDtATEBs7LG7W0cdAxUVsB/FjrFjxu7WccZWVFBCCUEpRbExEan7/YP37Id9GsRx5vesz3Wd64K1V+971b3XuhchNDQUrq6uEIlEcHZ2xooVK3h+MjIy0KNHD5QrVw5isRgVK1bEpEmTVMYpY/PmzXBycoKBgQGaN2+ucMRfxo4dO1CzZk2IxWLY29sjMDAQ+fn53LPBgwcDAKysrCAQCODk5MTLW79+/WBpaQmJRAJvb2/cvXtXIQ1V9VIW8efl5WHixIkwNzeHiYkJhg4dis+fP2usH2WYmpryFLayfDk4OCjsti5OTk4Ojh49im7dukEoFHLuUVFRaN26Nc9vmzZtkJeXhz/++INz+9EfO7RNv7jCVkadOnUU6uZHl4fBYDAYDMb/JmynLYPBYDAYjB/G1KlTsXLlSsycORNNmzbFn3/+iUmTJuHjx4+YPXs2hg0bhoyMDOzZs4fbsWdsbAyg6Ai0m5sb+vbtCyMjI0RHRyMoKAifP3/GnDlzSpyXw4cPw9XVFatWrYKuri4MDAxw7949jWl4e3tj27ZtXDxXr16FWCzG1atXQUQQCAScW/369dXmITIyEsnJyVi8eDEAYNasWWjdujUSExMhEokAFCn+xo0bB0dHR3z48AEbNmxAo0aNkJSUBHNzcy6uZ8+eYcKECZg1axbKly8PCwsLrFu3DmPHjsX27dtRpUoVtXlZunQppk2bhqFDh2LBggXIy8vDxYsX8erVKzg6OioNM2HCBGzZsgWBgYHw8vJCWFgYAgICIJFIONuoAwYMwLNnz7B69WqUK1cOjx8/1mgX99SpUxgxYgQGDRqEXr164c6dO+jVq5eCv+XLl2PatGmYNGkSli1bhoSEBAQGBqKgoACLFy9Gu3btMGvWLMyfPx/nzp2DiYkJV69ZWVlo0qQJDA0NsWbNGpiYmGDNmjXw8/PDw4cPYW1trbFeyiL+GTNmYP369QgJCUHdunWxZ88eBAYGKpTV19cXaWlpJbZz++TJE6Snp8Pd3V1tfX/48AF9+vThuefk5PCUuAC48iUkJJQoH6qQKdiLU1hYyHPX0dH5borUa9euqa0bBoPBYDAYjL8NYjAYDAaDwfibAEBLly4lIqJXr16Rvr4+TZ06lednxIgRJJVK6ePHj0REFBQURFKpVG28hYWFlJeXRwsWLCBbW1uVaarC0dGRLC0t6fPnzyVO4+LFiwSAUlNTiYioa9euNHr0aNLV1aX79+8TEdHAgQPJx8dHbR58fHxIR0eHkpKSOLekpCTS0dGhjRs3Kg2Tn59P2dnZZGhoyPMzcOBAAkC3b9/m+b906RIBoIiICJ57amoqAaCDBw8SEdG7d+/IwMCARowYoTK/8nE9evSIBAKBQl6nTp1KNjY2VFBQQEREUqmUVq9erbYu5PHy8qKmTZvy3GbMmEEAaPv27URE9OHDBzI0NKQZM2bw/K1bt44kEgm9fv2aiIi2b99OAOjVq1c8f3PmzCETExN68eIF55aTk0MODg6cjGpTL98S/5s3b0gikdDs2bN5YRs1asSTMaIieXF0dFSZD1V07tyZLC0t6e3btyr9dOnShezt7bl3JsPDw4Patm3Lc/vtt98IgMo6cXR0pLFjx2qdPwAafwMHDtQ6Pm36DxnHjh0jAPT777+rzZ+m/oTBYDAYDAajLGBnfRgMBoPBYPwQbt++jby8PPTs2ZPn3rt3b3z+/BlRUVFqw+fk5CAoKAiVKlWCSCSCvr4+AgMDkZmZiU+fPpU4P76+vjAwMChxGg0aNIBQKMSVK1cAFO3Ua9++PerUqcO5Xb16Fd7e3hrzUL16dVSuXJn7v3LlyqhevTpu3brFud26dQstW7aEhYUF9PT0YGBggE+fPimYSLC0tNS4s1cVN2/eRHZ2NoYOHap1mL/++gsA0LVrV+Tn53O/5s2b4/nz53jy5AkAoG7duggNDcWvv/6q0sRBcQoKCnD37l106dKF596tWzfe/2FhYfj06RO6d+/OS9/Pzw9fvnxBbGys2nTOnz+PZs2awdzcnAurq6uLpk2bIiIiotT1UpL479+/jy9fviiUtWvXrgrxXb58ucS7bBctWoQTJ05g27ZtMDMzU+rn/fv3OHPmDHr16qWwm3Xs2LE4e/YsVq1ahbdv3+L69esIDAyErq5ume18jYiI4P1sbW0RFBTEcwsODi6TtIoTHx+PQYMGoXv37ujXr1+Zx89gMBgMBoNRUph5BAaDwWAwGD+ErKwsAICNjQ3PXfb/27dv1YYPCAjA5s2bERQUhHr16sHU1BTHjx/H/PnzkZOTA0NDwxLlR3Y8vaRpSCQSeHp64urVq/Dy8sLr16/RuHFjeHt74+rVq+jcuTNSU1Ph4+NTqjxYW1sjMzMTAPD48WO0atUKHh4e2LhxI+zs7CAUCtGuXTvk5ORojEtb3rx5AwCws7PTOszr169BRLC0tFT6/MmTJ3B0dMT+/fsRGBiIwMBAjBkzBm5ubli4cCHPBnBxXr16hfz8fIXyyC69Kp4+UKQUVpW+pvzfunUL+vr6Cs9cXFwAlK5eShK/7D1rKmtp2LlzJwIDA7F27Vp06NBBpb9Dhw7h69ev6Nu3r8KzgQMH4v79+5gyZQomTpwIoVCIoKAgrFy5UqEdlxYPDw/e/0KhEE5OTgruZUlGRgbatGmDGjVq4Lfffvtu6TAYDAaDwWCUBKa0ZTAYDAaD8UOQ2V998eIF7O3tOffnz5/znqvi4MGDGDlyJAICAji306dPlzo/sku5SpOGt7c3Dhw4AC8vL9SqVQsmJibw9vbGyJEjceXKFejr66Nhw4Ya8/Dy5UulbvXq1QMAnDt3Dp8+fcKRI0dgamoKoMgGqDIFt7LyaIuFhQWAIru4Dg4OWoUxNzfnLluTt3sKAG5ubgAAW1tbbNu2DVu2bMHdu3cxf/589OzZE4mJiXB2dlYIZ2VlBT09PYW6efHihUL6AHDkyBGUL19eIZ6KFStqzH+bNm0wb948hWcyu62lqZeSxG9rawug6J0XbxPyZS0pJ06cwLBhwzBjxgyMGTNGrd89e/agSpUqqFOnjsIzgUCAZcuWYc6cOUhPT0eFChWQl5eHwMBANGjQ4Jvy+KN48+YNWrVqBWNjYxw/fhxisfhHZ4nBYDAYDAYDAFPaMhgMBoPB+EHUr18f+vr6OHDgAG935P79+yGVSjk3oVCIr1+/KoT/8uULTzlYUFCAffv2lWketU3D29sbixYtwp49e7gdtU2bNsXLly+xadMm1KtXT8H0gjJiY2Px8OFDzkTCw4cPERsbyynavnz5AoFAwNuteeDAAaWXNylDVhb5XbnyNGzYEAYGBti+fbvWJhaaN28OoEgJpm4npwwdHR14enpi/vz5OHHiBB49eqRUaaurq4u6devi6NGjmDRpEud+6NAhnr9GjRrBwMAAGRkZCuYFiqOqDlq0aIFdu3bB3d0dUqlUaVht6uVb4q9RowYkEgmOHj3KU5oePnxYZXk0ceXKFfTs2RMDBgzAggUL1PrNzMzE5cuXNZofMDExQc2aNQEAc+bMgZOTE1q0aFHqPKqjpCYgSsKnT5/Qtm1bfPr0CWFhYSpNRjAYDAaDwWD8CJjSlsFgMBgMxg/B0tISP//8M0JDQyESidC4cWNcuHABGzduREhICKfYcnd3R35+PlatWoVGjRrB2NgYbm5uaNmyJTZv3oyqVavCysoK69atU6rc/Ra0TaNx48bQ1dXFlStXMGHCBABFOyurV6+OK1euYNq0aVqlV65cOXTs2BHz5s0DEWH27Nmwt7fHwIEDAQB+fn4AgMGDB2PkyJGIj49HaGgot+tWE66urtDV1cW2bdugq6sLfX19pcfOTUxMEBQUhICAABQUFKBz584oLCzEpUuX0Lt3b6VhXF1dMXbsWPTv3x9Tp06Fl5cX8vLykJSUhEuXLuHYsWN4//49Wrdujf79+8PNzQ15eXlYvXo1TE1NVZo1AIDAwEB06tQJgwcPRq9evXDnzh3s2bNHIc9z587FtGnTkJGRgWbNmkFHRwcpKSk4fvw4Dh8+DAMDA7i7uwMA1q1bh86dO8PAwAA1atTAL7/8gt27d8PHxwcTJkxAhQoV8OrVK9y+fRt2dnaYNGmSVvXyLfGbm5tj1KhRWLx4MSQSCerWrYs9e/YgPT1doU6aN2+O9PR0tXaBHzx4gE6dOqFixYoYMmQIzzaysbExqlatyvO/b98+FBYWok+fPkrjCw8Px5UrV1C7dm18+fIFJ06cwO+//46zZ89CV1eX85eens7Z6c3OzkZycjKnZJe3RSxP8TyqwsrKijMpoQpZevHx8SgoKOD+9/T0hKOjI4AiW8FRUVHYvHkzMjIykJGRwYWvU6cOtwM6Pj4e8fHx3LP79+/j0KFDkEqlaNu2rcb8MhgMBoPBYJSKH30TGoPBYDAYjP8dIHfzekFBAc2fP58cHR1JX1+fXFxcaPny5bwweXl5NGbMGCpXrhwJBALy8fEhIqLnz59T586dycjIiMqVK0cBAQG0efNmAkCvXr1SmaYyVN1wr20aREQeHh4kEAh47uPGjSMAdOrUKY114+PjQ+3ataNt27aRk5MTiUQi8vb2pvj4eJ6/nTt3krOzM4nFYmrQoAGFh4cr5H/gwIFUrVo1pels2LCBnJ2dSU9Pj2RTwdTUVAJABw8e5Pndtm0b1ahRg4RCIVlYWFD79u0pPT2diIguXbpEACgiIoLzX1hYSGvWrKHq1auTUCgkMzMzatCgAfdOc3JyaNiwYeTm5kYSiYTMzc2pVatWFB4errF+NmzYQOXLlyexWEw+Pj4UFhZGAGj79u08f3v37iVPT0+SSCRkbGxMderUodmzZ1NeXh7nJzg4mBwcHEhHR4ccHR0598zMTBo6dCjZ2tqSUCgkBwcH6tatG924cUPrevnW+L9+/Urjx48nU1NTMjY2poEDB9L27dsJAKWmpnL+fHx8eHErQxZO2U/Wjorj4eFB9evXVxlfVFQUeXl5kaGhIRkaGlLz5s0pLCysROlqQlW44r+BAweWOp7i8qIujeJ1HRQUpNSPpvpnMBgMBoPB+BYERERlrglmMBgMBoPBYJQIX19fGBoa4tSpUz86KwwGg8FgMBgMBuMHo/OjM8BgMBgMBoPBYDAYDAaDwWAwGIz/wpS2DAaDwWAwGAwGg8FgMBgMBoPxD4KZR2AwGAwGg8FgMBgMBoPBYDAYjH8QbKctg8FgMBgMBoPBYDAYDAaDwWD8g2BKWwaDoZTg4GAYGhqWebyXL1/GwoULyzzefwolLd/u3btRv359mJiYwNjYGO7u7hg2bBhevnxZonSjo6MRHByM7OxsrdwvX74MgUCAO3fulCidH8WZM2fQpk0bWFhYQCgUwtHREWPGjEFycnKZpvOj6+Xu3bto1aoVbGxsIBKJUKFCBQwdOhTPnj3TGFaWd00MGjQIAoFA4demTRvOj5OTE+eup6cHFxcXjBkzBq9fv+b85OfnY82aNahVqxYMDQ1hZmaGWrVqYdy4cfj69Wupyv/27VuMGTMGtra2EIvFcHV1xcaNG3l+8vLyMGPGDNja2sLAwADNmjXDvXv3tIpfIBDg8uXLKp8XL7eqX3BwsNo0VLU5bXFycsK4cePU+tmxYwf27NlTqvj/TXz+/BnTp0+Hi4sLDAwMULlyZQQHB/Pk6+PHj+jWrRsqVqwIiUQCKysrtG3bFhEREby4VMm9QCDA4sWLlaafkZEBQ0NDCAQCnuyrIjg4GL6+viqfv3z5Enp6epg/f75KP15eXmjUqBGAoovp2rdvrzFdQ0NDjXL5d6KNDH8vMjMzMW3aNNSuXRtGRkaws7ND9+7d8ejRI56/iIgIDB06FJUrV4aBgQEqVaqEyZMn48OHDwpxJiUloU2bNpBKpbC2tsaECRPw5csXjXnx9fVVKm8PHjzg+Xv69Cl69uwJExMTGBkZoWPHjkhNTf22iighZ86cQZ06dSAWi1GpUiWsX79eq3Dz5s1Dy5YtYWJionLslI1N8r9evXppjF9Wh8r85ubmwtzcHAKBAKGhoZx7cHAwLx2JRIJq1aph5cqV0OaA6aBBgzBo0CCVz+XjV/ZzcnLSmM4/haSkJIwfPx5Vq1aFVCqFo6Mjhg4diufPn6sMU1hYiLp160IgEODQoUNapaNJxrRtu9+b8PBwNG7cGBKJBA4ODggJCUFhYaFWYUNDQ+Hk5ASxWAxPT0+F+UZ6ejo6dOgABwcHiMViroxJSUnfoSQMBuP/Cno/OgMMBuN/i8uXLyM0NBQzZ8780Vn5LpSkfIsXL8bMmTMxadIkzJ07F0SE2NhY7N69G8+ePYO1tbXW6UZHRyMkJATjxo2DgYGBRve6devi5s2bcHd3L1kBfwCzZs3CggUL0KVLF2zcuBHW1tZIS0vDzp070aJFi799cfs9effuHdzd3TF8+HBYW1sjOTkZc+fORUREBCIiIiASicokHWdnZ+zevZvnZmpqyvu/W7dumDx5MvLy8nDr1i0EBwcjJiYG165dg46ODsaNG4edO3dixowZaNSoEbKzsxEdHY3ff/8d8+fPL3FeP336BB8fH0gkEqxatQrW1tZ4+PAh8vLyeP4mTZqE3377DcuWLYOTkxOWLFmC5s2b4/79+7CxsSlVfcg4evQoTyHYpUsXNGnSBJMnT+bcHBwc1Mahqs2VJTt27IChoSH69OnzXeL/pzB69GgcO3YMCxYsQPXq1REeHo7Zs2fj7du3WL16NYAixY1EIkFwcDAqVKiAd+/eYeXKlfDz88Pdu3fh6uoKAJg9ezZGjRrFi3///v1YuXIl2rZtqzT9yZMnw9DQEJ8/fy6T8lhbW6NFixbYu3cvZs2apfA8OTkZ4eHhWLt2LQBg/fr10NXVLZO0/06OHj0KMzOzH5L23bt3cfjwYQwZMgQNGzZEVlYWFi5ciPr16+PevXtc+92/fz8SExMxZcoUuLm5ISkpCbNnz8aNGzcQFhYGHZ2ifS3v3r2Dn58fHB0dcfjwYbx8+RK//PIL3rx5g127dmnMT+PGjXkKRQA8hV5BQQHatm2Lz58/Y+PGjRCLxQgJCYGfnx/u37//XT6ey3Pz5k106tQJAwYMwPLly3Hjxg2MHz8eQqEQw4YNUxt248aNcHFxQcuWLXH48GG1frdv344qVapw/1taWmqVP0NDQ5w8eRKfPn3i1ceZM2cUxgcZEokEFy9eBABkZ2fj/PnzmDRpEvT09L75g8KwYcN4Hzm3bNmCPXv2cOkBKLOx+u/g/PnzuHLlCkaMGIHatWsjIyMDwcHBaNiwoUoZ3Lhxo1Yfk2VoI2Patt3vSUpKClq0aAFfX1+cOnUKCQkJmDZtGvLy8tR+bAPAzf0XLlyIunXrYvPmzWjbti3Cw8NRo0YNAEXzHFtbW/Tu3Rv29vZ49uwZFi1ahGbNmiEmJkbrNsFgMP7HIAaDwVBCUFAQSaXSHxrvly9fyjz9701JymdnZ0eDBw9W+qygoKBE6W7fvp0A0KtXr7Ry/7dw9uxZAkAzZsxQ+vzEiRNlmt6lS5cIAEVERJRpvN/C+fPnCQDduHFDrT9Z3jUxcOBAqlatmlo/jo6ONHbsWJ7b3Llzubr5/Pkz6evrU0hIiNLwhYWFGvMhz4wZM8jFxYWys7NV+snIyCBdXV1at24d5/bhwweysLCggIAAjWkAoEuXLmmdJ2X1oIlvbXPapOnj40Pt2rXTGFd+fj7l5uaWKh8/mry8PBKLxTRnzhye++jRo8na2lpt2I8fP5JQKKQFCxao9efj40NVq1ZV+uzChQtkbm5OoaGhWr/PoKAg8vHxUetn586dBICio6MVns2dO5f09PTo5cuXGtMqjlQqpaCgoBKF+b9KVlYW5eXl8dxevnxJQqGQgoODeW7yHD9+nADQ5cuXObfFixeTgYEB7/3v3r2bAFB8fLzavGjTTvfu3UsA6N69e5xbRkYGiUQiWr58udqw8mg7BsjTpk0bql+/Ps9t+PDhZGtrq3EuInuubuz8lnHVx8eHWrduTZaWlvT777/znnXv3p369etHAGjp0qWcu6p5WLNmzahZs2Ya0xw4cCANHDhQ6zx+r/ny38WrV68UxuyYmBgCQDt27FDq39zcnLZu3UoA6ODBgxrT0EbGtG272pCamkoAKDU1tUThRo0aReXLl6ecnBzObcGCBSQWiykrK0tluJycHDIxMaGpU6dybvn5+eTu7k49e/ZUm2ZSUhIBoN27d5corwwG438HZh6BwWBoRVpaGgQCAXbt2oVx48bBzMwMtra2mDJlCvLz8zl/GRkZ6NGjB8qVKwexWIyKFSti0qRJAIqOlIWEhODz58/cETLZUVKZOYbw8HA0bNgQYrEYa9asUXlcvX379rxjqLLwd+/ehZeXFyQSCerUqYO7d+8iJycHo0ePhrm5ORwcHLBy5UqF8t28eRN+fn6QSqUwMTFBnz59eCYKtCm/uvIp4927d7C1tVX6TLbLR8aOHTtQs2ZNiMVi2NvbIzAwkEt3x44dGDx4MADAysqKO5qnyh1QbgZAIBBgyZIlCAoKQrly5WBpaYnBgwcr7DK7fv06d8StevXqOHfuHKpXr847ThgXFwd/f39YWFjAwMAAbm5uWLJkicq6UEVoaCjKlSuHkJAQpc87dOgAAPjll19QoUIFhSNs58+fh0Ag4B2f/+2337j8W1pawt/fH+np6SrzQEQIDQ2Fq6srRCIRnJ2dsWLFCp4fdXL/rVhYWACAyh1Ffxf16tUDAKSmpuLz58/Iy8tTKb/amGmQZ9u2bRg6dCgkEolKP+fPn0dBQQHvqKyRkRE6dOiA06dPlzjN0rBp0ya4u7tz5itmzZqlsS0CRUc/hwwZAmdnZ0gkElSuXBkzZ84ssSkJX19fXLlyBadPn1Yw2SA7Tr9z5064ublBJBIhOjpa67QLCwuxfPlyrnw2Njbo3r073r9/z/lJSEhAp06dYGJiAqlUinbt2imYKdm2bRuqVasGiUQCCwsLNGnSRMFcgSaICPn5+TAxMeG5m5qaajziLJVKIRaL1baZp0+f4tq1a+jbt6/Cs7y8PIwbNw4hISFc+ysrunTpAolEgr179yo827t3L1q2bAkrKysAys0jHD9+HFWqVIFYLEb9+vVV1uvp06e5sdDKygqjR49W6MsfP36M7t27w9TUFAYGBvDz89PKNIym91vcPIJs7FT227FjBxdG0xisLaamptDT4x8ktLKygoODA29noKyOi1OnTh0A4Pk7c+YMWrRowdsB17VrV4hEIpw5c6bE+ZMnKioKtra23E48ALC3t0f16tVx8uTJb45fE1+/fsXFixcVzA/07dsXmZmZiIqKUhtefq7yPdDT00P37t15bebjx484depUiU4bGBkZ/W3jaEZGBvr16wdLS0tIJBJ4e3vj7t27PD+ydrJs2TI4ODjA0NAQAwYMQE5ODqKjo9G4cWNIpVJ4enri/v37vLAysy7Tpk2DlZUVjIyMMGjQIHz8+LHEebW0tFQYs2vUqAFdXV2lu2lnzJiBZs2awc/PT6v4tZUxbdvu9+TMmTPo0qULb6d03759kZOTgwsXLqgMFxYWhvfv36N3796cm66uLnr27IkzZ86oHbP+KXM8BoPxz4UpbRkMRokIDAyEjo4ODhw4gJEjR2LZsmXYsmUL93zAgAG4d+8eVq9ejXPnziEkJAQFBQUAio6UyZQyN2/exM2bN3k2rXJzc9G3b1/0798f586dQ6tWrUqUt7y8PAwZMgSjR4/G4cOHkZ+fj59++olLc//+/ejcuTMmTZqEsLAwLtzNmzfh6+sLExMT7N+/H5s2bUJERAQ6duxYovJrKp889erVw4YNG7Blyxa1tsOWL1+OYcOGoXXr1jh58iQCAgKwevVq7nhtu3btuL/PnTuHmzdv4ujRoyrd1bF27Vo8evQIO3fuxOzZs7Fnzx7MmzePe56ZmYk2bdrAyMgIBw4cQEBAAMaOHauQ/44dOyIrKwtbt27F6dOnMWXKFJ7CQKY0Lr5olyc/Px83btxAixYtoK+vrzbfw4cPx5MnT/Dnn3/y3Ldt2wYPDw/UrFkTALB06VIMHDgQ9erVw5EjR7B161ZUrlwZr169Uhn3hAkTMGfOHAwcOBCnT5/GoEGDEBAQgA0bNnB+1Mk98F/FhbZ2JwsKCpCbm4sHDx5g2rRpqFu3Lho3bqxVWG3Jz8/n/TQhM0NhZ2cHKysrVKhQAfPnz8e+ffuQlZWlMpzMlqimuF+8eAEzMzO0b98eIpEIFhYWGDt2LM92ZEJCAsqVKwdzc3Ne+KpVqyIxMVFru3OlZc2aNRg5ciT8/Pxw4sQJjBo1CkuWLMHIkSMBqG6LAPD69WuYm5tj+fLlOHfuHKZNm4adO3di9OjRJcrD+vXrUadOHTRu3JjrZ4ofYb5z5w6WLVuGefPm4cyZMyhfvrzWaY8fPx7Tpk1D+/btcfLkSaxbtw5GRkb49OkTgKKjo40aNcLbt285u7qvXr1C8+bNOQXw1atXMXToUPj7++PMmTP47bff0Lx5c7x7945LRxuZ0NfXx+DBg7FmzRrcvn0bnz59wqVLl7B582alx5sLCwuRn5+PzMxMTJ48GTo6Oujfv7/K+Pfu3YvCwkLeIlvGqlWroKurW+J3ow0ym6X79u3jLeSjoqKQkJCgVIksIzo6Gl27dkXlypVx5MgRDBgwAD169FBY7B86dAgdO3ZEjRo1cPToUSxZsgRHjhzB0KFDOT8fP36Ej48PIiIisG7dOuzduxdfv36Fr6+vgs3V4mjzfotja2vLyansN2rUKOjo6KBSpUoAtB+DfX19S2Ur9MmTJ0hPT9doDujatWsAwPOXkJCgEE4kEsHFxQUJCQka075y5Qr3EcHHxwdXr17lPc/JyYFQKFQIJxKJNMYv+7Ah+8nGHPm+XZ3CKDk5Gbm5uQplrFq1KgBoVUZt8ff3h66uLhwcHDB16lSt7ALL6NOnD86fP8/Zlj569CgMDQ3RsmVLlWFk5f/w4QMOHTqEc+fOoVu3bt9cDk1kZWWhSZMmiI6Oxpo1a3D48GFIpVL4+fkpfIg4fvw4Ll68iE2bNmHx4sXYt28fxo8fj/79+2PYsGE4ePAgcnJy0L17d4Xxbc2aNUhISMDOnTuxePFiHD58GMOHD+f5cXJyUrt5QBU3b95EQUGBglxERERgz549CiY/1PEtMqZt21XVFgoKCrRuC58/f8bjx48V0nJ0dISBgYHafMqeFTf/ISvjx48f8fTpU557YWEh8vLykJaWhnHjxqF8+fLo3Lmz2jIyGIz/YX7YHl8Gg/GPRv64l+yoUffu3Xn+GjduTM2bN+f+l0qltHr1aq3jLe4OgA4cOMBzV3Wsrl27drxjqLLwZ8+e5dxOnjxJAHhHk/Lz88na2pomTpzIuXl7e1OjRo14x8NiY2NJIBDQ6dOnS1T+khyTu3//PlWqVIkAEACqWLEi/fzzz7zjXB8+fCBDQ0MF8wDr1q0jiURCr1+/JqKSm0dQVq8AyNPTk+evb9++5OLiwv0/depUMjExoQ8fPijEJTtO+OrVKwKg1nSBLMz27dtV+nn+/DkBoOnTp6v0U5wmTZpQjx49uP/fvn1LIpGIfv31VyIievfuHRkYGNCIESM05ktWL48ePSKBQEAbN27k+Zs6dSrZ2Nhwx/o0yb1MfrQ9wty4cWNOLjw8POj58+caw5TEPIIs7uK/a9eucX4cHR1pzJgxlJeXR9nZ2XTp0iWys7MjZ2dnzmzJpUuXqFy5cgSABAIBubu70/Tp0xVkTZaeOsLCwggAGRoa0uDBg+mvv/6i1atXk1QqpWHDhnH+hg0bRm5ubgrhN2/eTADo/fv3atPBN5hHyM/PJ0tLS4U+YOHChSQQCCg5OZmItDePkJeXR7t37yY9PT36/Pmz0jRVoerYtY+PDwmFQnry5EmJ005MTCSBQEALFy5UGW7AgAFUsWJFnumaly9fklQq5UxWLF26lMzNzdWmr41MEBXV+bBhw3hyOn78eKV+AwMDOT/W1tYUFhamNu46depQw4YNFdyfPn1KRkZG9NdffxFRycxdaGMegei/x/CvX7/OuU2dOpUMDAzo06dPnJv8e+7ZsydVrFiR8vPzObeNGzfy+pbCwkJydHSk3r1789I8ffo0CQQCio2NJSKiVatW8f4nKjIrYW5urvZouDbvV50M37hxg4RCIc2fP59z02YMJiqqD0dHR7VpK6Nz585kaWlJb9++VeknKyuLnJyceOM5EZGenh4tWrRIwX/jxo2pS5cuatOdM2cObd26la5evUr79u0jDw8P0tfX58nm2rVrSVdXl54+fcq5ffz4kUxMTEgoFKqNXyabmn7q+rzr168TALp58ybPPS8vjwDQqlWr1OZBhjoTCJGRkTRt2jQ6deoUXbhwgQIDA0kkEmll4kXWBmRyvX79eiIiatWqFY0ZM4aISKl5BGX1MGjQIK1M93yreYQ5c+aQiYkJvXjxgnPLyckhBwcH3vF5R0dHKl++PH39+pVz69q1q8q5bHGTKrI5Y/G+YMuWLSQQCCghIYGXhjZ9UnFyc3PJ09OT3NzceOYKCgoKyNPTk2bPnk1E/53XaDKP8C0ypk3bJfqv/Gn6qZtzZmRkEADau3evwjN7e3uaNGmSyrDz588nkUik4P7nn38SAIqJieG59+3bl8uTi4sLJSYmqi0fg8H434bttGUwGCVCfvdr1apVkZGRwf1ft25dhIaG4tdffy3Vja/+/v6lzpuOjg7vuJbsApoWLVpwbrq6unBxccGTJ08AFF1QcePGDXTv3p33Rd7NzQ22trYKR081lb8kVK9eHXFxcTh9+jQmTJgAExMTrF69GjVr1kR0dDSAoiNXnz59Qvfu3Xm7Bfz8/PDlyxfExsaWKm1VaCpfREQEmjVrBiMjI85NtkNKhoWFBRwdHTFjxgzs3LlTaf34+vqCiNTe0Ez/f0eEtkfthw8fjuPHj+Pt27cAgN27d0NHR4fbSXfz5k1kZ2fzdptp4q+//gJQdBy2eP03b94cz58/5+RIk9w7OTmBiLTeabt161bcunULu3btwpcvX9CyZUult5qXFhcXF+5yM9mvdu3aPD/r16+Hvr4+DAwM0KxZM9jb2+Pw4cMQi8UAit5hcnIyDh48iJEjR6KgoACLFy9G9erVeUcZd+zYofE4u2wHkbu7O7Zt24bmzZtj/PjxmDt3LrZt28bbya1MHkoqK6XhwYMHeP36NXr27Mlz7927N4gIN27cUBueiLBy5UpUrVoVEokE+vr66Nu3L/Lz85GSklJm+axZs6bChS3apH3x4kUQkdr2cf78eXTq1Al6enpcWzAzM0OtWrW4vrJu3bp4+/YtBg0ahD///BPZ2dkK8WgjEwAwffp0nDp1Cps2bcKVK1ewfPly7NixA0FBQQp+x4wZg4iICJw4cQL169eHv78/IiMjlcb74MEDREVFKT1aPWXKFLRs2RLNmzfXmL/S0rZtW5ibm3PHvYkI+/btQ6dOnSCVSlWGu337Njp06MC7nEx+52BSUhLS09PRo0cPXp/l4+PDM4lz7do1VKtWDdWqVePCGhoaokOHDtyOU2Vo835VkZGRgZ9++gkdOnTgLussyRh8+fJlpKWlaZ0eACxatAgnTpzAtm3bVF6OVlBQgL59++Lz58/YunWrwnNVfY6m/iYkJARDhgxB06ZN0bNnT1y+fBl2dna80yt9+vSBiYkJBg0ahOTkZDx9+hTDhw/Hp0+fNJoe6NChA68Pl53+kO/bZaZt1KGqLGXRp9apUwf/+c9/0K5dO/j5+WH+/PlYtmwZTp8+jfDwcK3iEAgE6N27N/bs2YOXL1/iwoULanelSyQSrvzXr1/HqlWrcPToUYXLCL8H58+fR7NmzWBubs7Js66uLpo2baowp/T29ubttHZ1dVU5l5XNN2TI9wU//fQTiIhXp2lpabh8+XKJ8j9u3DjExsZi165dPHMFW7ZsQWZmJqZPn16i+GSUVMa0absy6tWrx5P5EydOAABOnDjBc5eZ1CppPrVp7yWZm8ybNw/h4eE4dOgQbGxs0KJFCzx+/Fhj3hgMxv8mTGnLYDBKhPwN80KhEDk5Odz/+/fvR/PmzREYGIjKlSujSpUqOHLkiFZxGxgYqF2wakIikfAmv7K/1eU5KysLBQUFmDRpEvT19Xm/Z8+eKUySNZW/pAiFQvj7+2PlypWIiorCuXPnkJ2djblz5wIAdxSwbt26vLzJjm/J5+9bUVa+4nYvMzMzldoCLO4mEAjwxx9/wN3dHWPHjkX58uVRr149hWOhmrC0tIRYLNZ6Itu9e3dIJBLuRu+tW7eiW7dunEL5zZs3AIqO92vL69evQUSwtLTk1b/s5mhZ/X+L3CvDzc0NXl5e6Nu3L/766y88ePAAmzZtKnV88ojFYnh4ePB+8jdE9+jRAxEREYiOjsabN28QHh6uoNiVSqXo1q0bfv31VyQmJmLz5s148eJFiY5OAuDMHcjbyPPz80NhYSF39NDMzEypKYZ3795BX1//m/oPTcjStbGx4bnL/pd9LFDFypUrMXnyZHTq1AnHjx9HeHg41q1bBwDf1IfIY21tXaq037x5Az09PaXhZbx+/RorV65U6CvDwsK4tuDn54fff/8dcXFxaN26NSwtLTFgwACN9SNPbGwsQkNDsXHjRgwfPhze3t6YNGkS5s2bh4ULFyocM7azs4OHhwc6dOiA48ePw9HREXPmzFEa9+7du6Gnp4cePXrw3G/evIlDhw5h1qxZePfuHd69e8cpJT98+FAiBaU69PX10b17dxw4cAD5+fm4du0anjx5olYJBRT1v/Lvx9zcnKdYkY0ZXbp04b0jQ0NDFBYWcu8pKytLQZaBInlW965K+36/fPmCzp07w8rKCjt27OCUGCUdg0vCzp07ERgYiDVr1qhV1owYMYKzE+3o6Mh7pq7P0aRIkkdmA7q4bVMzMzPs27cP8fHxqFSpEme/c+DAgUrfT3EsLCx4fbibmxsAKPTtxT+yyiMrg3wZZf+XtIzaImt78nZe1dGnTx/cuHEDS5cuRfny5dGwYUOVfnV0dLjyN27cGD///DNmz56NTZs2IS4u7pvzr47Xr1/j2LFjCvK8d+9ereaUquay8uOEfF9gZmYGfX19ZGZmljrvISEh2Lp1K/bt2wcPDw/O/dOnT5g5cyZmzZqF3NxcvHv3jvuQnJ2drfajcmlkTNu2K8PIyIgn8zIb0TVq1OC5q7NRriqfgOb2bmZmhpycHIV3JDMbIx+2YsWK8PT0RNeuXfHHH38gNze3VPc+MBiM/w30NHthMBgM7bG1tcW2bduwZcsW3L17F/Pnz0fPnj2RmJgIZ2dntWGVfaWW7erLzc3lub99+1apHbiSYmpqCoFAgJkzZyq1J1X88pG/g9atW6NWrVqckkqmzDpy5AjKly+v4L9ixYp/a/5sbW2V2n+Vd3Nzc8PBgweRl5eHsLAwzJw5Ex06dMDTp08VlIOq0NPTQ5MmTfDXX38hLy9Po11biUSCvn37Ytu2bfD29kZ0dDTv0jnZZP3Zs2cKOxFVYW5uDoFAgOvXryuVN9ki+VvkXhM2Njaws7Mr1c71b8HKyoq3aNOGYcOGISAgoMR2EF1cXJTWr2yXimzHmbu7O16+fIm3b9/y7NrGx8fDzc3tu16KI0vvxYsXPHfZLmB5O7vyHDx4EB07dsSiRYs4t/j4+DLOpfJ+VJu0LSwskJ+fj5cvX6pU3Jqbm6Ndu3YYM2aMwrPiiqF+/fqhX79+eP36NY4fP84p5JTtYlSFLH/yHwpq166N/Px8pKenq8ynjo4OateujVu3bil9vnfvXrRo0UIhfGJiIvLy8lC3bl2FMC4uLujZsyf27dundRnU0adPH2zcuBEXLlzA0aNHYWlpqdGOu62trYKy+u3btzyb1DI5XLt2Lby8vBTikH20Mjc3V2q79vnz5xpluTTvd+jQoUhJSUFERARvDPheY/CJEycwbNgwzJgxQ6m8ypg2bRp+++03HD9+HJ6engrP3d3dFfqzr1+/Ijk5GUOGDClxvpTtMG/ZsiXS09ORlJTEXWTZrl07NGjQoMTxlxRZ35uQkMB9jAT+2/402RL9O6lRowaqVauG5cuXIyAgoMS7gGU2VGNjY3k7zMsac3NztGnThrejWkbxC66+Ffm+ICsrS+0FoZpYv349goODsXHjRgV70q9fv8abN28watQohd3KAwcORLly5VTezVBSGdO27ZY1BgYGqFChgkJ7T09PR3Z2ttq2IHuWkJDAXWgIFJXRyMgI9vb2KsNKpVJUqVLlb5/jMRiMfw9spy2Dwfgu6OjowNPTE/Pnz0d+fj43GZHfuakJmXKt+CTq5cuXuHfvXpnkUyqVomHDhkhISFDYneLh4VHiS09KUj555Q9QtBvpyZMn3A6bRo0awcDAABkZGUrzJ1NEqtqJocq9tHh6euLixYu8G4ovXbrEu12+OPr6+vDx8cH06dPx4cOHEt8APHnyZLx48YLbeSzPqVOneP8PHz4cMTExmDBhAipVqgRvb2/uWcOGDWFgYIDt27drnb7siPSbN2+U1r/8DiZVcv8tpKenIyMj45uVv2VJXl6e0t0oL1++xPv37zXuEJNHKBSiZcuWCrczX7hwAXp6etxiu1WrVtxFgDI+ffqEkydPol27dqUoifa4ubnBysqKlzZQtMtaIBCgSZMmXFkAxTb35csXBcX07t27S5WXku7w1yZtPz8/CAQCte2jRYsWiI2NRZ06dRTaguwDRnEsLS0xdOhQtGzZssSKfNmOR/mdeLLj/er65ry8PISHhyttM7dv30ZycrJS0wht2rTBpUuXeL+AgAAAwLFjx1Tu3C0NTZs2RYUKFbBz504cOnQI3bt31/hhqn79+jh58iTvksNDhw7x/FSpUgUODg5ISUlR2mfJlLZNmjRBbGwsT3n/+fNnnDp1Ck2bNtWqDNq+38WLF+PAgQPYt28fXFxceM/KegwGii7/6tmzJwYMGIAFCxao9LdkyRKEhoZiy5YtKk0z+fv748KFC9xJDaDoEqyvX7+W2JzT58+fcfr0aaXKYV1dXbi7u6NixYp48OAB/vrrL4VLpTQhMztUEkQiEfz8/BT6tb1798LW1pangCpLZB8/lNWFOgICAtChQwcMGDCgxGnKzEl974/xLVq0QHx8PNzd3RXkWbYDtCyQ7wuOHDkCgUBQ4joFwF2ANnfuXIwYMULhuY2NjULfKDPvEhwcrPZkUUlkTNu2qwmZSaqS9h/+/v44duwYb6PI3r17IRaL1ZrMadSoEXeRooyCggIcOHAA/v7+aj8wvHv3DrGxsf+oOR6DwfhnwXbaMhiMMuP9+/do3bo1+vfvDzc3N+Tl5WH16tUwNTXldi65u7sjPz8fq1atQqNGjWBsbKx0sS/DwcEBXl5eCAkJgYmJCXR1dbF48WKeDdVvZenSpfDz80PPnj3Rq1cvmJmZISMjA3/++ScGDx5copt3S1K+GjVqoEOHDmjdujVsbW3x7NkzrFmzBq9fv8aECRMAACYmJpg7dy6mTZuGjIwMNGvWDDo6OkhJScHx48dx+PBhGBgYcF/5161bh86dO8PAwAA1atRQ6V5aJk2ahPXr16Ndu3aYOnUq3r17h+DgYFhYWHC7HO/du4fJkyejZ8+ecHFxwfv377Fo0SI4OTlxC/YrV66gefPm2LZtm9rFV5s2bRAYGIj58+cjISEBvXv3hrW1NdLT0/H7778jKSkJ7du35/zXqlULnp6euHr1KhYuXMibKJuYmCAoKAgBAQEoKChA586dUVhYiEuXLqF3795Kd5W6urpi7Nix6N+/P6ZOnQovLy/k5eUhKSkJly5dwrFjx7SS+/T0dLi4uGDOnDlqFT+jRo2CpaUlPDw8YGJigsTERISGhsLGxqZEtni/N+/fv0flypUxYMAAznZfamoqQkNDoauri9GjR3N+hw4dip07d/J2Aypjzpw5aNKkCQYMGIB+/fohPj4eQUFBGDduHGd+w97eHqNGjUJAQAD09PTg6OjImWKYOHHidysvUKRUmTNnDsaPHw8rKyt06NABkZGRCAoKwuDBg7ld76raXMuWLbFq1SqsXbsWrq6u2L17d6mV+u7u7ti5cydOnjwJW1tb2NnZqTX7oU3arq6uGDVqFGbNmoW3b9+iefPmyM7OxunTpxEcHAx7e3uEhITA09MTrVu3xogRI7jdVVeuXEHTpk3Ru3dvBAUF4c2bN/D19YW1tTXu37+Pc+fO4ZdffuHS0kYmPDw8UL9+fYwaNQovXryAm5sbIiIiMHfuXPTs2ZOTiU2bNiE8PBwtWrSAra0tMjMzsXHjRjx69AgbN25UiHfPnj2QSCTo0qWLwjMbGxuFDw4yG6qNGzcuU2WPzEbnkiVLQEQaTSMARTZ+PT090blzZ4wZMwYpKSkIDQ3lKeQFAgGWL1+OPn364PPnz2jXrh2kUinS09Nx+vRpLFy4EK6urhg8eDBWrFiB9u3bY/78+TA0NMSSJUvw5csXtTYrtXm/xblx4wYCAwPRq1cvGBsb83Y/u7i4wMrKSusxuHnz5khPT1fbbh48eIBOnTqhYsWKGDJkCC89Y2Nj7gPQnj17EBAQgF69esHNzY3nz8HBgftgPHLkSKxZswadOnXC7Nmz8fLlS/zyyy/o27cvb+edvExfu3YNoaGh6NKlCxwdHfHs2TMsW7YMz58/x8GDB3l5DggIQIMGDWBiYoKYmBjMnz8fAwYMUDAXI8+rV6+QnJys1g9QtMPU2NhY5fM5c+bA29sbw4cPR9++fXHjxg1s3rwZGzdu5J1eqFSpEhwdHXkf165cuYJXr15xJgcuXryItLQ0ODk5cWNqv379UKlSJdStWxdisRgXL17EihUr0KlTpxKf5pDt8tZEYWEh905zc3O5EzBVq1blfcz9Hvzyyy/YvXs3fHx8MGHCBFSoUAGvXr3C7du3YWdnh0mTJpVJOl+/fuX6gtTUVAQEBKBbt248uVT2zuS5cuUKBgwYgKZNm6Jly5a8tmBlZQUXFxeIxWKFubCsb6xWrRoaNWrEuc+dOxdz585FcnIy9/FNGxnTtu0q48OHD1qdXJH1OaqYOnUqdu/ejR49emD8+PFITEzEvHnzMHnyZJ4pC/m+SCQSYdasWZg5cyasrKxQt25dbNmyBSkpKbzTGcHBwXj//j0aN24MKysrpKWlYeXKlcjLy/vucxgGg/Ev5m+99ozBYPxrkL8NV9UtsWPHjuVuc87JyeFud5dIJGRubk6tWrWi8PBwzn9eXh6NGTOGypUrRwKBgLvVVj694jx69IiaNWtGUqmUKlWqRPv27aN27drxbsRVFl5VnpXdvB4REUH+/v5kYmJCEomEKleuTKNGjeJuYdem/OrKp4x169ZRmzZtyN7enoRCIdnZ2VGbNm3o4sWLCn737t1Lnp6eJJFIyNjYmOrUqUOzZ8/m3ewbHBxMDg4OpKOjw8uTMndlNz1D7gZmoqKbwuWHiqtXr1Lt2rVJKBSSu7s7nT59msqXL08TJ04kIqIXL15Qv379yNnZmUQiEVlbW1PXrl0pKSmJi0OWvrqbfItz6tQpatWqFZmZmZGenh5VqFCBhg4dSnFxcQp+Fy5cqHAbd3G2bdtGNWrUIKFQSBYWFtS+fXtKT09XWS+FhYW0Zs0aql69OgmFQjIzM6MGDRrQ8uXLiUg7uZfJj+yGd1Vs3bqVvLy8yNTUlCQSCbm5udHPP/9Mz58/11hHsrxrYuDAgVStWjW1ftTd/k5E9PXrV1q8eDH5+vqSjY0NiUQiqlChAnXt2pXu3r2rkJ62043z589TvXr1SCgUkq2tLQUEBFBubq5C2gEBAVSuXDkSi8Xk4+PDu1VbHdBwk7o8yuphw4YN5ObmRvr6+uTg4ECBgYG8dkikvM19/PiRBg0aRGZmZmRmZkbDhw/nbgUvLm+a6p6o6JZrf39/MjU15cmVsr6tJGkXFBTQkiVLqHLlyqSvr082NjbUs2dPev/+PecnKSmJevToQRYWFiQSicjJyYkGDBhAsbGxRFR003nz5s3JysqKRCIRubi4UFBQEK+OtJWJFy9e0IgRI8jJyYnEYjFVrlyZpk+fTh8/fuT8XL9+nVq3bk3W1tYkFAo5OVQmE/n5+WRjY0M9evTQmLaM7du3EwB69eqVRr9BQUEluqn93r17BIAcHR2V3mqv7H0eOXKEXF1dSSQSUb169ejWrVsklUoV+pbz58+Tj48PSaVSkkqlVK1aNZo8eTK9e/eO85Oenk7dunUjY2Njkkgk5Ovry+u3lKHN+y0uw7L6U/Yr3v9rGoNl9VF8bFOGuvSKvxuZDCr7yddlYmIitWrVigwMDMjS0pLGjx9P2dnZPD/yMv3w4UNq3bo12djYkL6+PpmampK/vz/dvn1bIc99+vShcuXKkVAoJFdXVwoNDaX8/Hy15dRU1uI/bfq806dPU61atUgoFJKzszOtXbtWwY+jo6OCfPv4+ChNc+DAgZyfhQsXUrVq1cjQ0JD09fXJ1dWVgoOD6evXrxrzpapPK4783CUoKIiXFz09PapYsSKNGTOGXrx4oTHNgQMH8vKvCWXzz8zMTBo6dCjZ2tqSUCgkBwcH6tatG924cYPzo6yv13YuC4AWLVpEv/zyC5mbm5OhoSH179+f11fL0tDUJ8nXl6r3KI+qebEsvtTUVJ67JhnTtu0qQzb/0fTTZs5569YtatiwIYlEIrKzs6OgoCCF9qisLyosLKQlS5ZQhQoVSCQSkYeHh8J8/vjx4+Tj48ONn87OzjRo0CBKTk7WmC8Gg/G/i4CohOdoGAwGg8EoRlJSEqpUqYLt27dj4MCBPzo78Pb2homJCU6ePPmjs/K3cvnyZTRr1qzEx2P/lxAIBLh06VKJds8zGNoSHByMy5cvl/i2dgaD8c9h0KBBAIAdO3b80HyoQyAQYOnSpZgyZcqPzgqDwWAwvjPMPAKDwWAwSsSMGTNQs2ZN2NnZISUlBQsXLoSdnR26du36Q/N1584dXLt2DdeuXcOff/75Q/PCYDAYDAaDwWAwGAzGt8CUtgwGg8EoEbm5uZg+fTqeP38OiUQCX19fLF26lHcj+I/A09MTJiYmmD17Nlq0aPFD88JgMBgMBoPBYDAYDMa3wJS2DAaDwSgRy5Ytw7Jly350NhRgZgEYDAaDwWD8X4fNdxgMBuN/B2bTlsFgMBgMBoPBYDAYDAaDwWAw/kHo/OgMMBgMBoPBYDAYDAaDwWAwGAwG478wpS2DwWAwGAwGg8FgMBgMBoPBYPyDYEpbBoNRarp27Yrx48f/6Gz8bdy/fx8CgQBv3rz50VlRio2NDfbv3/+js8Hx4cMHdOvWDebm5hAIBIiLi/vRWUJmZiYEAgGSkpK08j958mR06NDhO+eKz+LFi9GwYcO/Nc3vzf+lMp08eRLGxsZlZlMwLy8PxsbGOH/+fJnEpwxNfUNJ24W29OzZE6NHj/7meC5fvozatWtDLBbD09OzDHL2fxt5GV20aBEaNWr0XdPcu3cv7OzsvmsajB/DzZs3IRQK8fHjxx+dFbWU9XhdVn3939H+/slkZ2dDV1cXt2/f/tFZUcvHjx+ho6ODu3fv/uisMP7FyK+N3dzcsGnTph+YI8b/BZjS9l+Kt7c3BAIBBAIBhEIh3N3dsWfPnh+drR/Cu3fvoKOjg+jo6B+dlX8d2tZddHQ0dHR0kJWVpeBeq1at75jDkkNEMDc3x7Fjx8o87ujoaDg4OMDCwqLM4/5Wnj9/jhcvXvyj3se8efOQnp6OmzdvIjMzE1WrVv1b0584cSI6d+7Mc7OwsMDz58/h6uqqVRzR0dGoXbt22WdODTExMSV6j6ra57eirP5KS0nLpIzatWtj5cqVZZKfbyE6Oho1a9aEQCAok/h0dHSQlJSEFi1alEl88mjTN5S0XWhLWYwRBQUF6N27N7p06YLk5GT8+eefZZS7/7vI6l0mo2PGjMG5c+fKLH5lbbFDhw7/iA9z/2usXLmyTMeozp07Y+LEiTy32rVr49mzZzAyMiqzdL4HZT1ey7cjTaiaf5Z1+1NGSkoKty6U//3oPjMmJgYAUKNGjR+aD03I5lLVqlX70VkpNUSE/fv3o3HjxrCwsIBUKkWVKlUwe/Zszs+2bdt48mFsbIz69etj9erVyM/P/4G5/7+BfD909epVDB48uMzi/6fMhRl/L0xp+y+EiBAdHY3Q0FBkZmYiMTERbdq0wYABA5Camvqjs/e3ExERAbFYjOrVq5dZnPn5+d/tZtbCwkIUFhZ+l7hLirZ1FxERgUqVKsHMzIxz+/jxI1JTU/9RSkIAePToEbKyslC/fv0yj/tHKPC0JSoqChKJpMyVLt8ygTt+/DiGDh0KNzc32NjYlJmSSxN5eXkAiuRWXg6EQiHKlSundVwxMTE/RGlbkjSVtc+yQFn9lZZvrccvX74gLi7uu7TrklLWH6t0dXVhY2MDHZ3vMyXTpm8oabvQhuzsbDx69Oib28/t27fx6tUrTJkyBfb29jA1NS1xHN9zTC8Nsj7qeyE/VpmYmMDY2LhM4lbVFg0NDcu8DyoLvndd/2i06adLIv/K4pNIJLC0tCx1Hv8uynq8LumcT9X8syzbnyoiIyOhq6uLtLQ0ZGZm8n5+fn5Kw6hqG6VtM6rCRUdHo3LlyjAwMChVvMogojJXMEZHR8PNzQ1isbhM4y0tly9fhoODQ4nCTJ06FWPHjsXQoUNx69YthIeHY8aMGZBIJJyfyMhI1KlTB5mZmXj69CkuX76M7t27Y/bs2ejfv39ZF0Mt/4SxuSzX5bK1cfF+o1y5ctDX1y+T+P9Jc2HG3wwx/nUkJiYSAIqNjeXc7t+/TwDo7NmznFt6ejr17NmTHBwcSCwWU5UqVejQoUPc8549e9LPP//M/T9t2jRevF++fCFzc3M6d+6cyrykpKRQjx49yMzMjAwMDKh+/foUHx9PRET5+fm0ePFicnFxIYlEQp6ennTz5k0u7NmzZ0ksFlNeXh7nFh8fTwDo1atXRES0efNmcnFxoQMHDlD16tVJIpGQr68v9zwoKIgA8H5nzpyhgoICWrBgAVWqVIlEIhFZW1tTv379VJYjKCiIvL29aeXKleTk5ERGRkZUWFhIeXl5tGzZMnJ2diaxWEx16tShq1evcuFu3rxJOjo6tGHDBnJzcyORSESenp5cHRARbd++nSpUqEC7d++mKlWqkI6ODr18+ZKIiHbs2EHu7u4kEomoSpUqdOTIES6cpjJ8/vyZZs6cSfb29mRgYEBNmzal+/fvc89LW3fyDBw4UMFffHw8Xb16lXR1dens2bPk5eVFYrGY6tatSw8fPuSFP3nyJNWrV49EIhE5OTnRr7/+qvI9EBHFxMRQhw4dqFy5clydX758mefn5cuXNHz4cLK2tiaxWEw1a9akK1eu0Pbt2xXyun79ekpISCAAlJGRwcXx6dMn0tHRoTt37nBuY8eOJVdXVxKLxWRnZ0dz5szhpevn50ezZs1SmffAwEBq3rw5rVu3jipVqkQGBgbUuXNnysnJ4fyUK1eOdu3axQtXv359Cg0NJSKi3Nxc0tfXpy1btlC7du1IIpGQm5sb3blzh2JiYsjX15cMDAyobt26lJKSwsWxcOFCql+/Pq1atYocHR3JxMSEBg8eTF++fOGlpU7mUlNTCQAdPHiQmjZtSkKhkA4cOKC0rLL+xczMjMzNzWnQoEH0/v17IiK6evWqwnv466+/FOLQpv28f/+eBg0aRE5OTpwMrV+/nhePo6MjLV68mPr06UOGhoY0ZswY0tfX56Vfv359IiqS50GDBnFhCwsLadOmTeTu7k5CoZDs7Oxo5syZRET0+PFjAkCPHj3i/MfFxVG7du1IKpWSlZUVjR07lvd+L126RN7e3mRiYkJGRkbk4eHB66vliY+PJx8fHxKLxVS7dm2KiIggXV1dCgsL0ypNVe1Tm7yqKrtMBpXVH5F6GdK2TPIcPHiQGjduTKampmRkZERt27alx48fc3UqX8Zp06apjEtTn7N+/XqqV68eGRkZkZmZGfXs2ZPevXvH83P58mVq0qQJSSQSMjMzo7Zt23JjlbOzMy1atIgGDx5MpqamZGVlRZs3b1aZH1kZVMlFUFAQ+fr6ElFRvy8WixXKa2xsTIWFhUREFBYWRt7e3lw/FRQUpDZtbfoG+XbRuHFjmjFjBk2cOJGsrKzI1NSUgoOD1aYjT1hYGOno6NCxY8eoTp06ZGBgQL6+vpSWlqbgT1V5hgwZwqsHBwcHIlLf/8jqtDRjujLs7e1pxYoVNHDgQJXv+/79+9S2bVsyMjKicuXK0S+//EJfv37l1W+fPn1o1qxZZGNjQ9WqVSOior7rP//5D/Xt25cMDQ2pfPnydPbsWUpLS6OOHTuSVColV1dXioiI4OLSpk90dnamLVu2ENF/xxTZOLpgwQIF+QJAy5YtI6LSt0VHR0fasWMHlwdN70ib8VIZR44cIQ8PD5JIJOTi4kLbtm3jPVc2HihDXd+vzTvt27cvDRo0iEJCQsjBwYEMDQ1p+PDhXDslIvr69SstWrSInJ2dSSgUkqOjI61Zs4Z7rqmP1tQOHR0dee/C2tqaiFTL/8WLF6lFixZkaWlJBgYG1KRJE7p37x4R/Xf8L/7r0aMHERH5+Pjw0n3z5g2NGDGCrK2tydjYmDp37kzPnj3jnmuaf6pC3ZpC09xQ2XitLj4vLy+aP38+L/0ePXrQuHHjuP+LtyMi9WOHqvmnfPsjKhvZkmfGjBlUtWpVtfXr4+NDv/zyC40dO5bMzc2pXbt2XP42b95M7dq1I7FYTEuWLCEioitXrpC3tzcZGBhQ+fLlaeHChRrjU8aIESOoa9euNHv2bLKxsSELCwuaOnUqrzzqZJOoqO/R09OjU6dOUd26dUlPT4/Cw8O5dyyTC1dXV9q3b5/KOjh8+DB5eHiQVColU1NTatKkCb148YKIisabnj170rx586h8+fJK612d7H/58oUAUGRkJBER5eXlkYODA3l4eHDh9+/fTw4ODry1ryouXbpE9vb2Gv3JePv2Lenq6tLWrVvV+mvYsCGNGDFCwX3Hjh0EgKKjo1WG/ZY1GpHqvik7O5umTZtGDg4OJJVKyc/PjxISErg4P3/+TFOmTCFHR0cSCoVkb29PAQEB3PMNGzZwfY25uTm1atWK8vPzlZahtOvy58+fEwBauXIl1a5dm0QiEVWtWpWuXbvG+bl69Srp6elx/fj27dvJycmJl766+WVpx19NOgHZO7G3t+fmDStXrlT5nhn/PJjS9l/Inj17yMzMjBtEnjx5Qh07diSRSMRTSoWFhdHmzZvp3r17lJycTLNmzSKRSERv374lIqKRI0fS4MGDiaiosVtYWJC1tTXX+Wzfvp2qVKmicpJw9+5dkkqlNHbsWLp79y4lJCTQsmXLuDz06dOH3N3d6a+//qJHjx7Rzz//TJaWllz6ixYtonr16imUrfgANW7cODI2Nqa+fftSVFQURUREkL29PTd5fP/+PXXv3p369etHmZmZlJmZSXl5eTR//nyqVq0aXbx4kdLS0uj69eu8iZc8nTp1IqlUSsOGDaP79+9TfHw8FRYWUufOncnHx4euXLlCycnJFBgYSObm5tzC49dffyUdHR3y9PSk8PBwiomJIQ8PD2rSpAkX94QJE0gqlVLHjh3p7t27XCf6888/U82aNencuXOUkpJCa9euJZFIxE041ZXhy5cv5OXlRV27dqXw8HBKSkqiIUOGUKVKlbj3Vdq6k+ft27fUoEEDmjFjBuevsLCQVq9eTSKRiNq2bUs3b96k2NhYqlWrFm/hv3z5cnJ0dKRDhw5RSkoK7du3j8RisdpF8qlTp2j37t0UHx9PDx48oKFDh3ILESKitLQ0srGxoW7dutHNmzcpKSmJNm/eTJGRkfTp0yf65ZdfyMfHh8trTk4O7d27lywsLHjphIWFka6uLqe4ePPmDc2fP5/Cw8MpLS2N9u7dS2KxmKfItrCw4H34kKd9+/ZkYmJCkyZNori4OLp48SJJpVJuISsb8IsPpPn5+WRgYEB//vknERFFRUURAKpXrx6dO3eOEhISqF69elSnTh3y9fWlS5cu0b1798jV1ZW3GO3RowcZGhrSyJEjKSkpiU6ePEmGhoa0aNEizo8mmTt69CgBIA8PDzp//jw9fPhQQZlFVLQYt7W1peHDh1N8fDxFRERQtWrVaMiQIURElJOTQydPniQdHR1KTU1VKVvatJ/ExERauXIlRUVFUWpqKq1du5Z0dHQoLi6OiIiysrIIANnZ2dG2bdsoOTmZ0tPT6fbt29zkMzMzk7KysoiIqFatWrRixQou/j59+pCTkxMdPHiQHj16RBcuXKBNmzYREdGJEye4CaVMZkxNTWnlypX08OFDunr1KlWqVInmzZtHREQPHz4kIyMjWrt2LSUnJ9P9+/dp/fr13IJAnoSEBDIyMqLJkyfTo0eP6MiRI2RnZ0c6Ojr06dMnrdJU1T41hVNX9oKCApX1p0mGtCmTMtavX09nz56l5ORkCg8PpwYNGnAKgy9fvtCKFSvI2dmZK+Pnz5+VxqNNn/Of//yHLl68SKmpqXTlyhWqXLkyTwm8YcMGkkqltGLFCkpISKA7d+7Q3LlziYjow4cPJBAIyMXFhXbt2kWPHj2igIAAEolEvEV3cTTJRadOnWjChAlEVKRIkpUxMzOTbt26RVZWVtx7O3ToEFlaWtL27dspOTmZzp49SxYWFvT777+rrFtt+obi7aKwsJCMjY3J3t6e1q9fTw8fPqTVq1cTAAWFqzrWr19PQqGQ/Pz86M6dO3T37l2qXr06tW7dmvOjqTxZWVnUr18/6tKlC2VmZtLr16819j+yOi3NmC7P69evCQBVrlyZdu/eTcnJyQrvOzIykoyMjCgwMJAePnxIly9fJltbW05mZPVrZGREM2bMoAcPHlBSUhLXd1WuXJkOHDhADx8+JH9/f3J2diYfHx86efIkJSYmUuPGjcnf35+LS1OfKJNR2QdJ2Zgim3t9+PCBJ2OjR48mR0dHevLkCffeStoWZWWJiooiIs1jBJHm8VIZmzZtIhMTE9qxYwelpKTQjh07SFdXl1MIKBsPZOWSR13fr807rV69OllYWNCCBQsoMTGRDh06RAKBgFNgfP36lXx8fKhWrVpcfZ48eZL7EKqpj9amHT5+/Jj09PTo5MmTXPsgUi7/RES7d++mo0ePUlJSEt27d486dOjAfZDLzc2lQ4cOkUgkoqdPn1JmZiZ9+PCBiIjMzMzo6NGjRET07t07qlq1KnXq1IkiIyMpNjaWvL29yc/Pj6sbTfNPZWhaU2iaG8qP1+riKygoIAMDAzp58iQvD66urtwHGfl2RKR+7FA1/5Rvf2UhW8po1aqV2s0pRMR9QFm2bBk9fPiQUlJSuPy5urrS4cOHKTk5mV68eEFnzpwhAwMDWrFiBaWkpNCJEydIKpXSb7/9pjY+ZdSvX5+MjIwoKCiIkpOT6bfffiMdHR3au3cv50edbBIRrVixgkQiEXl7e9P169cpPj6ecnJyyNXVlUaNGkWxsbH08OFDOnTokMoPxFevXiUTExPas2cPpaamUnR0NIWGhnLz07p165KZmRnNmzePHjx4oFDv2si+SCTi+qM9e/aQjY0Nubi4cM99fHx4Y686Sqq0TU9PJwA0c+ZMlQpL2Zpjw4YNCs+ePXtGAGjnzp0q0/iWNRqR8r4pNzeXfH19qXHjxhQWFkaJiYnUvXt3cnNz497NsGHDyNvbm8LCwig1NZUuXLhA+/fvJyKiXbt2kZ2dHZ08eZLS0tIoIiKCVq1apbIMpV2Xnz17lmsrly5dogcPHpC/vz9VqFCBCgoKiIho9erVVL16dV5anTp14v5XN78kKt34q41OoEWLFvTTTz9RZGQkpaSk0OnTp5Vu1GL8c2FK238hU6ZMIR0dHZJKpSSRSAgASSQS2r59u9pwX79+5X0BnD59OnXv3p2IijoJf39/8vPzoxMnThARkYeHh8LuDRkFBQVUvXp1GjhwoNLnJ0+eJJFIxH0dkqWvp6fHdRK9evWioUOH8sJNnTqVtzhp2rQpNWnShKc47tatG40dO5b7v1atWrydC7Jw6nZiyePk5KTwhfj333+nypUrK+z6MDQ05AbkUaNGkYWFBU+xdejQIdLX1+cGTF9fX6pVqxZPaXXt2jUyMzPjJtgyqlevzg2W6sowb9488vHx4dWLbHGZnp7OhS9N3SnD1NRUYXIrGxCys7M5tylTpnD1mJKSQlKplB48eMAL1759ewoJCdGYpgzZznLZhLdt27bk6+ur8mNCp06daPLkyTy36dOnU/PmzXlu69at07groWHDhrR8+XIiUr6LQ57y5ctT3759eW4eHh60dOlSIiI6d+4ciUQinizExcURAE6BI1uEJiYmcn6Cg4PJ0NCQnj59yrmNGjWKfvrpJ+5/V1dX6tChAy/t4cOHcwoSbWQuODiYpFIppaamqq2Xrl278topEdG2bdt4E7dff/2V3Nzc1MajTftRhq2tLff1+/Lly9zu4OIcPXpUQVGfm5tLQqGQLl26RERFOx5EIhHva35x5s6dyymQ8/Pzyc3NTWFXV2hoKDVr1oyIiLZu3UoODg4qlXfy+Pn5KSyyevXqRa6urlqnSaTYPrUJp6nsyupPGxnSVCZt2bhxI9WsWZP7f8KECdS1a1e1YUrb58yYMYM6duxIREWLHpFIpHI8le0iP3/+POcWGxvLOyEijya5cHJyUppeWloaOTo68j60WVhY0IULF3j+xo0bx32AVYamvkG+XSQnJxMA3m7ST58+EQDejk9NjBgxgmxsbHgK9t27d5NEIqHCwkKty+Pl5cVb5GrT/5R2TJfnwoULBID7qEak+L7r1aunsJtzzpw5PGWYUCjkjb1E/+27ipd/x44dJBAIKCYmhnNbvHgx1a1bV2n+ZBTvE+V3+uzYsYMcHR2VhgsODiZHR0e1ynht2uLly5dJX1+fk3Ft3pGm8VKeFy9ekEQiUTj94e3tzc2VVI0H8mjq/zS9U9l8NjAwkOfH0tKSS/s///kPWVpacju4iqNNH61NO4yKiiKBQKDw0UGZ/Cvjjz/+IGNjY+7/FStWKGymSEtLIwDcvGDy5MlUs2ZN3jzm4sWLJBAIuA9z2sw/i6NpTaEM+blh8fFaU3wPHjwgADyFvuz0VXh4OBEptiNlFB87iJTPP+XbX1nIljKsrKxIX1+fpFIp7yfr12XvUb59yfqc27dvc25fv34lW1tbbsetjAEDBnAKJFXxySNTkI8fP57n3rJlSxo5cqTKcPKyOWjQILKzs+PNF2VtpPjpLHXMnj2bvLy8lK4f8vLySCQS8XbbE/HrXRvZt7GxodOnTxNRkbJ62bJlZG5uTkRFY4dEIlGYQ8nIycnhvTuxWEwCgYDnpkkxP2bMGBIIBGRubk5du3alnTt38uYesvFL2Vj+6tUrAkB79uxRm0ZxSrpGU9Y3rVmzhmxsbHj9WEpKCu/dli9fXqVOon///pxcakNp1+WLFy8mfX193seJO3fuEABO3zFkyBDeO/L19eVOD2maXypDm/FXk04gNzeXdHV1mZL2Xw5T2v4LadasGY0fP54ePnxIERER5OPjQ9OnT1fwd+bMGWrbti05OTmRoaEhSaVSAsBNIBcvXkxt2rShwsJCcnNzo/Pnz1Pnzp1p586ddPv2bTI1NVW5M+rKlSukq6urdDJKVGR6oWfPnjy3goICEgqF3Nf6KlWq0OrVq3l+WrZsSTNmzOD+NzExUTjmUbduXW5H0NevX0lfX19h5+bSpUtJR0eHWrZsSb/++iu9efNGaT6Jir6cAqA//viD5+7r60t6enoKEyAAdOPGDSIiatCgAe94BlGRwlpPT4/rPM3MzGjjxo08P4MGDSJdXV2FuHV0dGj37t0ay+Dk5ERCoVBp3mSKvdLWnTyygbO4Al4Wl/yuiZ9++onbMRYcHKww2ZBKpaSrq0sLFixQmd7u3bvJ19eXOxImkUjI0NCQCgsLua/Isg8PyqhQoQJvJwARUZs2beiXX37huQ0fPpx69+7N/R8bG0sDBgwgNzc3MjY25t6HbIEov4tDnrdv3yoswImIzM3NOZlXtvjevXs32dracv9PnDiRfHx8eH5kR7aK4+/vzy0OPn/+TDo6OnTx4kWen4kTJ1LLli2JSDuZ69y5M69OlPHx40fS09PjmWIhKvrSbWJiwv0/cuRIjZMobdpPWFgYde3alSpVqkRGRkacnMsWVitXriQ7OzuF9zJnzhze7gcioujoaN7kslmzZjwTMfL89NNP3FFJmTLAwMCAV39CoZCr4wcPHpCFhQU5OzvT9OnT1R4xky145GW5f//+3Mc0bdJU1j61Caep7MrqT5MMaVMmZXz69Ik7eWFlZcXltbjSx9vbm7cbQRna9DmvXr2i6dOnU61atcjCwoKkUinp6+tzi+iQkBCqVauWyjRWr15NFStW5LmdOHGCJ/vyqJML2fgjX2fyClui/x5dlC+fvr4+DR8+XGna2vQN8u3iyJEjJBaLecrWe/fu8fxoQ/369RVMzBw7doyEQqHW5ZEt+GWLDW36n28Z0+VZvnw5lS9fnudW/H3LTO/IK//mz5/PyZGsfot/iCMq6rvklalz584lLy8vntuYMWN4izRNfaL8Tp+JEyfyFEsylClsS9sWV65cyS0stXlH2oyX8vz6669ka2vL7WiS0aJFC27eoWo8kEdd/6fNO42MjCQAlJyczD1///49b7dxxYoVuY++8mjTR2vTDrdt20bOzs68uFXJf15eHq1bt44aNmxItra2nFKo+MfrAQMG8HZDExW12eL9m42NjYLJmevXrxMA7kSGpvmnPJrWFETq54ZE/PFaU3z79u3jFGkybty4QTo6OtxGBPl2pGnsIFI+/yze/spKtuSRzY33799PDx8+5P1yc3OJ6L99r2z3dPH8NWzYkOd29uxZ0tfXV/gYMGzYMG7XoKr45JGZvZPfhdu5c2eun9dGNmvXrs1bHxIVrWPq1atHpqamNGzYMDp37pzatn/lyhUyMDCgGjVq0Ny5c3nm3GRtq3h/KF/v2sh+lSpVaN++fXTjxg2qUKECvXnzhnR0dKiwsJBGjx6tcqwmKtpdX/zd7dq1i2xsbHhumZmZKsPLePbsGW3ZsoW6d+9O+vr6VL9+fU5BuXPnTtLX11f6MeLGjRsa11jfskZT1Td5eXkprAUyMjJ4dT9u3DjS09Ojzp0702+//cbTT+zdu5f09PSoYcOGtGzZMt4GF2WUdl3eq1cvhbWYzDylbA5et25dztydLC3ZB1VN88vSjr/a6ATat29PEomE+vTpQ4cPH+b6Bca/Bz0w/nVERUVhxIgRqFSpEgBg/fr1qFGjBkaMGIGKFSsCAH7//XeMHj0aQUFBCAoKgpmZGY4cOYI1a9bAysoKAGBmZoZPnz7h7Nmz0NPTQ8uWLbFr1y5kZWVh3bp1GDZsGKRSqco8uLi4cHHJExcXh549e/LckpOTkZubixo1auDLly94+PAhz1B3YWEhIiMjMXToUABAWloa3r9/j4YNG3J+8vLyEBcXx4WLi4tDXl4eatasyUtrypQp6NixI44dO4Y1a9Zg5syZuHv3Llc/xYmJiYGuri6aNm2q4L5hwwb4+PgohHF0dERhYSHu37+PCRMm8J5FRkZyN4s/fvwYWVlZ8PX1VYh79uzZ6Nu3r0Lc9vb2astgYWGBtLQ0nD17lpOB4tjZ2X1T3ckTHR0Nc3NzlC9fnnPLz89HXFwcFi1axPMbFRWFDh06cGUcNmwYpk2bphCntbW10rTmzZuHtWvXIiQkBB4eHjAxMcGqVasQFRUFgUCAqKgoCIVC1KlTR2n4rKwsPH78WOGSoJiYGPTp04fnFhERgV69egEAEhMTUb9+fQwaNAgbNmxAuXLl8Pz5c/j5+XH1pekW4ZiYGOjo6MDLy4tze/z4Md6+fcvFoeySjIiICF5+Y2Ji0KBBA56fqKgohcsBYmJi0Lt3b+7vwsJChbgjIyO5uLSRuZiYGEyfPl1p+WQkJiYiPz9f4fK6+Ph43s3A0dHR6NSpk8p4tGk/ly5dQtu2bTFlyhRMmjQJlpaWCA8Px+DBg7n0Y2Ji4OPjo/BelF0WFRMTgwoVKnCX5URFRWH48OEq8xgdHQ1/f38ubM2aNXH48GEFf7Ibtd3c3JCamopTp07h0KFDqFu3LpYtW6ZwE7csbj09PYXblCMjIzlZ1SZNZe1Tm3DalF1Z/amTofPnz2sskzI6duyIDx8+ICAgAJUrV4ahoSF69+7Nk+eYmBhMnjxZZRwyP+r6nNzcXDRt2hTly5fHvHnz4OTkBIlEAm9vby6tqKgoXhuWJzo6Wmn7VHdZjTq5iImJgZ6eHu+26vT0dPj4+GDw4MEICgrila9t27ZYvXq1Qhrm5uYq60SbvqF4u5DJT/FLY6KiouDo6Kj1RVOFhYWIjY1V6E8iIyN5/aGm8iQlJSE7O5sLo03/U9oxXRkxMTG8cRTgv++4uDjo6+srXPImnx9bW1sFP6r6enn5i4mJQZs2bQBAqz5R/vKkmJgYhboICQnB9u3bceXKFV7ZS9sWi49t2r4jTeOlPHFxcahatSrvwj4iQkJCAjeWqxoP5FHX/2n7Tq2treHs7Mw9j46Ohr6+PqpWrYr3798jNTVV4f3K0KaP1qYdquqnlcn/yJEjcfXqVcyZMwfVqlWDsbExpk6dyos/Ojqam4MXj0+Wxps3b/D8+XOl79bBwQGmpqZazT/l0bSm0DQ3lOVdNl5rik/VXMzV1ZW7sKl4O9Jm7FA3/5S9i7KQLWVERkZCIBCgTZs2Ki88i4mJQb169Tj5Ku4uv0aJi4tDhQoVFOKKj4/nLjVTFZ880dHRMDY25q2/6P9fqC0bqzXJZn5+PuLj4/Gf//yHF7dQKMStW7dw6dIlHDt2DN26dUOLFi1w9OhRpXnx9vZGeno6Tpw4gX379iEkJAT79+9H165dER0dDVtbW15/WLzetZF94L9r65UrV2L8+PEwNTUFEeHp06fYtWsXwsLCVNaVQCDgresyMjKgq6urdK2nDltbWwwdOhRDhw7FunXrMG7cOCQmJqJatWqIjIxE1apVIRKJFMLt3bsXzs7OKtvpt67RVPVNcXFxGDduHM8tPj4eQqEQbm5uAIA1a9Zg6NChOHbsGGbPno3Zs2cjJiYGJiYm6NWrF5o0aYJjx45h165dmDVrFi5cuKAwdgP4pnV5dHQ0Bg4cyHsWGRkJS0tL2Nvbc2tjWf3J0tJ2flma8ffDhw8adQIAcOLECYSFheH48eMYNWoUFixYgIiIiO92AS6j7GFK238ZKSkpePfuHW/QqFq1KipVqoS9e/di5syZAIDt27djxIgRmDp1KgCgoKAAx48f5zV8U1NTfPz4EStWrOCUCsbGxnj06BEOHz6M2NhYlfnQ19fHhw8fVD43MjLCly9feG7Lly9H/fr14eLigtjYWBQUFHCdMQD88ccfePPmDTfhiYmJgYGBAc9PfHw8vn79ypXj/v37cHR0hImJiUIeXF1dMW3aNEyYMAEmJiaIj49XqrSV3RZa/GZNWRkLCwtVDpZJSUn4/PkzCgoKOLcvX75g48aNmDFjBhe3oaGhQhz6+vrIzc3VOBArK4Ovry8EAoHagfxb66449+/fV1DCJCQk4OvXr7yBWbZIkcWvr6+Pz58/l2iysX37dgQGBmLUqFEAgE+fPuHMmTNo27YtF2d+fj6ys7OV3kJ7//596Onpwd3dnXP79OkTMjMzeXWRkJCA6OhoTum8f/9+uLm5Yd26dZyfbdu28d6dpluEY2JiULlyZd6HjqioKJiamsLJyQlAkcz06NGDe56Tk4OjR49yC05ZPCNHjuT+ly12itf1mzdv8PTpU55CGQBPFqOionDjxg1s2LCBqzt1Micb+FVNtmTIJujF2/fHjx+xefNmhISEAPivQnbOnDkq43n06JHG9vP777/D398f8+fP5/zMmjWL116jo6M55XVx7t+/jy5duvDc5Be4+vr6eP/+vdL8yd8Aq6+vj7dv38LZ2VntJMfIyAi9e/dG7969MXjwYNy4cUOp0lZHRweFhYXIzc2Fnl7RUHzmzBnehE+bNJW1T23CqSu7LF75+tMkQ9qUSZ7Hjx/j4sWLnIICAO7du4fIyEhuQff48WO8f/9e4wcmTX3O1atX8eDBA9y6dYvr906fPo3MzExenaurl+joaF57BTQrbQHVchEdHQ13d3cIhUIAqhW2xfNWkj5Vm75Bvl3ExMQo9APalLE4iYmJyM7OVmjfmzZtQnBwsNbliY6OhpWVFWxtbQFo1/+UdkxXRkxMjMIH6OJ1YWRkhIKCAuTl5XGL4MePH+PQoUM4duwYlx9l/Wp0dLTSuDt27Mj9T0S4f/8+p/zWtk8s/pEkJiaGtyBWpbD9lrYYHR2Nfv36cXUCqH9H2oyX8iibVx44cABZWVncB0JV44E86tq5Nu9UVRupWrUqhEIh8vLyIBAIVKahTR+tTTu8f/8+vL29eX6UyX9BQQF+++03HD58mJOvZ8+e4eLFi5g1axaAIsVYQkKC0ncr6x+kUil0dHR476GgoACrVq3ifWzUNP9UVh/q1hSa5obKxmt18SUlJfHyR0TYt28frx8s3o5u3bqlcexQNv+U1Yes/ZWFbCkjMjISzs7OKhW2svIo64diYmIwevRonpuRkRFycnJ4brdv38atW7ewadMmtfEpS5eIUFhYyMn6sWPH8Pr1a3Tv3l0r2YyPj0dubq7S9GSbjlq2bIn69etjypQpavNjaWmJIUOGYMiQIWjWrBlu3brFKW3V1bs2sg8Ura3j4uJw/vx5bNq0CTo6OpBKpVi5ciXq16+voPT93nz58gUCgQAWFhYAimSlbt26Cv7++OMPbNiwAbt27VL50etb12iqxmZVOoPOnTvz/NauXRu1a9fG4MGD4eTkhIyMDK49Ojg4YNy4cRg7dixcXFwQGRmpVGlb2nW5bLNZ8TlNYWEhVq1ahYEDB0JHRwdxcXG8fk72wUI2pqkbd0o7/urq6mrUCQBFHwQaN26Mxo0bo127dmjWrBmys7NhaGioMgzjnwVTr//LuHv3LvT09BS+0rZs2ZL3ZdHc3BzXr19HfHw87ty5g+7du+PevXu8AcnMzAyPHj1CTEwMN9k2NjbG9u3b0bp1a5UTZwDw8/PDmzdvMH36dDx8+BD37t1DaGgoXr9+DQBo164dduzYgStXruDRo0eYM2cOdu/ezS0SLSwsIBAIEB4eDqBoQjRu3DhIJBJUrlwZwH+/7hef0EZHR8PR0ZH7ollYWIisrCzEx8fj+fPnKCgowJIlS7Bz507Ex8cjMTERs2bNgpmZGRo1aqS0LMomSADg7++PkJAQnDx5Emlpabh9+zYWLFiAW7ducXkxNDTEihUrEBMTg+joaLRp0wbly5fHiBEjVJZBFveaNWuwZ88epKam4u7du1i9ejVOnjwJAGrLIJVK4ePjg4kTJ+LSpUtIS0vD9evXMX36dKSmpn5T3SmjsLAQGRkZSE5OxvPnz7m47O3tebsY5HcC+Pv748CBA1i/fj1SUlIQExODrVu3Yvv27UrTAYrk9vz583j06BGuXbuGLl264MmTJ9z78fLygomJCUaPHo2EhATEx8djw4YNePDgAZfXwsJChIeH4/nz58jNzYVEIoGBgQEnawkJCZy8yybo5ubmSElJ4ZQ6s2bNwqZNm3g7a7VR2spPhOSVIZaWlrh79y4KCwvx9u1bDB48GBkZGZyfJ0+e4O3bt7x4ZJNV+S+tIpEIVapU4dIxNjZGQEAA0tPT8eeff6J9+/aYNm0at4DQJHOynU/yCkB5XFxcUKVKFcyePRsPHz7ErVu34O/vj2rVqmHYsGEAFHfHKUOb9mNubo6oqChERkbi/v37GD58OP744w9OHmS7L5S138LCQty7dw/Pnj3jJknyu2vatGmDpUuX4uLFi0hNTcXZs2exZ88eLn+6urrcBLt58+Z49eoVxowZw7XL48ePY/bs2QCAsLAwzJkzB7dv30Z6ejqOHDmCU6dOoWvXrkrLX69ePejr62PKlClISUnBiRMnuJ1fMnnQlKasnPLtU5tw6squqv40yZA2ZZLH2NgYenp6OHz4MFJTU3Ho0CH07t0bhYWF3HstLCwEAK5dy0/uZWjqc2S7N/fv34+UlBRs27YNY8aMga6uLif3bdq04XZrpKamIiwsDKGhoQD+e8JAXt7U9Q2a5KK4TGZkZMDHxwf+/v4YOXIknj9/jufPn+PNmzdc+W7evImQkBA8fPgQcXFx2LdvH5YtW6Y0bVneNPUN8u1CVV9W3M+BAwfU9uXR0dEwMjLC0qVLERcXh3v37qF169aoXLkyt5NPm/LIp6tN/1PaMV0eWf+iri68vLxgbm6O6dOnIyUlBRcvXkTbtm3RvXt3biGrLD/K4n737h3S0tJ45U1NTcWHDx84N236xOIfSWRjiuz/+fPnY+3atdi/fz9EIhEnY1+/fi11W5SVRZaGtu9I03gpT7t27XDr1i38/vvvSEtLw2+//YaRI0di5cqVsLS0VDseyKOu/9P2naqTC6lUiiZNmmD27Nm4desWkpOTceTIEZw+fRqAdn20Nu2wsLAQ8fHxePr0Kd6+fcuFk68DXV1dmJqa4uTJk0hNTcUff/yBrl274sOHD5xfKjKZh8jISDx//hyfPn3i4pOlKRaL4efnh4ULFyIuLg4xMTHo1q0biIjbNKLN/FMeTWsKTXND+fFaU3yWlpaIiYnB169fkZ2djYkTJyIyMpKTP/l2pM3YoWz+Kd/+ykK2lHH37l24ublx7Vn2y8rK4vwokwtZ/uTdW7RogdevX2Pp0qVIS0vD8ePH0bVrV8yYMYM7FaKqn5UnOjoaAoEAQUFBePLkCQ4ePIhBgwYhNDQUlpaWWslmTEyMwpojOzsbw4cPx19//cWtg9auXatyznXs2DEsWbIEkZGRSEtLw7Zt23Dnzh3eBx91Y7s2sg8Ura03b96Mfv36cfJubGyMDRs2KJwsU0bx91elShXcuXOH5/bu3Tul4a5fv46OHTvi+PHjSEpKQnx8PFauXImgoCCMHz8eNjY23A5nZ2dnPH/+HI8fP8bly5cxduxYdOjQAUFBQQofEovzrWs0VTLTrl07rF69Gnfv3kVCQgJGjhyJmJgYLF26FAAwffp0HDx4EElJSYiNjcXs2bNRtWpVVKlSBVu2bMH69etx7949JCcnIzQ0FG/evEHr1q2VlqG06/L79+9DIBBg165duHnzJhISEtCzZ0+8e/eO+7gQHR0NBwcHTkEuS0u2jlQ3vyzt+KuNTmD06NE4deoUUlJScPfuXSxevBitWrViCtt/Gz/ILAOjlEyfPl3p5UnHjh0jgUDAGdVPTEwkT09PEolEVKNGDTp27BiZm5vzjNhHREQQAJ6x+8WLFxMAtTeUyjh+/DjVqlWLxGIxWVpaUrdu3ThbQjk5OTRhwgSytrYmIyMjatu2Ld27d48Xfv78+WRmZkbly5en/v37U0BAAHl4eHDPf/rpJ4VLC+RvYXz//j21bNmSM9b+7t07CgkJIVdXVy5fnTp1Umukvl69ekoN6b9//55Gjx5N9vb2JBKJyNHRkfr3789dPjJjxgxq3749hYaGkoWFBRkbG9PIkSPp48ePXBxdu3ZVevFCbm4uzZw5kxwdHUkoFJKdnR117dqVs6+kqQzPnj2jPn36ULly5UgsFlOlSpVo5MiRnLH50tadMh4/fkxeXl4kFAo5G2CTJ09WMCS/YsUKBVs9y5Yto8qVK5NIJCJra2vy9/fnXXYgT1hYGLm7u5NYLKYGDRrQ6dOnFQzmX79+nRo2bEhSqZTMzMyodevWnH23vLw86tWrF2fLRyZzsgtQ7OzsqFOnTrRkyRKytLTk4szOzqaffvqJDAwMqHz58rRgwQLq3LkzV4fKbhGWp27dugpyVPxWeCKi8PBwqlKlCllYWFCDBg1o3759JBAIKDY2loiK7LnK283dvn27gt3D5cuX8y4L8fLyooULF9KQIUPIwMCA7O3tFS6Y0yRza9asoWrVqqksX3Hi4+PJx8eHS2vatGm8C+n27t3Lq19laNN+Xrx4Qc2bNyexWEyVK1emTZs2UZ06dbh6ltmSUnYB1O+//052dnYEgLNnbGFhQYcOHeL8vH37lgYPHsy1o+rVq3MXeq1Zs4Znz46oyM6bp6cnSaVSMjU1pQYNGnC33F+9epWaNGlCJiYmJJFIqFatWmpv4JXl0cHBgczMzMjPz49mzJihUG/q0iRS3j61Caeu7KrqT5MMaVsmeTZt2kTlypUjQ0ND+umnn2jdunUK7WDixIlkYmJCALjLMpWhqc8JCQkhMzMzMjMzo8GDB1NwcDBP7gsLC2nBggWcjTBHR0fOHq4yeZPdVq/K1qAmuahXrx5n/yw0NJQAKPzatGnDq9/q1auTRCIhCwsLatasmYLt0OJo0zcUbxeyvk7+khJTU1OerdHq1avTiBEjVKYbEBBA7dq1o9WrV5OlpSWZmJjQmDFjePY5tSlP69atacqUKbwwmvqf0o7p8shsHGp63zdu3CAPDw+SSCTk7OxMixYt4l2kaGFhoXCBkDJZunTpEu8yL6Iiu6bF24+2faLsMpXiY0phYSEZGxsrlbFbt24RUenaonya2rwjbcZLZWzevJmcnZ1JLBZTvXr1OFuBqupUFZr6v9K801q1avFstj5+/Jh++uknMjc3J6lUSp6enlw9E6nvo7Vth+fPnydnZ2fS0dHhLiZVJf8nTpwgR0dHkkgk1KJFC9qzZ49CfS1dupQsLS0JAK1evVrp3CcjI4M6dOhARkZGZGVlRcOHD+fduaDN/FMZ6tYUmuaGysZrdfElJyeTh4cHmZmZUe3atWnXrl1kampKp06dIiLFdkSkeexQNv9UNqcrC9mSx8bGRmm7HjBgABGplidl+Stef1WrVuVsy27atIl7pio+ZZQrV462bt1KnTp1IrFYTC4uLrR//36eH02yqWzN8fz5c+rQoQOVK1eOhEIhVaxYkQIDA1VeHHf48GHy9PQkIyMjMjQ05ORIhjb1rkn2iYjGjh1LAoGAkpKSOLcqVaqQs7Ozgj1ueb58+aL0PRb/qbqQNS4ujvr3708VK1bkZL5Jkya0b98+7v3KLuCT/QwNDcnNzY2GDh2q1bv81jWaqr4pKyuL+vfvT2ZmZmRqako9evTgLtUmIho9ejQ5OTmRSCQiGxsb6t+/P6fv2LBhA9WoUYMMDAzI1NSUmjdvTmFhYSrLUNp1+caNG6l69eq0a9cusrOzIwMDA+rRowd3iTRRkZy2b9+el5bMzjaR+vklUennwup0Al+/fqUePXqQvb09CYVCcnBwoHHjxnE2mBn/HgRERGWsB2Yw/ido164datWqhYULF/7orDAY/zpY+2Ew/p1ERUWhWbNmiI2NhYODw4/ODoPBYDAYDMZ3Y+zYscjKyuKdSGMw/k6YeQQGo5RER0drtLHIYDCUw9oPg/HvZM2aNVixYgVT2DIYDAaDwfg/D1uzMH40bKctg1EKXr9+DSsrK+42YwaDoT2s/TAYDAaDwWAwGIx/MkQEExMT7Nu3D/7+/j86O4z/UZjSlsFgMBgMBoPBYDAYDAaDwWAw/kEw8wgMBoPBYDAYDAaDwWAwGAwGg/EPgiltGQwGg8FgMBgMBoPBYDAYDAbjHwRT2jIYDAaDwWAwGAwGg8FgMBgMxj8IprRlMBgMBoPBYDAYDAaDwWAwGIx/EExpy2AwGAwGg8FgMBgMBoPBYDAY/yCY0pbBYDAYDAaDwWAwGAwGg8FgMP5BMKUtg8FgMBgMBoPBYDAYDAaDwWD8g2BKWwaDwWAwGAwGg8FgMBgMBoPB+AfBlLYMBoPBYDAYDAaDwWAwGAwGg/EP4v8BAn2Va9Ak7wIAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1400x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Successfully created raw count visualization!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "NTA Data Analysis - Raw Particle Count Visualization with Instrument Settings (Cell 10d)\n",
    "\n",
    "This cell creates plots showing the actual raw particle counts vs. size for averaged data,\n",
    "including critical instrument settings from metadata and error bars for replicate variability.\n",
    "\"\"\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "def plot_raw_counts_with_settings(distribution_df, uniqueID=None, metadata=None, config=None):\n",
    "    \"\"\"\n",
    "    Create visualizations of raw particle counts vs. size for averaged data, with instrument settings.\n",
    "    \n",
    "    Parameters:\n",
    "    distribution_df (DataFrame): Processed NTA data with averaged values\n",
    "    uniqueID (str): Unique identifier for the dataset\n",
    "    metadata (dict): Metadata dictionary\n",
    "    config (dict): Configuration dictionary\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (success_flag, filepath)\n",
    "    \"\"\"\n",
    "    # Validate input\n",
    "    if distribution_df is None or distribution_df.empty:\n",
    "        print(\"No data available for plotting\")\n",
    "        return False, \"No data available for plotting\"\n",
    "    \n",
    "    # Extract sample ID\n",
    "    id_text = uniqueID or (distribution_df['uniqueID'].iloc[0] if 'uniqueID' in distribution_df.columns else 'Unknown')\n",
    "    \n",
    "    # Create figure\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # Define color scheme - same as other plots\n",
    "    color = '#4C5B5C'  # Slate gray for raw counts\n",
    "    \n",
    "    # Determine which columns to use for raw counts (before dilution correction)\n",
    "    raw_count_col = None\n",
    "    raw_count_sd_col = None\n",
    "    \n",
    "    # Look for the averaged number data (before normalization)\n",
    "    if 'number_avg' in distribution_df.columns:\n",
    "        raw_count_col = 'number_avg'\n",
    "        raw_count_sd_col = 'number_sd' if 'number_sd' in distribution_df.columns else None\n",
    "    else:\n",
    "        print(\"Raw count data column not found in the dataset\")\n",
    "        return False, \"Raw count data column not found in the dataset\"\n",
    "    \n",
    "    # Plot 1: Linear scale raw counts\n",
    "    lin_df = distribution_df[distribution_df['scale'] == 'linear'].sort_values('size_nm')\n",
    "    if not lin_df.empty:\n",
    "        # Plot raw counts with error bars if available\n",
    "        if raw_count_sd_col and raw_count_sd_col in lin_df.columns:\n",
    "            ax1.errorbar(lin_df['size_nm'], lin_df[raw_count_col], \n",
    "                        yerr=lin_df[raw_count_sd_col],\n",
    "                        fmt='o', color=color, ecolor=color, alpha=0.7,\n",
    "                        capsize=3, capthick=1, markersize=4, linewidth=1.5)\n",
    "        else:\n",
    "            ax1.scatter(lin_df['size_nm'], lin_df[raw_count_col], color=color, s=30, alpha=0.8)\n",
    "        \n",
    "        ax1.set_title('Raw Particle Counts - Linear Scale', fontsize=14)\n",
    "        ax1.set_xlabel('Size (nm)', fontsize=12)\n",
    "        ax1.set_ylabel('Raw Counts (particles)', fontsize=12, color=color)\n",
    "        ax1.tick_params(axis='y', labelcolor=color)\n",
    "        ax1.spines['left'].set_color(color)\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Set reasonable x-axis limits\n",
    "        percentile_95 = np.percentile(lin_df['size_nm'], 95)\n",
    "        max_size = min(percentile_95 * 1.2, 500)  # Cap at 500 nm or use 95th percentile + 20%\n",
    "        ax1.set_xlim([0, max_size])\n",
    "        \n",
    "        # Set y-axis to start from 0\n",
    "        y_min, y_max = ax1.get_ylim()\n",
    "        ax1.set_ylim([0, y_max])\n",
    "    \n",
    "    # Plot 2: Log scale raw counts \n",
    "    log_df = distribution_df[distribution_df['scale'] == 'logarithmic'].sort_values('size_nm')\n",
    "    if not log_df.empty:\n",
    "        # Plot raw counts with error bars if available\n",
    "        if raw_count_sd_col and raw_count_sd_col in log_df.columns:\n",
    "            ax2.errorbar(log_df['size_nm'], log_df[raw_count_col], \n",
    "                        yerr=log_df[raw_count_sd_col],\n",
    "                        fmt='o', color=color, ecolor=color, alpha=0.7,\n",
    "                        capsize=3, capthick=1, markersize=4, linewidth=1.5)\n",
    "        else:\n",
    "            ax2.scatter(log_df['size_nm'], log_df[raw_count_col], color=color, s=30, alpha=0.8)\n",
    "        \n",
    "        ax2.set_title('Raw Particle Counts - Log Scale', fontsize=14)\n",
    "        ax2.set_xlabel('Size (nm)', fontsize=12)\n",
    "        ax2.set_ylabel('Raw Counts (particles)', fontsize=12, color=color)\n",
    "        ax2.tick_params(axis='y', labelcolor=color)\n",
    "        ax2.spines['left'].set_color(color)\n",
    "        ax2.set_xscale('log')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Set reasonable x-axis limits for log scale\n",
    "        min_size = max(30, log_df['size_nm'].min())  # Don't go below 30 nm\n",
    "        percentile_90 = np.percentile(log_df['size_nm'], 90)\n",
    "        max_size = min(percentile_90 * 1.3, 300)\n",
    "        ax2.set_xlim([min_size, max_size])\n",
    "        \n",
    "        # Set y-axis to start from 0\n",
    "        y_min, y_max = ax2.get_ylim()\n",
    "        ax2.set_ylim([0, y_max])\n",
    "    \n",
    "    # Determine replicate info for title\n",
    "    replicate_info = \"\"\n",
    "    if metadata and 'num_replicates' in metadata:\n",
    "        num_reps = metadata['num_replicates']\n",
    "        if num_reps and str(num_reps) != '1':\n",
    "            replicate_info = f\" (n={num_reps} replicates)\"\n",
    "    \n",
    "    # Add overall title\n",
    "    fig.suptitle(f'Raw Particle Counts vs Size: {id_text}{replicate_info}', fontsize=16, y=0.98)\n",
    "    \n",
    "    # Calculate total raw count (use linear data preferentially)\n",
    "    if not lin_df.empty:\n",
    "        total_raw_count = lin_df[raw_count_col].sum()\n",
    "        if raw_count_sd_col and raw_count_sd_col in lin_df.columns:\n",
    "            # Error propagation for sum: sqrt(sum of variances)\n",
    "            total_raw_count_sd = np.sqrt((lin_df[raw_count_sd_col] ** 2).sum())\n",
    "        else:\n",
    "            total_raw_count_sd = 0\n",
    "    elif not log_df.empty:\n",
    "        total_raw_count = log_df[raw_count_col].sum()\n",
    "        if raw_count_sd_col and raw_count_sd_col in log_df.columns:\n",
    "            total_raw_count_sd = np.sqrt((log_df[raw_count_sd_col] ** 2).sum())\n",
    "        else:\n",
    "            total_raw_count_sd = 0\n",
    "    else:\n",
    "        total_raw_count = 0\n",
    "        total_raw_count_sd = 0\n",
    "    \n",
    "    # Extract and format instrument settings from metadata\n",
    "    instrument_settings = []\n",
    "    if metadata:\n",
    "        # Extract key NTA parameters\n",
    "        cycles = metadata.get('nta_cycles', metadata.get('cycles', 'Unknown'))\n",
    "        if cycles != 'Unknown':\n",
    "            instrument_settings.append(f\"Cycles: {cycles}\")\n",
    "        \n",
    "        fps = metadata.get('nta_fps', metadata.get('fps', 'Unknown'))\n",
    "        if fps != 'Unknown':\n",
    "            instrument_settings.append(f\"FPS: {fps}\")\n",
    "        \n",
    "        # Extract total number of traces (summed across replicates)\n",
    "        traces = metadata.get('nta_number_of_traces_sum', metadata.get('nta_number_of_traces', 'Unknown'))\n",
    "        if traces != 'Unknown':\n",
    "            instrument_settings.append(f\"Total traces: {traces}\")\n",
    "        \n",
    "        # Extract video file info\n",
    "        avi_size = metadata.get('nta_avi_filesize', 'Unknown')\n",
    "        if avi_size != 'Unknown':\n",
    "            instrument_settings.append(f\"Video size: {avi_size}\")\n",
    "        \n",
    "        # Extract temperature\n",
    "        temp = metadata.get('nta_temperature', metadata.get('temperature', 'Unknown'))\n",
    "        if temp != 'Unknown':\n",
    "            instrument_settings.append(f\"Temp: {temp}\")\n",
    "    \n",
    "    # Add explanation text about what raw counts represent\n",
    "    explanation_parts = [\n",
    "        \"Raw counts represent the actual number of particles detected at each size bin,\",\n",
    "        \"before normalization or concentration calculation\"\n",
    "    ]\n",
    "    \n",
    "    if replicate_info:\n",
    "        explanation_parts.append(\"Error bars show ¬± SD across replicates\")\n",
    "    \n",
    "    explanation = \" \".join(explanation_parts)\n",
    "    fig.text(0.5, 0.01, explanation, ha='center', fontsize=10, style='italic')\n",
    "    \n",
    "    # Add data statistics and instrument settings\n",
    "    if total_raw_count_sd > 0:\n",
    "        stats_text = f\"Total raw particles detected: {total_raw_count:.0f} ¬± {total_raw_count_sd:.0f}\"\n",
    "    else:\n",
    "        stats_text = f\"Total raw particles detected: {total_raw_count:.0f}\"\n",
    "    \n",
    "    # Create text block for statistics and settings\n",
    "    text_block = [stats_text]\n",
    "    if instrument_settings:\n",
    "        text_block.append(\"Instrument Settings: \" + \" | \".join(instrument_settings))\n",
    "    \n",
    "    # Add the text block to the figure\n",
    "    fig.text(0.5, 0.04, \"\\n\".join(text_block), ha='center', fontsize=11)\n",
    "    \n",
    "    # Adjust layout to make room for the text\n",
    "    bottom_margin = 0.06 + 0.02 * len(text_block)\n",
    "    plt.tight_layout(rect=[0, bottom_margin, 1, 0.95])\n",
    "    \n",
    "    # Save figure\n",
    "    filepath = None\n",
    "    \n",
    "    # Determine the output directory\n",
    "    if config is not None and \"directory\" in config:\n",
    "        output_dir = os.path.join(config[\"directory\"], \"processed\")\n",
    "    else:\n",
    "        output_dir = os.path.join(os.getcwd(), \"processed\")\n",
    "    \n",
    "    # Ensure the output directory exists\n",
    "    try:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Create descriptive filename\n",
    "        base_path = os.path.join(output_dir, f\"Plot_{id_text}_raw_counts\")\n",
    "        \n",
    "        # Save in both PDF and PNG formats\n",
    "        pdf_path = f\"{base_path}.pdf\"\n",
    "        plt.savefig(pdf_path, bbox_inches='tight', dpi=300)\n",
    "        \n",
    "        png_path = f\"{base_path}.png\"\n",
    "        plt.savefig(png_path, bbox_inches='tight', dpi=300)\n",
    "        \n",
    "        print(f\"‚úì Saved raw count plots: {os.path.basename(pdf_path)}/.png\")\n",
    "        filepath = pdf_path\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö† Failed to save plot: {str(e)}\")\n",
    "        return False, str(e)\n",
    "    \n",
    "    # Update metadata with total raw particle count\n",
    "    if metadata and total_raw_count > 0:\n",
    "        try:\n",
    "            # Get output directory for metadata\n",
    "            if config is not None and \"directory\" in config:\n",
    "                metadata_dir = os.path.join(config[\"directory\"], \"metadata\")\n",
    "            else:\n",
    "                metadata_dir = os.path.join(os.getcwd(), \"metadata\")\n",
    "            \n",
    "            # Ensure metadata directory exists\n",
    "            os.makedirs(metadata_dir, exist_ok=True)\n",
    "            \n",
    "            # Construct metadata filepath\n",
    "            unique_id = metadata.get('persistentID', id_text)\n",
    "            metadata_path = os.path.join(metadata_dir, f\"Data_{unique_id}_metadata.txt\")\n",
    "            \n",
    "            # Read existing metadata to preserve all fields\n",
    "            existing_data = {}\n",
    "            if os.path.exists(metadata_path):\n",
    "                with open(metadata_path, 'r') as f:\n",
    "                    for line in f:\n",
    "                        parts = line.strip().split('\\t')\n",
    "                        if len(parts) >= 2:\n",
    "                            existing_data[parts[0]] = parts[1]\n",
    "            \n",
    "            # Add total raw particle count with uncertainty\n",
    "            if total_raw_count_sd > 0:\n",
    "                existing_data['nta_total_raw_particles'] = f\"{total_raw_count:.0f} ¬± {total_raw_count_sd:.0f}\"\n",
    "            else:\n",
    "                existing_data['nta_total_raw_particles'] = f\"{total_raw_count:.0f}\"\n",
    "            \n",
    "            # Write updated metadata\n",
    "            with open(metadata_path, 'w') as f:\n",
    "                for key, value in existing_data.items():\n",
    "                    f.write(f\"{key}\\t{value}\\t\\n\")\n",
    "            \n",
    "            print(f\"‚úì Updated metadata with total raw particle count\")\n",
    "            \n",
    "            # Update global metadata variable if it exists\n",
    "            if 'current_metadata' in globals():\n",
    "                if total_raw_count_sd > 0:\n",
    "                    globals()['current_metadata']['nta_total_raw_particles'] = f\"{total_raw_count:.0f} ¬± {total_raw_count_sd:.0f}\"\n",
    "                else:\n",
    "                    globals()['current_metadata']['nta_total_raw_particles'] = f\"{total_raw_count:.0f}\"\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö† Could not update metadata: {str(e)}\")\n",
    "    \n",
    "    plt.show()\n",
    "    return True, filepath\n",
    "\n",
    "\n",
    "# Execute if we have the required data\n",
    "if 'current_distribution_df' in globals() and current_distribution_df is not None:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"GENERATING RAW PARTICLE COUNT VISUALIZATION\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    uniqueID = current_metadata.get('persistentID', 'unknown') if 'current_metadata' in globals() else 'unknown'\n",
    "    metadata = current_metadata if 'current_metadata' in globals() else None\n",
    "    config = CONFIG if 'CONFIG' in globals() else None\n",
    "    \n",
    "    print(f\"Creating raw particle count plots for: {uniqueID}\")\n",
    "    \n",
    "    success, filepath = plot_raw_counts_with_settings(\n",
    "        current_distribution_df, \n",
    "        uniqueID, \n",
    "        metadata,\n",
    "        config\n",
    "    )\n",
    "    \n",
    "    if success:\n",
    "        current_raw_count_plot = filepath\n",
    "        print(f\"\\n‚úì Successfully created raw count visualization!\")\n",
    "    else:\n",
    "        print(f\"ERROR: {filepath}\")\n",
    "\n",
    "else:\n",
    "    print(\"No data found. Run the complete workflow first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13566dcb-0013-42eb-b53a-a95a94bf902c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "GENERATING RAW COUNT vs THEORETICAL SURFACE AREA PLOTS WITH D-VALUES\n",
      "================================================================================\n",
      "Creating count vs surface area plots for: 20250527_0004_23052025_SUV_5136664_after6weeks_size_488_avg3\n",
      "Surface Area = œÄ √ó diameter¬≤ (spherical particles) in ¬µm¬≤\n",
      "Includes: Linear + Logarithmic (Lognormal fits + D-values for both)\n",
      "Creating linear count vs surface area plot...\n",
      "      Fitting with 70 valid data points\n",
      "      Surface area range: 0.0002 - 2.1253 ¬µm¬≤\n",
      "      Count range: 0.3 - 176.0\n",
      "      Initial parameters: mu=-3.138, sigma=0.643, amp=88.0\n",
      "      Bounds: mu=[-10.944, 3.057], sigma=[0.050, 5.000]\n",
      "      Final parameters: mu=-2.935, sigma=0.558, amp=10.8\n",
      "      Curve range: 0.0001 - 2.5504 ¬µm¬≤, max value: 169.5\n",
      "      Fit parameters: geo_mean=0.0531 ¬µm¬≤, geo_std=1.748\n",
      "    ‚úì Lognormal fit successful for Linear scale\n",
      "  ‚úì Saved fit to: Fits_20250527_0004_23052025_SUV_5136664_after6weeks_size_488_avg3_all.json\n",
      "  ‚úì Saved: Plot_20250527_0004_23052025_SUV_5136664_after6weeks_size_488_avg3_linear_count_vs_surface_area.pdf/.png\n",
      "Creating log count vs surface area plot...\n",
      "      Fitting with 91 valid data points\n",
      "      Surface area range: 0.0001 - 2.1794 ¬µm¬≤\n",
      "      Count range: 0.3 - 148.3\n",
      "      Initial parameters: mu=-3.138, sigma=0.642, amp=74.2\n",
      "      Bounds: mu=[-11.670, 3.082], sigma=[0.050, 5.000]\n",
      "      Final parameters: mu=-2.789, sigma=0.551, amp=10.4\n",
      "      Curve range: 0.0001 - 2.6153 ¬µm¬≤, max value: 142.2\n",
      "      Fit parameters: geo_mean=0.0615 ¬µm¬≤, geo_std=1.736\n",
      "    ‚úì Lognormal fit successful for Logarithmic scale\n",
      "  ‚úì Saved fit to: Fits_20250527_0004_23052025_SUV_5136664_after6weeks_size_488_avg3_all.json\n",
      "  ‚úì Saved: Plot_20250527_0004_23052025_SUV_5136664_after6weeks_size_488_avg3_log_count_vs_surface_area.pdf/.png\n",
      "\n",
      "‚úì Successfully created 2 count vs surface area plots!\n",
      "  - Plot_20250527_0004_23052025_SUV_5136664_after6weeks_size_488_avg3_linear_count_vs_surface_area.pdf\n",
      "  - Plot_20250527_0004_23052025_SUV_5136664_after6weeks_size_488_avg3_log_count_vs_surface_area.pdf\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "NTA Data Analysis - Raw Particle Counts vs Theoretical Surface Area Plots (Cell 10e)\n",
    "\n",
    "This module creates two-subplot plots for raw particle counts vs theoretical surface area:\n",
    "- Linear scale with theoretical surface area bins (displayed in linear space)\n",
    "- Logarithmic scale with theoretical surface area bins (displayed in log space)\n",
    "\n",
    "Layout: 60% main distribution (top) + 40% cumulative (bottom)\n",
    "Visual style: Based on cell_10c surface area plots\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.lines import Line2D\n",
    "from scipy import stats as scipy_stats\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "\n",
    "def lognormal_pdf(x, mu, sigma, amplitude):\n",
    "    \"\"\"Calculate lognormal probability density function with numerical stability.\"\"\"\n",
    "    # Add small epsilon to avoid log(0)\n",
    "    x_safe = np.maximum(x, 1e-10)\n",
    "    \n",
    "    # Calculate the lognormal PDF\n",
    "    log_x = np.log(x_safe)\n",
    "    exponent = -((log_x - mu) ** 2) / (2 * sigma ** 2)\n",
    "    \n",
    "    # Avoid numerical overflow/underflow\n",
    "    exponent = np.clip(exponent, -50, 50)\n",
    "    \n",
    "    pdf = amplitude * np.exp(exponent) / (x_safe * sigma * np.sqrt(2 * np.pi))\n",
    "    \n",
    "    # Ensure no NaN or inf values\n",
    "    pdf = np.nan_to_num(pdf, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    \n",
    "    return pdf\n",
    "\n",
    "\n",
    "def fit_lognormal_distribution(surface_areas, counts):\n",
    "    \"\"\"Fit a lognormal distribution to count vs surface area data.\"\"\"\n",
    "    try:\n",
    "        # Check if we have enough data points\n",
    "        if len(surface_areas) < 5:\n",
    "            return False, \"Insufficient data points for fitting\"\n",
    "        \n",
    "        # Initial parameter estimates using more robust method\n",
    "        # Convert to log space for initial estimates\n",
    "        log_sa = np.log(surface_areas)\n",
    "        \n",
    "        # Weighted statistics for initial parameter estimates\n",
    "        weights = counts / np.sum(counts)  # Normalize weights\n",
    "        initial_mu = np.average(log_sa, weights=weights)\n",
    "        initial_sigma = np.sqrt(np.average((log_sa - initial_mu)**2, weights=weights))\n",
    "        \n",
    "        # Ensure sigma is reasonable\n",
    "        initial_sigma = max(0.1, min(initial_sigma, 3.0))\n",
    "        \n",
    "        # Scale amplitude based on data\n",
    "        max_count = np.max(counts)\n",
    "        initial_amplitude = max_count * 0.5  # Start with conservative amplitude\n",
    "        \n",
    "        initial_params = [initial_mu, initial_sigma, initial_amplitude]\n",
    "        \n",
    "        print(f\"      Initial parameters: mu={initial_mu:.3f}, sigma={initial_sigma:.3f}, amp={initial_amplitude:.1f}\")\n",
    "        \n",
    "        # More reasonable bounds\n",
    "        lower_bounds = [\n",
    "            np.log(surface_areas.min() * 0.1),  # mu lower bound\n",
    "            0.05,                               # sigma lower bound  \n",
    "            max_count * 0.01                    # amplitude lower bound\n",
    "        ]\n",
    "        upper_bounds = [\n",
    "            np.log(surface_areas.max() * 10),   # mu upper bound\n",
    "            5.0,                                # sigma upper bound\n",
    "            max_count * 5.0                     # amplitude upper bound\n",
    "        ]\n",
    "        \n",
    "        print(f\"      Bounds: mu=[{lower_bounds[0]:.3f}, {upper_bounds[0]:.3f}], sigma=[{lower_bounds[1]:.3f}, {upper_bounds[1]:.3f}]\")\n",
    "        \n",
    "        # Check if initial parameters are within bounds\n",
    "        for i, (param, lower, upper) in enumerate(zip(initial_params, lower_bounds, upper_bounds)):\n",
    "            if param < lower or param > upper:\n",
    "                print(f\"      Adjusting parameter {i}: {param:.3f} -> bounds [{lower:.3f}, {upper:.3f}]\")\n",
    "                initial_params[i] = max(lower, min(param, upper))\n",
    "        \n",
    "        # Perform curve fitting with adjusted parameters\n",
    "        params, covariance = curve_fit(\n",
    "            lognormal_pdf, surface_areas, counts, \n",
    "            p0=initial_params,\n",
    "            bounds=(lower_bounds, upper_bounds), \n",
    "            maxfev=5000,\n",
    "            method='trf'  # Trust Region Reflective algorithm, more robust\n",
    "        )\n",
    "        \n",
    "        # Generate fitted curve with more points and proper range\n",
    "        sa_min = surface_areas.min() * 0.8  # Extend slightly beyond data\n",
    "        sa_max = surface_areas.max() * 1.2\n",
    "        sa_range = np.linspace(sa_min, sa_max, 500)  # More points for smoother curve\n",
    "        fitted_curve = lognormal_pdf(sa_range, *params)\n",
    "        \n",
    "        # Ensure the curve is smooth and reasonable\n",
    "        if np.any(np.isnan(fitted_curve)) or np.any(np.isinf(fitted_curve)):\n",
    "            return False, \"Fit produced invalid values (NaN or inf)\"\n",
    "        \n",
    "        # Check if fit is reasonable\n",
    "        if np.max(fitted_curve) > max_count * 10 or np.max(fitted_curve) < max_count * 0.01:\n",
    "            return False, \"Fit amplitude is unreasonable\"\n",
    "        \n",
    "        print(f\"      Final parameters: mu={params[0]:.3f}, sigma={params[1]:.3f}, amp={params[2]:.1f}\")\n",
    "        print(f\"      Curve range: {sa_min:.4f} - {sa_max:.4f} ¬µm¬≤, max value: {np.max(fitted_curve):.1f}\")\n",
    "        \n",
    "        return True, (sa_range, fitted_curve, params)\n",
    "        \n",
    "    except Exception as e:\n",
    "        return False, f\"Lognormal fit failed: {str(e)}\"\n",
    "\n",
    "\n",
    "def add_d_value_lines_and_bands_surface_area(ax, stats):\n",
    "    \"\"\"Add D-value lines and uncertainty bands to a subplot, converted to surface area in ¬µm¬≤.\"\"\"\n",
    "    legend_elements = []\n",
    "    \n",
    "    if not stats or 'D10_avg' not in stats:\n",
    "        return legend_elements\n",
    "    \n",
    "    # Convert diameter D-values to surface area D-values using A = œÄ √ó d¬≤ and convert to ¬µm¬≤\n",
    "    d10_avg_size = stats['D10_avg']\n",
    "    d10_lower_size = stats.get('D10_lower', d10_avg_size)\n",
    "    d10_upper_size = stats.get('D10_upper', d10_avg_size)\n",
    "    \n",
    "    d50_avg_size = stats['D50_avg'] \n",
    "    d50_lower_size = stats.get('D50_lower', d50_avg_size)\n",
    "    d50_upper_size = stats.get('D50_upper', d50_avg_size)\n",
    "    \n",
    "    d90_avg_size = stats['D90_avg']\n",
    "    d90_lower_size = stats.get('D90_lower', d90_avg_size)\n",
    "    d90_upper_size = stats.get('D90_upper', d90_avg_size)\n",
    "    \n",
    "    # Convert to surface areas in ¬µm¬≤ (divide by 1e6 to convert from nm¬≤ to ¬µm¬≤)\n",
    "    d10_avg_sa = (np.pi * (d10_avg_size ** 2)) / 1e6\n",
    "    d10_lower_sa = (np.pi * (d10_lower_size ** 2)) / 1e6\n",
    "    d10_upper_sa = (np.pi * (d10_upper_size ** 2)) / 1e6\n",
    "    \n",
    "    d50_avg_sa = (np.pi * (d50_avg_size ** 2)) / 1e6\n",
    "    d50_lower_sa = (np.pi * (d50_lower_size ** 2)) / 1e6\n",
    "    d50_upper_sa = (np.pi * (d50_upper_size ** 2)) / 1e6\n",
    "    \n",
    "    d90_avg_sa = (np.pi * (d90_avg_size ** 2)) / 1e6\n",
    "    d90_lower_sa = (np.pi * (d90_lower_size ** 2)) / 1e6\n",
    "    d90_upper_sa = (np.pi * (d90_upper_size ** 2)) / 1e6\n",
    "    \n",
    "    span = stats.get('span_avg', (d90_avg_size-d10_avg_size)/d50_avg_size if d50_avg_size > 0 else 0)\n",
    "    \n",
    "    # Add D-value lines and bands using surface area values in ¬µm¬≤\n",
    "    for d_val_sa, d_lower_sa, d_upper_sa, style, width, alpha_band in [\n",
    "        (d10_avg_sa, d10_lower_sa, d10_upper_sa, '--', 1.5, 0.15),\n",
    "        (d50_avg_sa, d50_lower_sa, d50_upper_sa, '-', 2.5, 0.25), \n",
    "        (d90_avg_sa, d90_lower_sa, d90_upper_sa, '--', 1.5, 0.15)\n",
    "    ]:\n",
    "        if not np.isnan(d_val_sa):\n",
    "            ax.axvline(x=d_val_sa, color='gray', linestyle=style, alpha=0.8, linewidth=width, zorder=5)\n",
    "            if not np.isnan(d_lower_sa) and not np.isnan(d_upper_sa) and (d_lower_sa != d_val_sa or d_upper_sa != d_val_sa):\n",
    "                ax.axvspan(d_lower_sa, d_upper_sa, alpha=alpha_band, color='gray', zorder=1)\n",
    "    \n",
    "    # Create legend elements showing diameter in nm and surface area in ¬µm¬≤\n",
    "    legend_elements.extend([\n",
    "        Line2D([0], [0], color='gray', linestyle='--', linewidth=1.5, \n",
    "              label=f'D10: {d10_avg_size:.1f} nm ‚Üí {d10_avg_sa:.3f} ¬µm¬≤ ({d10_lower_size:.1f}-{d10_upper_size:.1f} nm)'),\n",
    "        Line2D([0], [0], color='gray', linestyle='-', linewidth=2.5, \n",
    "              label=f'D50: {d50_avg_size:.1f} nm ‚Üí {d50_avg_sa:.3f} ¬µm¬≤ ({d50_lower_size:.1f}-{d50_upper_size:.1f} nm)'),\n",
    "        Line2D([0], [0], color='gray', linestyle='--', linewidth=1.5, \n",
    "              label=f'D90: {d90_avg_size:.1f} nm ‚Üí {d90_avg_sa:.3f} ¬µm¬≤ ({d90_lower_size:.1f}-{d90_upper_size:.1f} nm)'),\n",
    "        Line2D([0], [0], color='white', linestyle='', \n",
    "              label=f'Span: {span:.3f}')\n",
    "    ])\n",
    "    \n",
    "    return legend_elements\n",
    "\n",
    "\n",
    "def add_count_fit_curve(ax, plot_df, is_log_scale, fit_color='#F25C54'):\n",
    "    \"\"\"Add lognormal fits for count vs surface area distributions in ¬µm¬≤.\"\"\"\n",
    "    fit_legend_elements = []\n",
    "    \n",
    "    surface_areas = plot_df['surface_area_um2'].values\n",
    "    counts = plot_df['number_avg'].values\n",
    "    \n",
    "    # Remove any zero or negative values\n",
    "    valid_mask = (counts > 0) & (surface_areas > 0) & np.isfinite(counts) & np.isfinite(surface_areas)\n",
    "    if not np.any(valid_mask):\n",
    "        print(f\"      No valid data points for fitting\")\n",
    "        return fit_legend_elements, None\n",
    "    \n",
    "    surface_areas_clean = surface_areas[valid_mask]\n",
    "    counts_clean = counts[valid_mask]\n",
    "    \n",
    "    print(f\"      Fitting with {len(surface_areas_clean)} valid data points\")\n",
    "    print(f\"      Surface area range: {surface_areas_clean.min():.4f} - {surface_areas_clean.max():.4f} ¬µm¬≤\")\n",
    "    print(f\"      Count range: {counts_clean.min():.1f} - {counts_clean.max():.1f}\")\n",
    "    \n",
    "    # Use lognormal fit for both linear and log scales\n",
    "    success, result = fit_lognormal_distribution(surface_areas_clean, counts_clean)\n",
    "    if success:\n",
    "        sa_range, fitted_curve, params = result\n",
    "        ax.plot(sa_range, fitted_curve, '-', color=fit_color, linewidth=2.5, \n",
    "               alpha=0.9, label='Lognormal Fit', zorder=4)\n",
    "        \n",
    "        geometric_mean_sa = np.exp(params[0])\n",
    "        geometric_std = np.exp(params[1])\n",
    "        \n",
    "        print(f\"      Fit parameters: geo_mean={geometric_mean_sa:.4f} ¬µm¬≤, geo_std={geometric_std:.3f}\")\n",
    "        \n",
    "        # Add fit info to legend with ¬µm¬≤ units\n",
    "        fit_legend_elements.append(\n",
    "            Line2D([0], [0], color=fit_color, linestyle='-', linewidth=2.5,\n",
    "                  label=f'Lognormal: geo_mean={geometric_mean_sa:.3f} ¬µm¬≤, geo_std={geometric_std:.2f}')\n",
    "        )\n",
    "        \n",
    "        return fit_legend_elements, ('lognormal', {'mu': params[0], 'sigma': params[1], 'amplitude': params[2]})\n",
    "    else:\n",
    "        print(f\"      Lognormal fit failed: {result}\")\n",
    "        return fit_legend_elements, None\n",
    "\n",
    "\n",
    "def create_count_vs_surface_area_plot(plot_df, is_log_scale, stats=None, uniqueID=None, metadata=None):\n",
    "    \"\"\"Create a two-subplot plot for raw counts vs theoretical surface area.\"\"\"\n",
    "    \n",
    "    scale_name = \"Logarithmic\" if is_log_scale else \"Linear\"\n",
    "    xscale = 'log' if is_log_scale else 'linear'\n",
    "    color = '#4059AD'  # Indigo blue (same as cell_10c surface area plots)\n",
    "    \n",
    "    # Calculate theoretical surface area for spherical particles: A = œÄ * d¬≤ and convert to ¬µm¬≤\n",
    "    plot_df = plot_df.copy()\n",
    "    plot_df['surface_area_um2'] = (np.pi * (plot_df['size_nm'] ** 2)) / 1e6  # Convert nm¬≤ to ¬µm¬≤\n",
    "    \n",
    "    # Sort by surface area\n",
    "    plot_df = plot_df.sort_values('surface_area_um2')\n",
    "    \n",
    "    # Create figure\n",
    "    fig = plt.figure(figsize=(7, 9))\n",
    "    gs = gridspec.GridSpec(2, 1, height_ratios=[0.6, 0.4], hspace=0.3, \n",
    "                          top=0.82, bottom=0.08)\n",
    "    \n",
    "    # =================================================================\n",
    "    # TOP SUBPLOT: MAIN DISTRIBUTION WITH ERROR BARS AND FITS\n",
    "    # =================================================================\n",
    "    ax1 = fig.add_subplot(gs[0])\n",
    "    \n",
    "    # Plot main distribution with error bars\n",
    "    if 'number_sd' in plot_df.columns:\n",
    "        ax1.errorbar(plot_df['surface_area_um2'], plot_df['number_avg'], \n",
    "                    yerr=plot_df['number_sd'],\n",
    "                    fmt='o', color=color, ecolor=color, alpha=0.7,\n",
    "                    capsize=3, capthick=1, markersize=6, linewidth=1.5,\n",
    "                    label='Raw Counts')\n",
    "    else:\n",
    "        ax1.scatter(plot_df['surface_area_um2'], plot_df['number_avg'], \n",
    "                   color=color, s=60, alpha=0.8, label='Raw Counts')\n",
    "    \n",
    "    # Add lognormal fit curve\n",
    "    fit_result = add_count_fit_curve(ax1, plot_df, is_log_scale)\n",
    "    if isinstance(fit_result, tuple):\n",
    "        fit_legend_elements, fit_results = fit_result\n",
    "    else:\n",
    "        fit_legend_elements = fit_result\n",
    "        fit_results = None\n",
    "    \n",
    "    # Debug: Print fit status\n",
    "    if fit_results:\n",
    "        print(f\"    ‚úì Lognormal fit successful for {scale_name} scale\")\n",
    "    else:\n",
    "        print(f\"    ‚ö† Lognormal fit failed for {scale_name} scale\")\n",
    "    \n",
    "    # Format top subplot\n",
    "    ax1.set_ylabel('Raw Particle Counts', color=color, fontsize=14, labelpad=10)\n",
    "    ax1.tick_params(axis='y', labelcolor=color, labelsize=12)\n",
    "    ax1.tick_params(axis='x', labelsize=12)\n",
    "    ax1.spines['left'].set_color(color)\n",
    "    \n",
    "    # Set x-axis scale and add better tick labels for log scale\n",
    "    ax1.set_xscale(xscale)\n",
    "    if is_log_scale:\n",
    "        from matplotlib.ticker import LogLocator, LogFormatter\n",
    "        ax1.xaxis.set_major_locator(LogLocator(base=10, numticks=12))\n",
    "        ax1.xaxis.set_minor_locator(LogLocator(base=10, subs=(0.2, 0.4, 0.6, 0.8), numticks=12))\n",
    "        ax1.xaxis.set_major_formatter(LogFormatter(base=10, labelOnlyBase=False))\n",
    "    \n",
    "    # Smart range calculation based on where the count signal is\n",
    "    weights_for_range = plot_df['number_avg'].values\n",
    "    sa_for_range = plot_df['surface_area_um2'].values\n",
    "    \n",
    "    # Find where 99% of the count signal is contained\n",
    "    cumsum_weights = np.cumsum(weights_for_range)\n",
    "    total_weight = cumsum_weights[-1]\n",
    "    \n",
    "    if total_weight > 0:\n",
    "        # Find 1st and 99th percentiles of the count-weighted distribution\n",
    "        p1_idx = np.searchsorted(cumsum_weights, 0.01 * total_weight)\n",
    "        p99_idx = np.searchsorted(cumsum_weights, 0.99 * total_weight)\n",
    "        \n",
    "        signal_min_sa = sa_for_range[max(0, p1_idx)]\n",
    "        signal_max_sa = sa_for_range[min(len(sa_for_range)-1, p99_idx)]\n",
    "        data_max_sa = plot_df['surface_area_um2'].max()\n",
    "        \n",
    "        if is_log_scale:\n",
    "            # Log scale: focus on the count signal range with some padding\n",
    "            min_sa = max(signal_min_sa * 0.7, (np.pi * (20**2)) / 1e6)  # Don't go below 20nm diameter equivalent\n",
    "            max_sa = min(signal_max_sa * 1.8, data_max_sa * 1.2)\n",
    "            ax1.set_xlim([min_sa, max_sa])\n",
    "        else:\n",
    "            # Linear scale: tighten the range, focus on main signal\n",
    "            min_sa = 0\n",
    "            max_sa = min(signal_max_sa * 1.15, (np.pi * (350**2)) / 1e6)  # Cap at ~350nm diameter equivalent\n",
    "            ax1.set_xlim([min_sa, max_sa])\n",
    "    else:\n",
    "        # Fallback if no signal\n",
    "        if is_log_scale:\n",
    "            ax1.set_xlim([(np.pi * (30**2)) / 1e6, (np.pi * (300**2)) / 1e6])\n",
    "        else:\n",
    "            ax1.set_xlim([0, (np.pi * (250**2)) / 1e6])\n",
    "    \n",
    "    # Set y-axis to start from 0\n",
    "    y_min, y_max = ax1.get_ylim()\n",
    "    ax1.set_ylim([0, y_max])\n",
    "    \n",
    "    # Add D-value lines and bands (converted to surface area)\n",
    "    d_legend_elements = add_d_value_lines_and_bands_surface_area(ax1, stats)\n",
    "    \n",
    "    # Create comprehensive legend for top plot - PLACE OUTSIDE\n",
    "    main_legend = [Line2D([0], [0], marker='o', color='w', markerfacecolor=color, \n",
    "                          markersize=8, label='Raw Counts')]\n",
    "    \n",
    "    all_legend_elements = main_legend + fit_legend_elements + d_legend_elements\n",
    "    leg1 = ax1.legend(handles=all_legend_elements, fontsize=9, frameon=True, \n",
    "                     bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    leg1.get_frame().set_alpha(0.95)\n",
    "    leg1.get_frame().set_edgecolor('lightgray')\n",
    "    \n",
    "    ax1.grid(True, linestyle='--', alpha=0.4)\n",
    "    ax1.set_xlabel('')  # No x-label on top plot\n",
    "    \n",
    "    # =================================================================\n",
    "    # BOTTOM SUBPLOT: CUMULATIVE DISTRIBUTION WITH UNCERTAINTY BANDS\n",
    "    # =================================================================\n",
    "    ax2 = fig.add_subplot(gs[1])\n",
    "    \n",
    "    # Calculate cumulative counts as percentage\n",
    "    cumsum_counts = np.cumsum(plot_df['number_avg'])\n",
    "    max_cumsum = cumsum_counts.iloc[-1] if len(cumsum_counts) > 0 else 1\n",
    "    \n",
    "    if max_cumsum > 0:\n",
    "        cumsum_percentage = (cumsum_counts / max_cumsum) * 100\n",
    "        ax2.plot(plot_df['surface_area_um2'], cumsum_percentage, '-', \n",
    "                color=color, linewidth=3, alpha=0.9, label='Cumulative %')\n",
    "        \n",
    "        # Add uncertainty bands if available\n",
    "        if 'number_sd' in plot_df.columns:\n",
    "            # Calculate cumulative uncertainty\n",
    "            cumsum_sd = np.sqrt(np.cumsum(plot_df['number_sd'] ** 2))\n",
    "            cumsum_sd_percentage = (cumsum_sd / max_cumsum) * 100\n",
    "            ax2.fill_between(plot_df['surface_area_um2'], \n",
    "                           cumsum_percentage - cumsum_sd_percentage,\n",
    "                           cumsum_percentage + cumsum_sd_percentage,\n",
    "                           color=color, alpha=0.25, zorder=1, label='¬± SD')\n",
    "    \n",
    "    ax2.set_ylim([0, 110])\n",
    "    ax2.set_ylabel('Cumulative Percentage (%)', color=color, fontsize=14, labelpad=10)\n",
    "    ax2.tick_params(axis='y', labelcolor=color, labelsize=12)\n",
    "    ax2.spines['left'].set_color(color)\n",
    "    \n",
    "    # Format bottom subplot\n",
    "    ax2.set_xlabel('Theoretical Surface Area (¬µm¬≤)', fontsize=14, labelpad=10)\n",
    "    ax2.tick_params(axis='x', labelsize=12)\n",
    "    ax2.set_xscale(xscale)\n",
    "    ax2.set_xlim(ax1.get_xlim())  # Match top plot limits\n",
    "    \n",
    "    if is_log_scale:\n",
    "        # Add same detailed log scale ticks to bottom plot\n",
    "        from matplotlib.ticker import LogLocator, LogFormatter\n",
    "        ax2.xaxis.set_major_locator(LogLocator(base=10, numticks=12))\n",
    "        ax2.xaxis.set_minor_locator(LogLocator(base=10, subs=(0.2, 0.4, 0.6, 0.8), numticks=12))\n",
    "        ax2.xaxis.set_major_formatter(LogFormatter(base=10, labelOnlyBase=False))\n",
    "    \n",
    "    # Add D-value lines to bottom plot (converted to surface area)\n",
    "    add_d_value_lines_and_bands_surface_area(ax2, stats)\n",
    "    \n",
    "    # Legend for bottom plot - PLACE OUTSIDE\n",
    "    cumulative_legend = [\n",
    "        Line2D([0], [0], color=color, linewidth=3, label='Cumulative %'),\n",
    "        Line2D([0], [0], color=color, alpha=0.25, linewidth=8, label='¬± SD')\n",
    "    ]\n",
    "    \n",
    "    leg2 = ax2.legend(handles=cumulative_legend, fontsize=10, frameon=True, \n",
    "                     bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    leg2.get_frame().set_alpha(0.95)\n",
    "    leg2.get_frame().set_edgecolor('lightgray')\n",
    "    \n",
    "    ax2.grid(True, linestyle='--', alpha=0.4)\n",
    "    \n",
    "    # =================================================================\n",
    "    # TITLE AND METADATA\n",
    "    # =================================================================\n",
    "    \n",
    "    # Extract replicate info\n",
    "    replicate_info = \"\"\n",
    "    if metadata and 'num_replicates' in metadata:\n",
    "        num_reps = metadata['num_replicates']\n",
    "        if num_reps and str(num_reps) != '1':\n",
    "            replicate_info = f\" (n={num_reps})\"\n",
    "    \n",
    "    # Set main title\n",
    "    main_title = f'{scale_name} Raw Counts vs\\nTheoretical Surface Area: {uniqueID}{replicate_info}'\n",
    "    fig.suptitle(main_title, fontsize=14, fontweight='bold', y=0.94)\n",
    "    \n",
    "    # Add subtitle\n",
    "    subtitle = f\"Error bars/bands: ¬± SD | Surface Area = œÄ √ó diameter¬≤ | Fits: Lognormal\"\n",
    "    fig.text(0.5, 0.87, subtitle, ha='center', fontsize=11, style='italic')\n",
    "    \n",
    "    return fig, fit_results\n",
    "\n",
    "\n",
    "def generate_count_vs_surface_area_plots(distribution_df, stats_dict=None, uniqueID=None, \n",
    "                                        metadata=None, output_dir=None, config=None):\n",
    "    \"\"\"Generate raw count vs surface area plots for both linear and log scales.\"\"\"\n",
    "    \n",
    "    if distribution_df is None or distribution_df.empty:\n",
    "        return False, \"No data available for plotting\"\n",
    "    \n",
    "    # Check if we have the required columns\n",
    "    if 'number_avg' not in distribution_df.columns:\n",
    "        return False, \"Raw count data (number_avg) not found in dataset\"\n",
    "    \n",
    "    plt.style.use('default')\n",
    "    \n",
    "    # Determine output directory\n",
    "    if output_dir is None:\n",
    "        if config is not None and \"directory\" in config:\n",
    "            base_dir = config[\"directory\"]\n",
    "            output_dir = os.path.join(base_dir, \"processed\")\n",
    "        else:\n",
    "            output_dir = os.path.join(os.getcwd(), \"processed\")\n",
    "    \n",
    "    try:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "    except Exception as e:\n",
    "        return False, f\"Failed to create output directory: {str(e)}\"\n",
    "    \n",
    "    created_files = []\n",
    "    \n",
    "    # Generate linear and logarithmic plots\n",
    "    for is_log_scale in [False, True]:\n",
    "        scale_type = 'logarithmic' if is_log_scale else 'linear'\n",
    "        scale_name = 'log' if is_log_scale else 'linear'\n",
    "        \n",
    "        print(f\"Creating {scale_name} count vs surface area plot...\")\n",
    "        \n",
    "        # Filter data for this scale\n",
    "        plot_df = distribution_df[distribution_df['scale'] == scale_type].copy()\n",
    "        \n",
    "        if plot_df.empty:\n",
    "            print(f\"  Warning: No {scale_type} scale data available\")\n",
    "            continue\n",
    "        \n",
    "        # Get statistics (use number-weighted stats from the appropriate scale)\n",
    "        stats = None\n",
    "        if stats_dict and scale_type in stats_dict and 'number' in stats_dict[scale_type]:\n",
    "            stats = stats_dict[scale_type]['number']\n",
    "        \n",
    "        # Create the plot (matching function signature)\n",
    "        fig, fit_results = create_count_vs_surface_area_plot(plot_df, is_log_scale, stats, uniqueID, metadata)\n",
    "        \n",
    "        if fig is None:\n",
    "            print(f\"  Failed to create plot\")\n",
    "            continue\n",
    "        \n",
    "        # Save fit results to comprehensive fits file\n",
    "        if fit_results:\n",
    "            fit_type, fit_data = fit_results\n",
    "            try:\n",
    "                import json\n",
    "                \n",
    "                # Load existing fits file or create new one\n",
    "                fits_filename = f\"Fits_{uniqueID}_all.json\"\n",
    "                fits_path = os.path.join(output_dir, fits_filename)\n",
    "                \n",
    "                if os.path.exists(fits_path):\n",
    "                    with open(fits_path, 'r') as f:\n",
    "                        all_fits = json.load(f)\n",
    "                else:\n",
    "                    all_fits = {'dataset': uniqueID, 'fits': {}}\n",
    "                \n",
    "                # Add this fit to the collection\n",
    "                fit_key = f\"count_vs_surface_area_{scale_name}\"\n",
    "                all_fits['fits'][fit_key] = {\n",
    "                    'distribution_type': 'count_vs_surface_area',\n",
    "                    'scale': scale_name,\n",
    "                    'fit_type': fit_type,\n",
    "                    'parameters': fit_data\n",
    "                }\n",
    "                \n",
    "                # Save updated fits file\n",
    "                with open(fits_path, 'w') as f:\n",
    "                    json.dump(all_fits, f, indent=2, default=str)\n",
    "                \n",
    "                print(f\"  ‚úì Saved fit to: {fits_filename}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  ‚ö† Failed to save fit: {str(e)}\")\n",
    "        \n",
    "        # Save the plot\n",
    "        try:\n",
    "            base_filename = f\"Plot_{uniqueID}_{scale_name}_count_vs_surface_area\"\n",
    "            \n",
    "            pdf_path = os.path.join(output_dir, f\"{base_filename}.pdf\")\n",
    "            fig.savefig(pdf_path, bbox_inches='tight', dpi=300)\n",
    "            \n",
    "            png_path = os.path.join(output_dir, f\"{base_filename}.png\")\n",
    "            fig.savefig(png_path, bbox_inches='tight', dpi=300)\n",
    "            \n",
    "            created_files.append(pdf_path)\n",
    "            print(f\"  ‚úì Saved: {base_filename}.pdf/.png\")\n",
    "            \n",
    "            plt.close(fig)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚úó Failed to save plot: {str(e)}\")\n",
    "            plt.close(fig)\n",
    "            continue\n",
    "    \n",
    "    return True, created_files\n",
    "\n",
    "\n",
    "# Execute if we have the required data\n",
    "if 'current_distribution_df' in globals() and current_distribution_df is not None:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"GENERATING RAW COUNT vs THEORETICAL SURFACE AREA PLOTS WITH D-VALUES\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    uniqueID = current_metadata.get('persistentID', 'unknown') if 'current_metadata' in globals() else 'unknown'\n",
    "    stats = current_stats if 'current_stats' in globals() else None\n",
    "    metadata = current_metadata if 'current_metadata' in globals() else None\n",
    "    config = CONFIG if 'CONFIG' in globals() else None\n",
    "    \n",
    "    print(f\"Creating count vs surface area plots for: {uniqueID}\")\n",
    "    print(\"Surface Area = œÄ √ó diameter¬≤ (spherical particles) in ¬µm¬≤\")\n",
    "    print(\"Includes: Linear + Logarithmic (Lognormal fits + D-values for both)\")\n",
    "    \n",
    "    success, plot_files = generate_count_vs_surface_area_plots(\n",
    "        current_distribution_df,\n",
    "        stats_dict=stats,\n",
    "        uniqueID=uniqueID,\n",
    "        metadata=metadata,\n",
    "        config=config\n",
    "    )\n",
    "    \n",
    "    if not success:\n",
    "        print(f\"ERROR: {plot_files}\")\n",
    "    else:\n",
    "        print(f\"\\n‚úì Successfully created {len(plot_files)} count vs surface area plots!\")\n",
    "        for filepath in plot_files:\n",
    "            print(f\"  - {os.path.basename(filepath)}\")\n",
    "        \n",
    "        current_count_vs_surface_area_plots = plot_files\n",
    "\n",
    "else:\n",
    "    print(\"No data found. Run the complete workflow first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9af9dcff-dccd-4d18-aaf7-722d1f5f9a7b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "GENERATING RAW COUNT vs THEORETICAL VOLUME PLOTS WITH D-VALUES\n",
      "================================================================================\n",
      "Creating count vs volume plots for: 20250527_0004_23052025_SUV_5136664_after6weeks_size_488_avg3\n",
      "Volume = (œÄ/6) √ó diameter¬≥ (spherical particles) in ¬µm¬≥\n",
      "Includes: Linear + Logarithmic (Lognormal fits + D-values for both)\n",
      "Creating linear count vs volume plot...\n",
      "  ‚úì Saved: Plot_20250527_0004_23052025_SUV_5136664_after6weeks_size_488_avg3_linear_count_vs_volume.pdf/.png\n",
      "Creating log count vs volume plot...\n",
      "  ‚úì Saved: Plot_20250527_0004_23052025_SUV_5136664_after6weeks_size_488_avg3_log_count_vs_volume.pdf/.png\n",
      "\n",
      "‚úì Successfully created 2 count vs volume plots!\n",
      "  - Plot_20250527_0004_23052025_SUV_5136664_after6weeks_size_488_avg3_linear_count_vs_volume.pdf\n",
      "  - Plot_20250527_0004_23052025_SUV_5136664_after6weeks_size_488_avg3_log_count_vs_volume.pdf\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "NTA Data Analysis - Raw Particle Counts vs Theoretical Volume Plots (Cell 10f)\n",
    "\n",
    "This module creates two-subplot plots for raw particle counts vs theoretical volume:\n",
    "- Linear scale with theoretical volume bins (displayed in linear space)\n",
    "- Logarithmic scale with theoretical volume bins (displayed in log space)\n",
    "\n",
    "Layout: 60% main distribution (top) + 40% cumulative (bottom)\n",
    "Visual style: Based on cell_10c surface area plots\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.lines import Line2D\n",
    "from scipy import stats as scipy_stats\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "\n",
    "def lognormal_pdf(x, mu, sigma, amplitude):\n",
    "    \"\"\"Calculate lognormal probability density function with numerical stability.\"\"\"\n",
    "    # Add small epsilon to avoid log(0)\n",
    "    x_safe = np.maximum(x, 1e-10)\n",
    "    \n",
    "    # Calculate the lognormal PDF\n",
    "    log_x = np.log(x_safe)\n",
    "    exponent = -((log_x - mu) ** 2) / (2 * sigma ** 2)\n",
    "    \n",
    "    # Avoid numerical overflow/underflow\n",
    "    exponent = np.clip(exponent, -50, 50)\n",
    "    \n",
    "    pdf = amplitude * np.exp(exponent) / (x_safe * sigma * np.sqrt(2 * np.pi))\n",
    "    \n",
    "    # Ensure no NaN or inf values\n",
    "    pdf = np.nan_to_num(pdf, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    \n",
    "    return pdf\n",
    "\n",
    "\n",
    "def fit_lognormal_distribution(volumes, counts):\n",
    "    \"\"\"Fit a lognormal distribution to count vs volume data with better handling of extreme skewness.\"\"\"\n",
    "    try:\n",
    "        # Check if we have enough data points\n",
    "        if len(volumes) < 5:\n",
    "            return False, \"Insufficient data points for fitting\"\n",
    "        \n",
    "        # For volume data, we need to be much more careful due to extreme skewness\n",
    "        # Find the peak location first\n",
    "        peak_idx = np.argmax(counts)\n",
    "        peak_volume = volumes[peak_idx]\n",
    "        peak_count = counts[peak_idx]\n",
    "        \n",
    "        print(f\"      Peak at volume: {peak_volume:.6f} ¬µm¬≥, count: {peak_count:.1f}\")\n",
    "        \n",
    "        # Use a more robust approach - focus on the main distribution\n",
    "        # Only use data points that are within reasonable range of the peak\n",
    "        volume_range = volumes.max() - volumes.min()\n",
    "        focus_mask = (volumes >= volumes.min()) & (volumes <= peak_volume + volume_range * 0.3)\n",
    "        \n",
    "        if np.sum(focus_mask) < 5:\n",
    "            focus_mask = np.ones_like(volumes, dtype=bool)  # Use all data if focus range too small\n",
    "        \n",
    "        volumes_focus = volumes[focus_mask]\n",
    "        counts_focus = counts[focus_mask]\n",
    "        \n",
    "        print(f\"      Using {len(volumes_focus)} focused data points for fitting\")\n",
    "        \n",
    "        # Initial parameter estimates using the focused data\n",
    "        log_vol = np.log(volumes_focus)\n",
    "        weights = counts_focus / np.sum(counts_focus)\n",
    "        \n",
    "        initial_mu = np.average(log_vol, weights=weights)\n",
    "        initial_sigma = np.sqrt(np.average((log_vol - initial_mu)**2, weights=weights))\n",
    "        \n",
    "        # Constrain sigma much more tightly for volume data\n",
    "        initial_sigma = max(0.2, min(initial_sigma, 2.0))  # Much tighter constraints\n",
    "        \n",
    "        # Better amplitude estimate based on peak\n",
    "        initial_amplitude = peak_count * 0.8\n",
    "        \n",
    "        initial_params = [initial_mu, initial_sigma, initial_amplitude]\n",
    "        \n",
    "        print(f\"      Initial parameters: mu={initial_mu:.3f}, sigma={initial_sigma:.3f}, amp={initial_amplitude:.1f}\")\n",
    "        \n",
    "        # Much tighter bounds for volume fitting\n",
    "        max_count = np.max(counts)\n",
    "        lower_bounds = [\n",
    "            np.log(volumes.min() * 0.5),        # mu lower bound\n",
    "            0.1,                                # sigma lower bound (much tighter)\n",
    "            max_count * 0.1                     # amplitude lower bound\n",
    "        ]\n",
    "        upper_bounds = [\n",
    "            np.log(peak_volume * 3),            # mu upper bound (focused around peak)\n",
    "            1.5,                                # sigma upper bound (much tighter)\n",
    "            max_count * 2.0                     # amplitude upper bound\n",
    "        ]\n",
    "        \n",
    "        print(f\"      Bounds: mu=[{lower_bounds[0]:.3f}, {upper_bounds[0]:.3f}], sigma=[{lower_bounds[1]:.3f}, {upper_bounds[1]:.3f}]\")\n",
    "        \n",
    "        # Check if initial parameters are within bounds\n",
    "        for i, (param, lower, upper) in enumerate(zip(initial_params, lower_bounds, upper_bounds)):\n",
    "            if param < lower or param > upper:\n",
    "                print(f\"      Adjusting parameter {i}: {param:.3f} -> bounds [{lower:.3f}, {upper:.3f}]\")\n",
    "                initial_params[i] = max(lower, min(param, upper))\n",
    "        \n",
    "        # Perform curve fitting with adjusted parameters - use all data but constrained parameters\n",
    "        params, covariance = curve_fit(\n",
    "            lognormal_pdf, volumes, counts, \n",
    "            p0=initial_params,\n",
    "            bounds=(lower_bounds, upper_bounds), \n",
    "            maxfev=10000,\n",
    "            method='trf'\n",
    "        )\n",
    "        \n",
    "        # Generate fitted curve with more points and proper range\n",
    "        vol_min = volumes.min() * 0.9\n",
    "        vol_max = min(volumes.max() * 1.1, peak_volume * 5)  # Don't extend too far beyond peak\n",
    "        vol_range = np.linspace(vol_min, vol_max, 500)\n",
    "        fitted_curve = lognormal_pdf(vol_range, *params)\n",
    "        \n",
    "        # Check if fit is reasonable - much stricter criteria for volume\n",
    "        if np.any(np.isnan(fitted_curve)) or np.any(np.isinf(fitted_curve)):\n",
    "            return False, \"Fit produced invalid values (NaN or inf)\"\n",
    "        \n",
    "        # Check fit quality - the peak should be reasonably close\n",
    "        fit_peak_idx = np.argmax(fitted_curve)\n",
    "        fit_peak_volume = vol_range[fit_peak_idx]\n",
    "        fit_peak_count = fitted_curve[fit_peak_idx]\n",
    "        \n",
    "        # Peak should be within reasonable range\n",
    "        if abs(fit_peak_volume - peak_volume) > peak_volume * 0.5:\n",
    "            return False, f\"Fit peak too far from data peak: {fit_peak_volume:.6f} vs {peak_volume:.6f}\"\n",
    "        \n",
    "        # Peak height should be reasonable\n",
    "        if fit_peak_count > max_count * 3 or fit_peak_count < max_count * 0.1:\n",
    "            return False, f\"Fit peak height unreasonable: {fit_peak_count:.1f} vs {peak_count:.1f}\"\n",
    "        \n",
    "        print(f\"      Final parameters: mu={params[0]:.3f}, sigma={params[1]:.3f}, amp={params[2]:.1f}\")\n",
    "        print(f\"      Curve range: {vol_min:.6f} - {vol_max:.6f} ¬µm¬≥, max value: {np.max(fitted_curve):.1f}\")\n",
    "        print(f\"      Fit peak at: {fit_peak_volume:.6f} ¬µm¬≥ (data peak: {peak_volume:.6f} ¬µm¬≥)\")\n",
    "        \n",
    "        return True, (vol_range, fitted_curve, params)\n",
    "        \n",
    "    except Exception as e:\n",
    "        return False, f\"Lognormal fit failed: {str(e)}\"\n",
    "\n",
    "\n",
    "def add_d_value_lines_and_bands_volume(ax, stats):\n",
    "    \"\"\"Add D-value lines and uncertainty bands to a subplot, converted to volume in ¬µm¬≥.\"\"\"\n",
    "    legend_elements = []\n",
    "    \n",
    "    if not stats or 'D10_avg' not in stats:\n",
    "        return legend_elements\n",
    "    \n",
    "    # Convert diameter D-values to volume D-values using V = (œÄ/6) √ó d¬≥ and convert to ¬µm¬≥\n",
    "    d10_avg_size = stats['D10_avg']\n",
    "    d10_lower_size = stats.get('D10_lower', d10_avg_size)\n",
    "    d10_upper_size = stats.get('D10_upper', d10_avg_size)\n",
    "    \n",
    "    d50_avg_size = stats['D50_avg'] \n",
    "    d50_lower_size = stats.get('D50_lower', d50_avg_size)\n",
    "    d50_upper_size = stats.get('D50_upper', d50_avg_size)\n",
    "    \n",
    "    d90_avg_size = stats['D90_avg']\n",
    "    d90_lower_size = stats.get('D90_lower', d90_avg_size)\n",
    "    d90_upper_size = stats.get('D90_upper', d90_avg_size)\n",
    "    \n",
    "    # Convert to volumes in ¬µm¬≥ (divide by 1e9 to convert from nm¬≥ to ¬µm¬≥)\n",
    "    d10_avg_vol = (np.pi / 6) * (d10_avg_size ** 3) / 1e9\n",
    "    d10_lower_vol = (np.pi / 6) * (d10_lower_size ** 3) / 1e9\n",
    "    d10_upper_vol = (np.pi / 6) * (d10_upper_size ** 3) / 1e9\n",
    "    \n",
    "    d50_avg_vol = (np.pi / 6) * (d50_avg_size ** 3) / 1e9\n",
    "    d50_lower_vol = (np.pi / 6) * (d50_lower_size ** 3) / 1e9\n",
    "    d50_upper_vol = (np.pi / 6) * (d50_upper_size ** 3) / 1e9\n",
    "    \n",
    "    d90_avg_vol = (np.pi / 6) * (d90_avg_size ** 3) / 1e9\n",
    "    d90_lower_vol = (np.pi / 6) * (d90_lower_size ** 3) / 1e9\n",
    "    d90_upper_vol = (np.pi / 6) * (d90_upper_size ** 3) / 1e9\n",
    "    \n",
    "    span = stats.get('span_avg', (d90_avg_size-d10_avg_size)/d50_avg_size if d50_avg_size > 0 else 0)\n",
    "    \n",
    "    # Add D-value lines and bands using volume values in ¬µm¬≥\n",
    "    for d_val_vol, d_lower_vol, d_upper_vol, style, width, alpha_band in [\n",
    "        (d10_avg_vol, d10_lower_vol, d10_upper_vol, '--', 1.5, 0.15),\n",
    "        (d50_avg_vol, d50_lower_vol, d50_upper_vol, '-', 2.5, 0.25), \n",
    "        (d90_avg_vol, d90_lower_vol, d90_upper_vol, '--', 1.5, 0.15)\n",
    "    ]:\n",
    "        if not np.isnan(d_val_vol):\n",
    "            ax.axvline(x=d_val_vol, color='gray', linestyle=style, alpha=0.8, linewidth=width, zorder=5)\n",
    "            if not np.isnan(d_lower_vol) and not np.isnan(d_upper_vol) and (d_lower_vol != d_val_vol or d_upper_vol != d_val_vol):\n",
    "                ax.axvspan(d_lower_vol, d_upper_vol, alpha=alpha_band, color='gray', zorder=1)\n",
    "    \n",
    "    # Create legend elements showing diameter in nm and volume in ¬µm¬≥\n",
    "    legend_elements.extend([\n",
    "        Line2D([0], [0], color='gray', linestyle='--', linewidth=1.5, \n",
    "              label=f'D10: {d10_avg_size:.1f} nm ‚Üí {d10_avg_vol:.4f} ¬µm¬≥ ({d10_lower_size:.1f}-{d10_upper_size:.1f} nm)'),\n",
    "        Line2D([0], [0], color='gray', linestyle='-', linewidth=2.5, \n",
    "              label=f'D50: {d50_avg_size:.1f} nm ‚Üí {d50_avg_vol:.4f} ¬µm¬≥ ({d50_lower_size:.1f}-{d50_upper_size:.1f} nm)'),\n",
    "        Line2D([0], [0], color='gray', linestyle='--', linewidth=1.5, \n",
    "              label=f'D90: {d90_avg_size:.1f} nm ‚Üí {d90_avg_vol:.4f} ¬µm¬≥ ({d90_lower_size:.1f}-{d90_upper_size:.1f} nm)'),\n",
    "        Line2D([0], [0], color='white', linestyle='', \n",
    "              label=f'Span: {span:.3f}')\n",
    "    ])\n",
    "    \n",
    "    return legend_elements\n",
    "\n",
    "\n",
    "def add_count_fit_curve_volume(ax, plot_df, is_log_scale, fit_color='#F25C54'):\n",
    "    \"\"\"No fitting for volume distributions - they're too skewed for standard fits.\"\"\"\n",
    "    # Return empty fit elements - no fitting needed\n",
    "    return [], None\n",
    "\n",
    "\n",
    "def create_count_vs_volume_plot(plot_df, is_log_scale, stats=None, uniqueID=None, metadata=None):\n",
    "    \"\"\"Create a two-subplot plot for raw counts vs theoretical volume.\"\"\"\n",
    "    \n",
    "    scale_name = \"Logarithmic\" if is_log_scale else \"Linear\"\n",
    "    xscale = 'log' if is_log_scale else 'linear'\n",
    "    color = '#2E7D32'  # Forest green (same as cell_10b volume plots)\n",
    "    \n",
    "    # Calculate theoretical volume for spherical particles: V = (œÄ/6) * d¬≥ and convert to ¬µm¬≥\n",
    "    plot_df = plot_df.copy()\n",
    "    plot_df['volume_um3'] = (np.pi / 6) * (plot_df['size_nm'] ** 3) / 1e9  # Convert nm¬≥ to ¬µm¬≥\n",
    "    \n",
    "    # Sort by volume\n",
    "    plot_df = plot_df.sort_values('volume_um3')\n",
    "    \n",
    "    # Create figure\n",
    "    fig = plt.figure(figsize=(7, 9))\n",
    "    gs = gridspec.GridSpec(2, 1, height_ratios=[0.6, 0.4], hspace=0.3, \n",
    "                          top=0.82, bottom=0.08)\n",
    "    \n",
    "    # =================================================================\n",
    "    # TOP SUBPLOT: MAIN DISTRIBUTION WITH ERROR BARS AND FITS\n",
    "    # =================================================================\n",
    "    ax1 = fig.add_subplot(gs[0])\n",
    "    \n",
    "    # Plot main distribution with error bars\n",
    "    if 'number_sd' in plot_df.columns:\n",
    "        ax1.errorbar(plot_df['volume_um3'], plot_df['number_avg'], \n",
    "                    yerr=plot_df['number_sd'],\n",
    "                    fmt='o', color=color, ecolor=color, alpha=0.7,\n",
    "                    capsize=3, capthick=1, markersize=6, linewidth=1.5,\n",
    "                    label='Raw Counts')\n",
    "    else:\n",
    "        ax1.scatter(plot_df['volume_um3'], plot_df['number_avg'], \n",
    "                   color=color, s=60, alpha=0.8, label='Raw Counts')\n",
    "    \n",
    "    # Add lognormal fit curve (disabled for volume - too skewed)\n",
    "    fit_result = add_count_fit_curve_volume(ax1, plot_df, is_log_scale)\n",
    "    if isinstance(fit_result, tuple):\n",
    "        fit_legend_elements, fit_results = fit_result\n",
    "    else:\n",
    "        fit_legend_elements = fit_result\n",
    "        fit_results = None\n",
    "    \n",
    "    # No fitting for volume distributions - they're inherently too skewed\n",
    "    \n",
    "    # Format top subplot\n",
    "    ax1.set_ylabel('Raw Particle Counts', color=color, fontsize=14, labelpad=10)\n",
    "    ax1.tick_params(axis='y', labelcolor=color, labelsize=12)\n",
    "    ax1.tick_params(axis='x', labelsize=12)\n",
    "    ax1.spines['left'].set_color(color)\n",
    "    \n",
    "    # Set x-axis scale and add better tick labels for log scale\n",
    "    ax1.set_xscale(xscale)\n",
    "    if is_log_scale:\n",
    "        from matplotlib.ticker import LogLocator, LogFormatter\n",
    "        ax1.xaxis.set_major_locator(LogLocator(base=10, numticks=12))\n",
    "        ax1.xaxis.set_minor_locator(LogLocator(base=10, subs=(0.2, 0.4, 0.6, 0.8), numticks=12))\n",
    "        ax1.xaxis.set_major_formatter(LogFormatter(base=10, labelOnlyBase=False))\n",
    "    \n",
    "    # Smart range calculation based on where the count signal is\n",
    "    weights_for_range = plot_df['number_avg'].values\n",
    "    vol_for_range = plot_df['volume_um3'].values\n",
    "    \n",
    "    # Find where 99% of the count signal is contained\n",
    "    cumsum_weights = np.cumsum(weights_for_range)\n",
    "    total_weight = cumsum_weights[-1]\n",
    "    \n",
    "    if total_weight > 0:\n",
    "        # Find 1st and 99th percentiles of the count-weighted distribution\n",
    "        p1_idx = np.searchsorted(cumsum_weights, 0.01 * total_weight)\n",
    "        p99_idx = np.searchsorted(cumsum_weights, 0.99 * total_weight)\n",
    "        \n",
    "        signal_min_vol = vol_for_range[max(0, p1_idx)]\n",
    "        signal_max_vol = vol_for_range[min(len(vol_for_range)-1, p99_idx)]\n",
    "        data_max_vol = plot_df['volume_um3'].max()\n",
    "        \n",
    "        if is_log_scale:\n",
    "            # Log scale: focus on the count signal range with some padding\n",
    "            min_vol = max(signal_min_vol * 0.7, (np.pi / 6) * (20**3) / 1e9)  # Don't go below 20nm diameter equivalent\n",
    "            max_vol = min(signal_max_vol * 1.8, data_max_vol * 1.2)\n",
    "            ax1.set_xlim([min_vol, max_vol])\n",
    "        else:\n",
    "            # Linear scale: tighten the range, focus on main signal\n",
    "            min_vol = 0\n",
    "            max_vol = min(signal_max_vol * 1.15, (np.pi / 6) * (350**3) / 1e9)  # Cap at ~350nm diameter equivalent\n",
    "            ax1.set_xlim([min_vol, max_vol])\n",
    "    else:\n",
    "        # Fallback if no signal\n",
    "        if is_log_scale:\n",
    "            ax1.set_xlim([(np.pi / 6) * (30**3) / 1e9, (np.pi / 6) * (300**3) / 1e9])\n",
    "        else:\n",
    "            ax1.set_xlim([0, (np.pi / 6) * (250**3) / 1e9])\n",
    "    \n",
    "    # Set y-axis to start from 0\n",
    "    y_min, y_max = ax1.get_ylim()\n",
    "    ax1.set_ylim([0, y_max])\n",
    "    \n",
    "    # Add D-value lines and bands (converted to volume)\n",
    "    d_legend_elements = add_d_value_lines_and_bands_volume(ax1, stats)\n",
    "    \n",
    "    # Create comprehensive legend for top plot - PLACE OUTSIDE\n",
    "    main_legend = [Line2D([0], [0], marker='o', color='w', markerfacecolor=color, \n",
    "                          markersize=8, label='Raw Counts')]\n",
    "    \n",
    "    all_legend_elements = main_legend + fit_legend_elements + d_legend_elements\n",
    "    leg1 = ax1.legend(handles=all_legend_elements, fontsize=9, frameon=True, \n",
    "                     bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    leg1.get_frame().set_alpha(0.95)\n",
    "    leg1.get_frame().set_edgecolor('lightgray')\n",
    "    \n",
    "    ax1.grid(True, linestyle='--', alpha=0.4)\n",
    "    ax1.set_xlabel('')  # No x-label on top plot\n",
    "    \n",
    "    # =================================================================\n",
    "    # BOTTOM SUBPLOT: CUMULATIVE DISTRIBUTION WITH UNCERTAINTY BANDS\n",
    "    # =================================================================\n",
    "    ax2 = fig.add_subplot(gs[1])\n",
    "    \n",
    "    # Calculate cumulative counts as percentage\n",
    "    cumsum_counts = np.cumsum(plot_df['number_avg'])\n",
    "    max_cumsum = cumsum_counts.iloc[-1] if len(cumsum_counts) > 0 else 1\n",
    "    \n",
    "    if max_cumsum > 0:\n",
    "        cumsum_percentage = (cumsum_counts / max_cumsum) * 100\n",
    "        ax2.plot(plot_df['volume_um3'], cumsum_percentage, '-', \n",
    "                color=color, linewidth=3, alpha=0.9, label='Cumulative %')\n",
    "        \n",
    "        # Add uncertainty bands if available\n",
    "        if 'number_sd' in plot_df.columns:\n",
    "            # Calculate cumulative uncertainty\n",
    "            cumsum_sd = np.sqrt(np.cumsum(plot_df['number_sd'] ** 2))\n",
    "            cumsum_sd_percentage = (cumsum_sd / max_cumsum) * 100\n",
    "            ax2.fill_between(plot_df['volume_um3'], \n",
    "                           cumsum_percentage - cumsum_sd_percentage,\n",
    "                           cumsum_percentage + cumsum_sd_percentage,\n",
    "                           color=color, alpha=0.25, zorder=1, label='¬± SD')\n",
    "    \n",
    "    ax2.set_ylim([0, 110])\n",
    "    ax2.set_ylabel('Cumulative Percentage (%)', color=color, fontsize=14, labelpad=10)\n",
    "    ax2.tick_params(axis='y', labelcolor=color, labelsize=12)\n",
    "    ax2.spines['left'].set_color(color)\n",
    "    \n",
    "    # Format bottom subplot\n",
    "    ax2.set_xlabel('Theoretical Volume (¬µm¬≥)', fontsize=14, labelpad=10)\n",
    "    ax2.tick_params(axis='x', labelsize=12)\n",
    "    ax2.set_xscale(xscale)\n",
    "    ax2.set_xlim(ax1.get_xlim())  # Match top plot limits\n",
    "    \n",
    "    if is_log_scale:\n",
    "        # Add same detailed log scale ticks to bottom plot\n",
    "        from matplotlib.ticker import LogLocator, LogFormatter\n",
    "        ax2.xaxis.set_major_locator(LogLocator(base=10, numticks=12))\n",
    "        ax2.xaxis.set_minor_locator(LogLocator(base=10, subs=(0.2, 0.4, 0.6, 0.8), numticks=12))\n",
    "        ax2.xaxis.set_major_formatter(LogFormatter(base=10, labelOnlyBase=False))\n",
    "    \n",
    "    # Add D-value lines to bottom plot (converted to volume)\n",
    "    add_d_value_lines_and_bands_volume(ax2, stats)\n",
    "    \n",
    "    # Legend for bottom plot - PLACE OUTSIDE\n",
    "    cumulative_legend = [\n",
    "        Line2D([0], [0], color=color, linewidth=3, label='Cumulative %'),\n",
    "        Line2D([0], [0], color=color, alpha=0.25, linewidth=8, label='¬± SD')\n",
    "    ]\n",
    "    \n",
    "    leg2 = ax2.legend(handles=cumulative_legend, fontsize=10, frameon=True, \n",
    "                     bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    leg2.get_frame().set_alpha(0.95)\n",
    "    leg2.get_frame().set_edgecolor('lightgray')\n",
    "    \n",
    "    ax2.grid(True, linestyle='--', alpha=0.4)\n",
    "    \n",
    "    # =================================================================\n",
    "    # TITLE AND METADATA\n",
    "    # =================================================================\n",
    "    \n",
    "    # Extract replicate info\n",
    "    replicate_info = \"\"\n",
    "    if metadata and 'num_replicates' in metadata:\n",
    "        num_reps = metadata['num_replicates']\n",
    "        if num_reps and str(num_reps) != '1':\n",
    "            replicate_info = f\" (n={num_reps})\"\n",
    "    \n",
    "    # Set main title\n",
    "    main_title = f'{scale_name} Raw Counts vs\\nTheoretical Volume: {uniqueID}{replicate_info}'\n",
    "    fig.suptitle(main_title, fontsize=14, fontweight='bold', y=0.94)\n",
    "    \n",
    "    # Add subtitle\n",
    "    subtitle = f\"Error bars/bands: ¬± SD | Volume = (œÄ/6) √ó diameter¬≥ | No fits (volume too skewed)\"\n",
    "    fig.text(0.5, 0.87, subtitle, ha='center', fontsize=11, style='italic')\n",
    "    \n",
    "    return fig, fit_results\n",
    "\n",
    "\n",
    "def generate_count_vs_volume_plots(distribution_df, stats_dict=None, uniqueID=None, \n",
    "                                  metadata=None, output_dir=None, config=None):\n",
    "    \"\"\"Generate raw count vs volume plots for both linear and log scales.\"\"\"\n",
    "    \n",
    "    if distribution_df is None or distribution_df.empty:\n",
    "        return False, \"No data available for plotting\"\n",
    "    \n",
    "    # Check if we have the required columns\n",
    "    if 'number_avg' not in distribution_df.columns:\n",
    "        return False, \"Raw count data (number_avg) not found in dataset\"\n",
    "    \n",
    "    plt.style.use('default')\n",
    "    \n",
    "    # Determine output directory\n",
    "    if output_dir is None:\n",
    "        if config is not None and \"directory\" in config:\n",
    "            base_dir = config[\"directory\"]\n",
    "            output_dir = os.path.join(base_dir, \"processed\")\n",
    "        else:\n",
    "            output_dir = os.path.join(os.getcwd(), \"processed\")\n",
    "    \n",
    "    try:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "    except Exception as e:\n",
    "        return False, f\"Failed to create output directory: {str(e)}\"\n",
    "    \n",
    "    created_files = []\n",
    "    \n",
    "    # Generate linear and logarithmic plots\n",
    "    for is_log_scale in [False, True]:\n",
    "        scale_type = 'logarithmic' if is_log_scale else 'linear'\n",
    "        scale_name = 'log' if is_log_scale else 'linear'\n",
    "        \n",
    "        print(f\"Creating {scale_name} count vs volume plot...\")\n",
    "        \n",
    "        # Filter data for this scale\n",
    "        plot_df = distribution_df[distribution_df['scale'] == scale_type].copy()\n",
    "        \n",
    "        if plot_df.empty:\n",
    "            print(f\"  Warning: No {scale_type} scale data available\")\n",
    "            continue\n",
    "        \n",
    "        # Get statistics (use number-weighted stats from the appropriate scale)\n",
    "        stats = None\n",
    "        if stats_dict and scale_type in stats_dict and 'number' in stats_dict[scale_type]:\n",
    "            stats = stats_dict[scale_type]['number']\n",
    "        \n",
    "        # Create the plot (matching function signature)\n",
    "        fig, fit_results = create_count_vs_volume_plot(plot_df, is_log_scale, stats, uniqueID, metadata)\n",
    "        \n",
    "        if fig is None:\n",
    "            print(f\"  Failed to create plot\")\n",
    "            continue\n",
    "        \n",
    "        # Save fit results to comprehensive fits file\n",
    "        if fit_results:\n",
    "            fit_type, fit_data = fit_results\n",
    "            try:\n",
    "                import json\n",
    "                \n",
    "                # Load existing fits file or create new one\n",
    "                fits_filename = f\"Fits_{uniqueID}_all.json\"\n",
    "                fits_path = os.path.join(output_dir, fits_filename)\n",
    "                \n",
    "                if os.path.exists(fits_path):\n",
    "                    with open(fits_path, 'r') as f:\n",
    "                        all_fits = json.load(f)\n",
    "                else:\n",
    "                    all_fits = {'dataset': uniqueID, 'fits': {}}\n",
    "                \n",
    "                # Add this fit to the collection\n",
    "                fit_key = f\"count_vs_volume_{scale_name}\"\n",
    "                all_fits['fits'][fit_key] = {\n",
    "                    'distribution_type': 'count_vs_volume',\n",
    "                    'scale': scale_name,\n",
    "                    'fit_type': fit_type,\n",
    "                    'parameters': fit_data\n",
    "                }\n",
    "                \n",
    "                # Save updated fits file\n",
    "                with open(fits_path, 'w') as f:\n",
    "                    json.dump(all_fits, f, indent=2, default=str)\n",
    "                \n",
    "                print(f\"  ‚úì Saved fit to: {fits_filename}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  ‚ö† Failed to save fit: {str(e)}\")\n",
    "        \n",
    "        # Save the plot\n",
    "        try:\n",
    "            base_filename = f\"Plot_{uniqueID}_{scale_name}_count_vs_volume\"\n",
    "            \n",
    "            pdf_path = os.path.join(output_dir, f\"{base_filename}.pdf\")\n",
    "            fig.savefig(pdf_path, bbox_inches='tight', dpi=300)\n",
    "            \n",
    "            png_path = os.path.join(output_dir, f\"{base_filename}.png\")\n",
    "            fig.savefig(png_path, bbox_inches='tight', dpi=300)\n",
    "            \n",
    "            created_files.append(pdf_path)\n",
    "            print(f\"  ‚úì Saved: {base_filename}.pdf/.png\")\n",
    "            \n",
    "            plt.close(fig)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚úó Failed to save plot: {str(e)}\")\n",
    "            plt.close(fig)\n",
    "            continue\n",
    "    \n",
    "    return True, created_files\n",
    "\n",
    "\n",
    "# Execute if we have the required data\n",
    "if 'current_distribution_df' in globals() and current_distribution_df is not None:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"GENERATING RAW COUNT vs THEORETICAL VOLUME PLOTS WITH D-VALUES\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    uniqueID = current_metadata.get('persistentID', 'unknown') if 'current_metadata' in globals() else 'unknown'\n",
    "    stats = current_stats if 'current_stats' in globals() else None\n",
    "    metadata = current_metadata if 'current_metadata' in globals() else None\n",
    "    config = CONFIG if 'CONFIG' in globals() else None\n",
    "    \n",
    "    print(f\"Creating count vs volume plots for: {uniqueID}\")\n",
    "    print(\"Volume = (œÄ/6) √ó diameter¬≥ (spherical particles) in ¬µm¬≥\")\n",
    "    print(\"Includes: Linear + Logarithmic (Lognormal fits + D-values for both)\")\n",
    "    \n",
    "    success, plot_files = generate_count_vs_volume_plots(\n",
    "        current_distribution_df,\n",
    "        stats_dict=stats,\n",
    "        uniqueID=uniqueID,\n",
    "        metadata=metadata,\n",
    "        config=config\n",
    "    )\n",
    "    \n",
    "    if not success:\n",
    "        print(f\"ERROR: {plot_files}\")\n",
    "    else:\n",
    "        print(f\"\\n‚úì Successfully created {len(plot_files)} count vs volume plots!\")\n",
    "        for filepath in plot_files:\n",
    "            print(f\"  - {os.path.basename(filepath)}\")\n",
    "        \n",
    "        current_count_vs_volume_plots = plot_files\n",
    "\n",
    "else:\n",
    "    print(\"No data found. Run the complete workflow first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda49071-7e0a-4eba-b13d-dd2974ad78d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": true,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
